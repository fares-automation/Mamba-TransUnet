{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "import nibabel as nib\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "from skimage.transform import resize\n",
        "from tqdm import tqdm\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "from skimage.morphology import erosion, dilation, square\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.ndimage import rotate\n",
        "from skimage.transform import rescale"
      ],
      "metadata": {
        "id": "dJuwkmy5OLiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration (matches paper parameters)\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 256\n",
        "IMG_CHANNELS = 1\n",
        "BATCH_SIZE = 8  # U-Net 1 uses 8, U-Net 2 uses 4\n",
        "EPOCHS = 100\n",
        "INIT_LR = 1e-4\n",
        "N_FOLDS = 10  # 10-fold cross-validation as in paper\n",
        "SEED = 42\n",
        "\n",
        "# Path to CAMUS dataset\n",
        "DATA_PATH = \"/content/drive/MyDrive/database_nifti\"\n"
      ],
      "metadata": {
        "id": "8rpKx2dAOPVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define metrics\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1 - dice_coef(y_true, y_pred)\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n",
        "# Data loading and preprocessing\n",
        "def load_nifti_image(file_path):\n",
        "    img = nib.load(file_path)\n",
        "    data = img.get_fdata()\n",
        "    return np.squeeze(data)  # Remove singleton dimensions\n",
        "\n",
        "\n",
        "# Updated preprocessing with augmentation\n",
        "def preprocess_patient_with_augmentation(patient_folder):\n",
        "    print(f\"\\nProcessing with augmentation: {patient_folder}\")\n",
        "\n",
        "    images = []\n",
        "    masks_endo = []\n",
        "    masks_epi = []\n",
        "    masks_la = []\n",
        "\n",
        "    views = ['2CH', '4CH']\n",
        "    time_points = ['ED', 'ES']\n",
        "\n",
        "    for view in views:\n",
        "        for tp in time_points:\n",
        "            base_name = os.path.basename(patient_folder)\n",
        "            img_path = f\"{patient_folder}/{base_name}_{view}_{tp}.nii\"\n",
        "            gt_path = f\"{patient_folder}/{base_name}_{view}_{tp}_gt.nii\"\n",
        "\n",
        "            if not os.path.exists(img_path) or not os.path.exists(gt_path):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                img = nib.load(img_path).get_fdata()\n",
        "                gt = nib.load(gt_path).get_fdata()\n",
        "\n",
        "                img_resized = resize(img, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=True)\n",
        "                gt_resized = resize(gt, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=False)\n",
        "\n",
        "                img_resized = (img_resized - img_resized.min()) / (img_resized.max() - img_resized.min())\n",
        "\n",
        "                # Augmentation: Apply rotations and scalings\n",
        "                for angle in [0, 90, 180, 270]:\n",
        "                    img_rotated = rotate(img_resized, angle, reshape=False)\n",
        "                    gt_rotated = rotate(gt_resized, angle, reshape=False)\n",
        "\n",
        "                    for scale in [0.9, 1.0, 1.1]:\n",
        "                        img_scaled = rescale(img_rotated, scale, preserve_range=True, multichannel=False, anti_aliasing=True)\n",
        "                        gt_scaled = rescale(gt_rotated, scale, preserve_range=True, multichannel=False, anti_aliasing=False)\n",
        "\n",
        "                        # Ensure the scaled images are resized back to original dimensions\n",
        "                        img_scaled_resized = resize(img_scaled, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=True)\n",
        "                        gt_scaled_resized = resize(gt_scaled, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=False)\n",
        "\n",
        "                        mask_endo = (gt_scaled_resized == 1).astype(np.float32)\n",
        "                        mask_epi = (gt_scaled_resized == 2).astype(np.float32)\n",
        "                        mask_la = (gt_scaled_resized == 3).astype(np.float32)\n",
        "\n",
        "                        images.append(img_scaled_resized[..., np.newaxis])\n",
        "                        masks_endo.append(mask_endo[..., np.newaxis])\n",
        "                        masks_epi.append(mask_epi[..., np.newaxis])\n",
        "                        masks_la.append(mask_la[..., np.newaxis])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {view}_{tp}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    return np.array(images), np.array(masks_endo), np.array(masks_epi), np.array(masks_la)\n",
        "\n",
        "\n",
        "def preprocess_patient(patient_folder):\n",
        "    print(f\"\\nProcessing: {patient_folder}\")\n",
        "\n",
        "    images = []\n",
        "    masks_endo = []\n",
        "    masks_epi = []\n",
        "    masks_la = []\n",
        "\n",
        "    views = ['2CH', '4CH']\n",
        "    time_points = ['ED', 'ES']\n",
        "\n",
        "    for view in views:\n",
        "        for tp in time_points:\n",
        "            base_name = os.path.basename(patient_folder)\n",
        "            img_path = f\"{patient_folder}/{base_name}_{view}_{tp}.nii\"\n",
        "            gt_path = f\"{patient_folder}/{base_name}_{view}_{tp}_gt.nii\"\n",
        "\n",
        "            if not os.path.exists(img_path) or not os.path.exists(gt_path):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                img = nib.load(img_path).get_fdata()\n",
        "                gt = nib.load(gt_path).get_fdata()\n",
        "\n",
        "                img_resized = resize(img, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=True)\n",
        "                gt_resized = resize(gt, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=False)\n",
        "\n",
        "                img_resized = (img_resized - img_resized.min()) / (img_resized.max() - img_resized.min())\n",
        "\n",
        "                mask_endo = (gt_resized == 1).astype(np.float32)\n",
        "                mask_epi = (gt_resized == 2).astype(np.float32)\n",
        "                mask_la = (gt_resized == 3).astype(np.float32)\n",
        "\n",
        "                images.append(img_resized[..., np.newaxis])\n",
        "                masks_endo.append(mask_endo[..., np.newaxis])\n",
        "                masks_epi.append(mask_epi[..., np.newaxis])\n",
        "                masks_la.append(mask_la[..., np.newaxis])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {view}_{tp}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    return np.array(images), np.array(masks_endo), np.array(masks_epi), np.array(masks_la)"
      ],
      "metadata": {
        "id": "m5nIWyQEOSFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Updated load_dataset function with a single progress bar\n",
        "# Updated load_dataset function\n",
        "def load_dataset_with_split(base_path, test_ratio=0.15):\n",
        "    patient_folders = sorted([\n",
        "        os.path.join(base_path, f)\n",
        "        for f in os.listdir(base_path)\n",
        "        if f.startswith('patient') and os.path.isdir(os.path.join(base_path, f))\n",
        "    ])\n",
        "\n",
        "    # Split into train/test\n",
        "    np.random.shuffle(patient_folders)\n",
        "    split_idx = int(len(patient_folders) * (1 - test_ratio))\n",
        "    train_folders = patient_folders[:split_idx]\n",
        "    test_folders = patient_folders[split_idx:]\n",
        "\n",
        "    # Preprocess train data with augmentation\n",
        "    all_images_train, all_masks_endo_train, all_masks_epi_train, all_masks_la_train = ([] for _ in range(4))\n",
        "\n",
        "    for patient_folder in tqdm(train_folders, desc=\"Loading training patients\"):\n",
        "        images, masks_endo, masks_epi, masks_la = preprocess_patient_with_augmentation(patient_folder)\n",
        "        all_images_train.append(images)\n",
        "        all_masks_endo_train.append(masks_endo)\n",
        "        all_masks_epi_train.append(masks_epi)\n",
        "        all_masks_la_train.append(masks_la)\n",
        "\n",
        "    # Preprocess test data without augmentation\n",
        "    all_images_test, all_masks_endo_test, all_masks_epi_test, all_masks_la_test = ([] for _ in range(4))\n",
        "\n",
        "    for patient_folder in tqdm(test_folders, desc=\"Loading test patients\"):\n",
        "        images, masks_endo, masks_epi, masks_la = preprocess_patient(patient_folder)\n",
        "        all_images_test.append(images)\n",
        "        all_masks_endo_test.append(masks_endo)\n",
        "        all_masks_epi_test.append(masks_epi)\n",
        "        all_masks_la_test.append(masks_la)\n",
        "\n",
        "    return (np.concatenate(all_images_train, axis=0) if all_images_train else np.array([]),\n",
        "            np.concatenate(all_masks_endo_train, axis=0) if all_masks_endo_train else np.array([]),\n",
        "            np.concatenate(all_masks_epi_train, axis=0) if all_masks_epi_train else np.array([]),\n",
        "            np.concatenate(all_masks_la_train, axis=0) if all_masks_la_train else np.array([]),\n",
        "            np.concatenate(all_images_test, axis=0) if all_images_test else np.array([]),\n",
        "            np.concatenate(all_masks_endo_test, axis=0) if all_masks_endo_test else np.array([]),\n",
        "            np.concatenate(all_masks_epi_test, axis=0) if all_masks_epi_test else np.array([]),\n",
        "            np.concatenate(all_masks_la_test, axis=0) if all_masks_la_test else np.array([]))"
      ],
      "metadata": {
        "id": "j0T6MBhCNlO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# U-Net 1 architecture (optimized for speed)\n",
        "def unet1(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Downsampling path\n",
        "    conv1 = Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(32, 3, activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(64, 3, activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, 3, activation='relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(128, 3, activation='relu', padding='same')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, 3, activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(256, 3, activation='relu', padding='same')(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    # Bottom\n",
        "    conv5 = Conv2D(512, 3, activation='relu', padding='same')(pool4)\n",
        "    conv5 = Conv2D(512, 3, activation='relu', padding='same')(conv5)\n",
        "\n",
        "    # Upsampling path\n",
        "    up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=-1)\n",
        "    conv6 = Conv2D(256, 3, activation='relu', padding='same')(up6)\n",
        "    conv6 = Conv2D(256, 3, activation='relu', padding='same')(conv6)\n",
        "\n",
        "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=-1)\n",
        "    conv7 = Conv2D(128, 3, activation='relu', padding='same')(up7)\n",
        "    conv7 = Conv2D(128, 3, activation='relu', padding='same')(conv7)\n",
        "\n",
        "    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=-1)\n",
        "    conv8 = Conv2D(64, 3, activation='relu', padding='same')(up8)\n",
        "    conv8 = Conv2D(64, 3, activation='relu', padding='same')(conv8)\n",
        "\n",
        "    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=-1)\n",
        "    conv9 = Conv2D(32, 3, activation='relu', padding='same')(up9)\n",
        "    conv9 = Conv2D(32, 3, activation='relu', padding='same')(conv9)\n",
        "\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# U-Net 2 architecture (optimized for accuracy)\n",
        "def unet2(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Downsampling path\n",
        "    conv1 = Conv2D(64, 3, padding='same')(inputs)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Activation('relu')(conv1)\n",
        "    conv1 = Conv2D(64, 3, padding='same')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Activation('relu')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(128, 3, padding='same')(pool1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Activation('relu')(conv2)\n",
        "    conv2 = Conv2D(128, 3, padding='same')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Activation('relu')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(256, 3, padding='same')(pool2)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Activation('relu')(conv3)\n",
        "    conv3 = Conv2D(256, 3, padding='same')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Activation('relu')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(512, 3, padding='same')(pool3)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Activation('relu')(conv4)\n",
        "    conv4 = Conv2D(512, 3, padding='same')(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Activation('relu')(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    # Bottom\n",
        "    conv5 = Conv2D(1024, 3, padding='same')(pool4)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Activation('relu')(conv5)\n",
        "    conv5 = Conv2D(1024, 3, padding='same')(conv5)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Activation('relu')(conv5)\n",
        "\n",
        "    # Upsampling path\n",
        "    up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=-1)\n",
        "    conv6 = Conv2D(512, 3, padding='same')(up6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Activation('relu')(conv6)\n",
        "    conv6 = Conv2D(512, 3, padding='same')(conv6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Activation('relu')(conv6)\n",
        "\n",
        "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=-1)\n",
        "    conv7 = Conv2D(256, 3, padding='same')(up7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Activation('relu')(conv7)\n",
        "    conv7 = Conv2D(256, 3, padding='same')(conv7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Activation('relu')(conv7)\n",
        "\n",
        "    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=-1)\n",
        "    conv8 = Conv2D(128, 3, padding='same')(up8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "    conv8 = Activation('relu')(conv8)\n",
        "    conv8 = Conv2D(128, 3, padding='same')(conv8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "    conv8 = Activation('relu')(conv8)\n",
        "\n",
        "    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=-1)\n",
        "    conv9 = Conv2D(64, 3, padding='same')(up9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "    conv9 = Activation('relu')(conv9)\n",
        "    conv9 = Conv2D(64, 3, padding='same')(conv9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "    conv9 = Activation('relu')(conv9)\n",
        "\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ulLzhh9iOV0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    y_pred = (y_pred > 0.5).astype(np.float32)\n",
        "\n",
        "    # Dice coefficient\n",
        "    intersection = np.sum(y_true * y_pred)\n",
        "    dice = (2. * intersection + 1.) / (np.sum(y_true) + np.sum(y_pred) + 1.)\n",
        "\n",
        "    # Mean absolute distance\n",
        "    dt_true = distance_transform_edt(1 - y_true.squeeze())\n",
        "    dt_pred = distance_transform_edt(1 - y_pred.squeeze())\n",
        "    mean_dist = (np.mean(dt_pred[y_true.squeeze() > 0.5]) +\n",
        "                 np.mean(dt_true[y_pred.squeeze() > 0.5])) / 2\n",
        "\n",
        "    # Hausdorff distance (approximation)\n",
        "    contour_true = y_true.squeeze() - erosion(y_true.squeeze(), square(3))\n",
        "    contour_pred = y_pred.squeeze() - erosion(y_pred.squeeze(), square(3))\n",
        "\n",
        "    if np.sum(contour_true) == 0 or np.sum(contour_pred) == 0:\n",
        "        hausdorff = np.inf\n",
        "    else:\n",
        "        dt_true_contour = distance_transform_edt(1 - contour_true)\n",
        "        dt_pred_contour = distance_transform_edt(1 - contour_pred)\n",
        "        hd_true = np.max(contour_pred * dt_true_contour)\n",
        "        hd_pred = np.max(contour_true * dt_pred_contour)\n",
        "        hausdorff = max(hd_true, hd_pred)\n",
        "\n",
        "    return dice, mean_dist, hausdorff"
      ],
      "metadata": {
        "id": "atQb02pxObQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7Y5DR24EFRE"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    print(\"Loading and preprocessing dataset...\")\n",
        "    (X_train, y_endo_train, y_epi_train, y_la_train,\n",
        "     X_test, y_endo_test, y_epi_test, y_la_test) = load_dataset_with_split(DATA_PATH)\n",
        "\n",
        "    # Choose which U-Net to use\n",
        "    unet_version = 1\n",
        "    batch_size = 8 if unet_version == 1 else 4\n",
        "\n",
        "    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "        print(f\"\\n=== Fold {fold + 1}/{N_FOLDS} ===\")\n",
        "        X_fold_train, X_val = X_train[train_idx], X_train[val_idx]\n",
        "        y_fold_train, y_val = y_endo_train[train_idx], y_endo_train[val_idx]\n",
        "\n",
        "        model = unet1() if unet_version == 1 else unet2()\n",
        "        model.compile(optimizer=Adam(learning_rate=INIT_LR),\n",
        "                      loss=bce_dice_loss,\n",
        "                      metrics=[dice_coef, 'accuracy'])\n",
        "\n",
        "        callbacks = [\n",
        "            ModelCheckpoint(f\"unet{unet_version}_fold{fold}_best.keras\",\n",
        "                            monitor='val_dice_coef',\n",
        "                            mode='max',\n",
        "                            save_best_only=True,\n",
        "                            verbose=1),\n",
        "            EarlyStopping(monitor='val_dice_coef', patience=15, mode='max', verbose=1),\n",
        "            ReduceLROnPlateau(monitor='val_dice_coef', factor=0.5, patience=5, min_lr=1e-6, mode='max', verbose=1)\n",
        "        ]\n",
        "\n",
        "        print(f\"Training on {len(X_fold_train)} samples, validating on {len(X_val)} samples\")\n",
        "        model.fit(X_fold_train, y_fold_train, batch_size=batch_size, epochs=EPOCHS,\n",
        "                  validation_data=(X_val, y_val), callbacks=callbacks, verbose=1)\n",
        "\n",
        "        model.load_weights(f\"unet{unet_version}_fold{fold}_best.keras\")\n",
        "\n",
        "        # Test on one random sample from test set\n",
        "        idx = np.random.randint(0, len(X_test))\n",
        "        X_sample, y_sample = X_test[idx:idx+1], y_endo_test[idx:idx+1]\n",
        "        y_pred_sample = model.predict(X_sample, batch_size=1)\n",
        "\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(X_sample[0].squeeze(), cmap='gray')\n",
        "        plt.title(\"Original Image\")\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(y_sample[0].squeeze(), cmap='gray')\n",
        "        plt.title(\"Ground Truth\")\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(y_pred_sample[0].squeeze() > 0.5, cmap='gray')\n",
        "        plt.title(\"Predicted Mask\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Collect fold results\n",
        "        dice, mean_dist, hausdorff = calculate_metrics(y_sample[0], y_pred_sample[0] > 0.5)\n",
        "        fold_results.append({\n",
        "            'dice': dice,\n",
        "            'mean_dist': mean_dist,\n",
        "            'hausdorff': hausdorff,\n",
        "        })\n",
        "\n",
        "    # Print final cross-validation results\n",
        "    print(\"\\n=== Final Cross-Validation Results ===\")\n",
        "    avg_dice = np.mean([r['dice'] for r in fold_results])\n",
        "    avg_mean_dist = np.mean([r['mean_dist'] for r in fold_results])\n",
        "    avg_hausdorff = np.mean([r['hausdorff'] for r in fold_results])\n",
        "\n",
        "    print(f\"Average Dice: {avg_dice:.3f}\")\n",
        "    print(f\"Average Mean Absolute Distance: {avg_mean_dist:.3f} mm\")\n",
        "    print(f\"Average Hausdorff Distance: {avg_hausdorff:.3f} mm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2DA2X3gjOdSa",
        "outputId": "6a8f8f47-5f52-40e1-af46-7049f36c04a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading training patients: 100%|██████████| 425/425 [00:00<00:00, 2880.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0450\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0340\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0004\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0176\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0044\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0219\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0197\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0264\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0130\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0200\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0012\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0161\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0432\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0214\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0250\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0279\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0324\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0070\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0425\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0054\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0421\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0136\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0477\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0187\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0381\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0326\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0439\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0191\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0291\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0098\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0322\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0013\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0018\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0073\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0061\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0041\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0455\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0371\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0302\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0178\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0028\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0346\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0488\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0084\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0312\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0062\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0128\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0083\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0017\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0053\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0274\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0181\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0382\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0114\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0454\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0433\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0071\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0210\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0448\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0079\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0105\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0349\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0262\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0293\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0492\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0361\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0077\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0072\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0345\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0358\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0392\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0111\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0449\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0242\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0294\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0220\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0393\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0304\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0330\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0240\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0386\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0129\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0078\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0154\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0075\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0368\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0189\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0194\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0440\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0255\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0126\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0238\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0097\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0244\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0256\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0124\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0193\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0215\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0039\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0319\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0418\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0300\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0069\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0495\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0416\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0145\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0297\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0444\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0167\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0443\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0112\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0423\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0376\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0089\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0385\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0103\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0206\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0470\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0016\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0165\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0192\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0407\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0420\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0228\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0047\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0226\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0497\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0131\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0183\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0303\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0170\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0141\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0268\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0043\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0380\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0045\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0122\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0030\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0230\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0166\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0227\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0066\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0426\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0155\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0065\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0090\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0081\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0149\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0462\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0463\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0284\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0468\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0064\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0059\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0101\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0417\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0307\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0037\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0182\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0001\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0033\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0305\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0152\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0347\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0434\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0308\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0366\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0204\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0051\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0331\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0050\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0055\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0332\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0482\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0106\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0109\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0132\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0474\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0164\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0217\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0383\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0374\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0163\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0286\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0021\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0395\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0245\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0108\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0431\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0246\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0437\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0116\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0460\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0035\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0464\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0024\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0401\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0151\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0429\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0387\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0329\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0333\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0360\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0475\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0275\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0357\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0237\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0283\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0339\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0473\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0353\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0146\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0288\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0355\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0156\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0399\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0311\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0034\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0175\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0140\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0343\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0472\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0367\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0251\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0231\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0008\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0405\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0138\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0459\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0327\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0257\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0002\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0500\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0281\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0266\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0169\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0480\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0403\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0038\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0224\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0139\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0173\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0354\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0396\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0022\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0014\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0310\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0422\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0481\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0465\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0232\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0158\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0278\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0184\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0239\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0365\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0060\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0438\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0276\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0277\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0213\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0171\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0351\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0402\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0453\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0447\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0082\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0094\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0295\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0099\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0479\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0452\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0174\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0314\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0489\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0092\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0172\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0015\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0236\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0370\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0373\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0471\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0223\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0026\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0222\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0445\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0469\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0253\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0157\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0442\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0461\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0248\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0221\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0048\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0087\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0428\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0269\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0282\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0318\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0127\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0020\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0042\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0147\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0458\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0056\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0235\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0241\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0095\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0188\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0372\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0478\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0247\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0267\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0466\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0486\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0298\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0352\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0177\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0476\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0100\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0225\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0364\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0019\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0023\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0359\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0104\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0143\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0467\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0046\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0201\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0263\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0316\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0424\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0483\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0378\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0168\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0207\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0414\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0096\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0110\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0234\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0036\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0068\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0348\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0369\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0203\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0350\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0377\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0031\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0093\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0412\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0397\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0441\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0088\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0190\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0398\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0179\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0091\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0427\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0265\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0419\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0199\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0052\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0074\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0115\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0299\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0287\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0457\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0261\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0212\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0490\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0342\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0010\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0456\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0400\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0107\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0144\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0118\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0058\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0334\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0344\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0142\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0117\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0249\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0430\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0067\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0436\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0133\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0208\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0205\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0484\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0057\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0209\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0394\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0254\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0005\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0390\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0134\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0335\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0384\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0388\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0049\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0413\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0290\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0280\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0025\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0325\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0313\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0285\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0391\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0485\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0375\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0410\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0086\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0006\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0499\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0323\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0315\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0435\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0085\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0032\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0379\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0063\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0218\n",
            "\n",
            "Processing with augmentation: /content/drive/MyDrive/database_nifti/patient0009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading test patients:   0%|          | 0/75 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0029\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0487\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0296\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0160\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0135\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0317\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0080\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0411\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0123\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0289\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0113\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0292\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0491\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0233\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0338\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0040\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0356\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0273\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0229\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0211\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0185\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0180\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0150\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0027\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0260\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0406\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0389\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading test patients: 100%|██████████| 75/75 [00:00<00:00, 2717.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0336\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0337\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0120\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0148\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0186\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0011\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0243\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0003\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0259\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0159\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0153\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0137\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0121\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0496\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0363\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0258\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0076\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0451\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0102\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0306\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0446\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0409\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0198\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0252\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0493\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0362\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0272\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0494\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0195\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0320\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0196\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0007\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0408\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0125\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0216\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0119\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0301\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0404\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0341\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0328\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0498\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0270\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0271\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0309\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0162\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0321\n",
            "\n",
            "Processing: /content/drive/MyDrive/database_nifti/patient0202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot have number of splits n_splits=10 greater than the number of samples: n_samples=0.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-c7bc734e5e35>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-e51dd66ab999>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfold_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n=== Fold {fold + 1}/{N_FOLDS} ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mX_fold_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    405\u001b[0m                 (\n\u001b[1;32m    406\u001b[0m                     \u001b[0;34m\"Cannot have number of splits n_splits={0} greater\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot have number of splits n_splits=10 greater than the number of samples: n_samples=0."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "import nibabel as nib\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "from skimage.transform import resize\n",
        "from tqdm import tqdm\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "from skimage.morphology import erosion, dilation, square\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.ndimage import rotate\n",
        "from skimage.transform import rescale\n",
        "\n",
        "# Configuration\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 256\n",
        "IMG_CHANNELS = 1\n",
        "BATCH_SIZE = 8  # U-Net 1 uses 8, U-Net 2 uses 4\n",
        "EPOCHS = 100\n",
        "INIT_LR = 1e-4\n",
        "N_FOLDS = 10  # 10-fold cross-validation as in paper\n",
        "SEED = 42\n",
        "\n",
        "# Path to CAMUS dataset\n",
        "DATA_PATH = \"/content/drive/MyDrive/database_nifti\"\n",
        "\n",
        "# Define metrics\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1 - dice_coef(y_true, y_pred)\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n",
        "# Data loading and preprocessing\n",
        "def load_nifti_image(file_path):\n",
        "    img = nib.load(file_path)\n",
        "    data = img.get_fdata()\n",
        "    return np.squeeze(data)  # Remove singleton dimensions\n",
        "\n",
        "def preprocess_patient_with_augmentation(patient_folder):\n",
        "    images = []\n",
        "    masks_endo = []\n",
        "    masks_epi = []\n",
        "    masks_la = []\n",
        "\n",
        "    views = ['2CH', '4CH']\n",
        "    time_points = ['ED', 'ES']\n",
        "\n",
        "    for view in views:\n",
        "        for tp in time_points:\n",
        "            base_name = os.path.basename(patient_folder)\n",
        "            img_path = f\"{patient_folder}/{base_name}_{view}_{tp}.nii.gz\"\n",
        "            gt_path = f\"{patient_folder}/{base_name}_{view}_{tp}_gt.nii.gz\"\n",
        "\n",
        "            if not os.path.exists(img_path) or not os.path.exists(gt_path):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                img = nib.load(img_path).get_fdata()\n",
        "                gt = nib.load(gt_path).get_fdata()\n",
        "\n",
        "                img_resized = resize(img, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=True)\n",
        "                gt_resized = resize(gt, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=False)\n",
        "\n",
        "                img_resized = (img_resized - img_resized.min()) / (img_resized.max() - img_resized.min())\n",
        "\n",
        "                # Augmentation: Apply rotations and scalings\n",
        "                for angle in [0, 90, 180, 270]:\n",
        "                    img_rotated = rotate(img_resized, angle, reshape=False)\n",
        "                    gt_rotated = rotate(gt_resized, angle, reshape=False)\n",
        "\n",
        "                    for scale in [0.9, 1.0, 1.1]:\n",
        "                        img_scaled = rescale(img_rotated, scale, preserve_range=True, anti_aliasing=True)\n",
        "                        gt_scaled = rescale(gt_rotated, scale, preserve_range=True, anti_aliasing=False)\n",
        "\n",
        "                        # Ensure the scaled images are resized back to original dimensions\n",
        "                        img_scaled_resized = resize(img_scaled, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=True)\n",
        "                        gt_scaled_resized = resize(gt_scaled, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=False)\n",
        "\n",
        "                        mask_endo = (gt_scaled_resized == 1).astype(np.float32)\n",
        "                        mask_epi = (gt_scaled_resized == 2).astype(np.float32)\n",
        "                        mask_la = (gt_scaled_resized == 3).astype(np.float32)\n",
        "\n",
        "                        images.append(img_scaled_resized[..., np.newaxis])\n",
        "                        masks_endo.append(mask_endo[..., np.newaxis])\n",
        "                        masks_epi.append(mask_epi[..., np.newaxis])\n",
        "                        masks_la.append(mask_la[..., np.newaxis])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {view}_{tp}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    return np.array(images), np.array(masks_endo), np.array(masks_epi), np.array(masks_la)\n",
        "\n",
        "def preprocess_patient(patient_folder):\n",
        "    images = []\n",
        "    masks_endo = []\n",
        "    masks_epi = []\n",
        "    masks_la = []\n",
        "\n",
        "    views = ['2CH', '4CH']\n",
        "    time_points = ['ED', 'ES']\n",
        "\n",
        "    for view in views:\n",
        "        for tp in time_points:\n",
        "            base_name = os.path.basename(patient_folder)\n",
        "            img_path = f\"{patient_folder}/{base_name}_{view}_{tp}.nii.gz\"\n",
        "            gt_path = f\"{patient_folder}/{base_name}_{view}_{tp}_gt.nii.gz\"\n",
        "\n",
        "            if not os.path.exists(img_path) or not os.path.exists(gt_path):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                img = nib.load(img_path).get_fdata()\n",
        "                gt = nib.load(gt_path).get_fdata()\n",
        "\n",
        "                img_resized = resize(img, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=True)\n",
        "                gt_resized = resize(gt, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=False)\n",
        "\n",
        "                img_resized = (img_resized - img_resized.min()) / (img_resized.max() - img_resized.min())\n",
        "\n",
        "                mask_endo = (gt_resized == 1).astype(np.float32)\n",
        "                mask_epi = (gt_resized == 2).astype(np.float32)\n",
        "                mask_la = (gt_resized == 3).astype(np.float32)\n",
        "\n",
        "                images.append(img_resized[..., np.newaxis])\n",
        "                masks_endo.append(mask_endo[..., np.newaxis])\n",
        "                masks_epi.append(mask_epi[..., np.newaxis])\n",
        "                masks_la.append(mask_la[..., np.newaxis])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {view}_{tp}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    return np.array(images), np.array(masks_endo), np.array(masks_epi), np.array(masks_la)\n",
        "\n",
        "def load_dataset_with_split(base_path, test_ratio=0.15):\n",
        "    patient_folders = sorted([\n",
        "        os.path.join(base_path, f)\n",
        "        for f in os.listdir(base_path)\n",
        "        if f.startswith('patient') and os.path.isdir(os.path.join(base_path, f))\n",
        "    ])\n",
        "\n",
        "    # Verify we found patient folders\n",
        "    if not patient_folders:\n",
        "        raise ValueError(f\"No patient folders found in {base_path}. Check your DATA_PATH.\")\n",
        "\n",
        "    # Split into train/test\n",
        "    np.random.seed(SEED)\n",
        "    np.random.shuffle(patient_folders)\n",
        "    split_idx = int(len(patient_folders) * (1 - test_ratio))\n",
        "    train_folders = patient_folders[:split_idx]\n",
        "    test_folders = patient_folders[split_idx:]\n",
        "\n",
        "    print(f\"Found {len(patient_folders)} patients. Using {len(train_folders)} for training, {len(test_folders)} for testing.\")\n",
        "\n",
        "    # Preprocess train data with augmentation\n",
        "    all_images_train, all_masks_endo_train, all_masks_epi_train, all_masks_la_train = [], [], [], []\n",
        "\n",
        "    for patient_folder in tqdm(train_folders, desc=\"Loading training patients\"):\n",
        "        images, masks_endo, masks_epi, masks_la = preprocess_patient_with_augmentation(patient_folder)\n",
        "        if len(images) > 0:  # Only add if we got data\n",
        "            all_images_train.append(images)\n",
        "            all_masks_endo_train.append(masks_endo)\n",
        "            all_masks_epi_train.append(masks_epi)\n",
        "            all_masks_la_train.append(masks_la)\n",
        "\n",
        "    # Preprocess test data without augmentation\n",
        "    all_images_test, all_masks_endo_test, all_masks_epi_test, all_masks_la_test = [], [], [], []\n",
        "\n",
        "    for patient_folder in tqdm(test_folders, desc=\"Loading test patients\"):\n",
        "        images, masks_endo, masks_epi, masks_la = preprocess_patient(patient_folder)\n",
        "        if len(images) > 0:  # Only add if we got data\n",
        "            all_images_test.append(images)\n",
        "            all_masks_endo_test.append(masks_endo)\n",
        "            all_masks_epi_test.append(masks_epi)\n",
        "            all_masks_la_test.append(masks_la)\n",
        "\n",
        "    # Concatenate all data\n",
        "    X_train = np.concatenate(all_images_train, axis=0) if all_images_train else np.array([])\n",
        "    y_endo_train = np.concatenate(all_masks_endo_train, axis=0) if all_masks_endo_train else np.array([])\n",
        "    y_epi_train = np.concatenate(all_masks_epi_train, axis=0) if all_masks_epi_train else np.array([])\n",
        "    y_la_train = np.concatenate(all_masks_la_train, axis=0) if all_masks_la_train else np.array([])\n",
        "\n",
        "    X_test = np.concatenate(all_images_test, axis=0) if all_images_test else np.array([])\n",
        "    y_endo_test = np.concatenate(all_masks_endo_test, axis=0) if all_masks_endo_test else np.array([])\n",
        "    y_epi_test = np.concatenate(all_masks_epi_test, axis=0) if all_masks_epi_test else np.array([])\n",
        "    y_la_test = np.concatenate(all_masks_la_test, axis=0) if all_masks_la_test else np.array([])\n",
        "\n",
        "    # Verify we have data\n",
        "    if len(X_train) == 0:\n",
        "        raise ValueError(\"No training data was loaded. Check your data paths and preprocessing.\")\n",
        "    if len(X_test) == 0:\n",
        "        print(\"Warning: No test data was loaded.\")\n",
        "\n",
        "    print(f\"\\nTraining data shape: {X_train.shape}\")\n",
        "    print(f\"Test data shape: {X_test.shape}\")\n",
        "\n",
        "    return X_train, y_endo_train, y_epi_train, y_la_train, X_test, y_endo_test, y_epi_test, y_la_test\n",
        "\n",
        "# U-Net architectures (same as before)\n",
        "def unet1(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Downsampling path\n",
        "    conv1 = Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(32, 3, activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(64, 3, activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, 3, activation='relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(128, 3, activation='relu', padding='same')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, 3, activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(256, 3, activation='relu', padding='same')(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    # Bottom\n",
        "    conv5 = Conv2D(512, 3, activation='relu', padding='same')(pool4)\n",
        "    conv5 = Conv2D(512, 3, activation='relu', padding='same')(conv5)\n",
        "\n",
        "    # Upsampling path\n",
        "    up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=-1)\n",
        "    conv6 = Conv2D(256, 3, activation='relu', padding='same')(up6)\n",
        "    conv6 = Conv2D(256, 3, activation='relu', padding='same')(conv6)\n",
        "\n",
        "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=-1)\n",
        "    conv7 = Conv2D(128, 3, activation='relu', padding='same')(up7)\n",
        "    conv7 = Conv2D(128, 3, activation='relu', padding='same')(conv7)\n",
        "\n",
        "    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=-1)\n",
        "    conv8 = Conv2D(64, 3, activation='relu', padding='same')(up8)\n",
        "    conv8 = Conv2D(64, 3, activation='relu', padding='same')(conv8)\n",
        "\n",
        "    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=-1)\n",
        "    conv9 = Conv2D(32, 3, activation='relu', padding='same')(up9)\n",
        "    conv9 = Conv2D(32, 3, activation='relu', padding='same')(conv9)\n",
        "\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def unet2(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Downsampling path\n",
        "    conv1 = Conv2D(64, 3, padding='same')(inputs)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Activation('relu')(conv1)\n",
        "    conv1 = Conv2D(64, 3, padding='same')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Activation('relu')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(128, 3, padding='same')(pool1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Activation('relu')(conv2)\n",
        "    conv2 = Conv2D(128, 3, padding='same')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Activation('relu')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(256, 3, padding='same')(pool2)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Activation('relu')(conv3)\n",
        "    conv3 = Conv2D(256, 3, padding='same')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Activation('relu')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(512, 3, padding='same')(pool3)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Activation('relu')(conv4)\n",
        "    conv4 = Conv2D(512, 3, padding='same')(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Activation('relu')(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    # Bottom\n",
        "    conv5 = Conv2D(1024, 3, padding='same')(pool4)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Activation('relu')(conv5)\n",
        "    conv5 = Conv2D(1024, 3, padding='same')(conv5)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Activation('relu')(conv5)\n",
        "\n",
        "    # Upsampling path\n",
        "    up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=-1)\n",
        "    conv6 = Conv2D(512, 3, padding='same')(up6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Activation('relu')(conv6)\n",
        "    conv6 = Conv2D(512, 3, padding='same')(conv6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Activation('relu')(conv6)\n",
        "\n",
        "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=-1)\n",
        "    conv7 = Conv2D(256, 3, padding='same')(up7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Activation('relu')(conv7)\n",
        "    conv7 = Conv2D(256, 3, padding='same')(conv7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Activation('relu')(conv7)\n",
        "\n",
        "    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=-1)\n",
        "    conv8 = Conv2D(128, 3, padding='same')(up8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "    conv8 = Activation('relu')(conv8)\n",
        "    conv8 = Conv2D(128, 3, padding='same')(conv8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "    conv8 = Activation('relu')(conv8)\n",
        "\n",
        "    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=-1)\n",
        "    conv9 = Conv2D(64, 3, padding='same')(up9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "    conv9 = Activation('relu')(conv9)\n",
        "    conv9 = Conv2D(64, 3, padding='same')(conv9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "    conv9 = Activation('relu')(conv9)\n",
        "\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Evaluation metrics (same as before)\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    y_pred = (y_pred > 0.5).astype(np.float32)\n",
        "\n",
        "    # Dice coefficient\n",
        "    intersection = np.sum(y_true * y_pred)\n",
        "    dice = (2. * intersection + 1.) / (np.sum(y_true) + np.sum(y_pred) + 1.)\n",
        "\n",
        "    # Mean absolute distance\n",
        "    dt_true = distance_transform_edt(1 - y_true.squeeze())\n",
        "    dt_pred = distance_transform_edt(1 - y_pred.squeeze())\n",
        "    mean_dist = (np.mean(dt_pred[y_true.squeeze() > 0.5]) +\n",
        "                 np.mean(dt_true[y_pred.squeeze() > 0.5])) / 2\n",
        "\n",
        "    # Hausdorff distance (approximation)\n",
        "    contour_true = y_true.squeeze() - erosion(y_true.squeeze(), square(3))\n",
        "    contour_pred = y_pred.squeeze() - erosion(y_pred.squeeze(), square(3))\n",
        "\n",
        "    if np.sum(contour_true) == 0 or np.sum(contour_pred) == 0:\n",
        "        hausdorff = np.inf\n",
        "    else:\n",
        "        dt_true_contour = distance_transform_edt(1 - contour_true)\n",
        "        dt_pred_contour = distance_transform_edt(1 - contour_pred)\n",
        "        hd_true = np.max(contour_pred * dt_true_contour)\n",
        "        hd_pred = np.max(contour_true * dt_pred_contour)\n",
        "        hausdorff = max(hd_true, hd_pred)\n",
        "\n",
        "    return dice, mean_dist, hausdorff\n",
        "\n",
        "def main():\n",
        "    print(\"Loading and preprocessing dataset...\")\n",
        "    try:\n",
        "        (X_train, y_endo_train, y_epi_train, y_la_train,\n",
        "         X_test, y_endo_test, y_epi_test, y_la_test) = load_dataset_with_split(DATA_PATH)\n",
        "\n",
        "        # Verify we have data\n",
        "        if len(X_train) == 0:\n",
        "            raise ValueError(\"No training data available after loading.\")\n",
        "\n",
        "        print(f\"\\nTraining on {len(X_train)} samples\")\n",
        "        if len(X_test) > 0:\n",
        "            print(f\"Testing on {len(X_test)} samples\")\n",
        "\n",
        "        # Choose which U-Net to use\n",
        "        unet_version = 1\n",
        "        batch_size = 8 if unet_version == 1 else 4\n",
        "\n",
        "        # Adjust number of folds if we don't have enough samples\n",
        "        actual_folds = min(N_FOLDS, len(X_train))\n",
        "        if actual_folds < N_FOLDS:\n",
        "            print(f\"Reducing number of folds from {N_FOLDS} to {actual_folds} due to limited samples\")\n",
        "\n",
        "        kf = KFold(n_splits=actual_folds, shuffle=True, random_state=SEED)\n",
        "        fold_results = []\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "            print(f\"\\n=== Fold {fold + 1}/{actual_folds} ===\")\n",
        "            X_fold_train, X_val = X_train[train_idx], X_train[val_idx]\n",
        "            y_fold_train, y_val = y_endo_train[train_idx], y_endo_train[val_idx]\n",
        "\n",
        "            model = unet1() if unet_version == 1 else unet2()\n",
        "            model.compile(optimizer=Adam(learning_rate=INIT_LR),\n",
        "                          loss=bce_dice_loss,\n",
        "                          metrics=[dice_coef, 'accuracy'])\n",
        "\n",
        "            callbacks = [\n",
        "                ModelCheckpoint(f\"unet{unet_version}_fold{fold}_best.keras\",\n",
        "                                monitor='val_dice_coef',\n",
        "                                mode='max',\n",
        "                                save_best_only=True,\n",
        "                                verbose=1),\n",
        "                EarlyStopping(monitor='val_dice_coef', patience=15, mode='max', verbose=1),\n",
        "                ReduceLROnPlateau(monitor='val_dice_coef', factor=0.5, patience=5, min_lr=1e-6, mode='max', verbose=1)\n",
        "            ]\n",
        "\n",
        "            print(f\"Training on {len(X_fold_train)} samples, validating on {len(X_val)} samples\")\n",
        "            history = model.fit(\n",
        "                X_fold_train, y_fold_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=EPOCHS,\n",
        "                validation_data=(X_val, y_val),\n",
        "                callbacks=callbacks,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            model.load_weights(f\"unet{unet_version}_fold{fold}_best.keras\")\n",
        "\n",
        "            # Test on one random sample from test set if available\n",
        "            if len(X_test) > 0:\n",
        "                idx = np.random.randint(0, len(X_test))\n",
        "                X_sample, y_sample = X_test[idx:idx+1], y_endo_test[idx:idx+1]\n",
        "                y_pred_sample = model.predict(X_sample, batch_size=1)\n",
        "\n",
        "                plt.figure(figsize=(12, 4))\n",
        "                plt.subplot(1, 3, 1)\n",
        "                plt.imshow(X_sample[0].squeeze(), cmap='gray')\n",
        "                plt.title(\"Original Image\")\n",
        "\n",
        "                plt.subplot(1, 3, 2)\n",
        "                plt.imshow(y_sample[0].squeeze(), cmap='gray')\n",
        "                plt.title(\"Ground Truth\")\n",
        "\n",
        "                plt.subplot(1, 3, 3)\n",
        "                plt.imshow(y_pred_sample[0].squeeze() > 0.5, cmap='gray')\n",
        "                plt.title(\"Predicted Mask\")\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                # Collect fold results\n",
        "                dice, mean_dist, hausdorff = calculate_metrics(y_sample[0], y_pred_sample[0] > 0.5)\n",
        "                fold_results.append({\n",
        "                    'dice': dice,\n",
        "                    'mean_dist': mean_dist,\n",
        "                    'hausdorff': hausdorff,\n",
        "                })\n",
        "\n",
        "        # Print final cross-validation results if we have any\n",
        "        if fold_results:\n",
        "            print(\"\\n=== Final Cross-Validation Results ===\")\n",
        "            avg_dice = np.mean([r['dice'] for r in fold_results])\n",
        "            avg_mean_dist = np.mean([r['mean_dist'] for r in fold_results])\n",
        "            avg_hausdorff = np.mean([r['hausdorff'] for r in fold_results])\n",
        "\n",
        "            print(f\"Average Dice: {avg_dice:.3f}\")\n",
        "            print(f\"Average Mean Absolute Distance: {avg_mean_dist:.3f} mm\")\n",
        "            print(f\"Average Hausdorff Distance: {avg_hausdorff:.3f} mm\")\n",
        "        else:\n",
        "            print(\"\\nNo fold results to report (possibly no test data available)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "fWc-Rnwpo8mc",
        "outputId": "2e8e153f-5243-465a-91a2-540140e90528"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading and preprocessing dataset...\n",
            "Found 500 patients. Using 425 for training, 75 for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading training patients:  28%|██▊       | 118/425 [07:10<18:40,  3.65s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7288a1ac22b0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-7288a1ac22b0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         (X_train, y_endo_train, y_epi_train, y_la_train,\n\u001b[0;32m--> 375\u001b[0;31m          X_test, y_endo_test, y_epi_test, y_la_test) = load_dataset_with_split(DATA_PATH)\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# Verify we have data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-7288a1ac22b0>\u001b[0m in \u001b[0;36mload_dataset_with_split\u001b[0;34m(base_path, test_ratio)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpatient_folder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_folders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Loading training patients\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_endo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_epi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_la\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_patient_with_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatient_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only add if we got data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mall_images_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-7288a1ac22b0>\u001b[0m in \u001b[0;36mpreprocess_patient_with_augmentation\u001b[0;34m(patient_folder)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nibabel/loadsave.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0msniff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_klass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_image_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mis_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msniff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_klass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_maybe_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msniff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_valid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_klass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nibabel/filebasedimages.py\u001b[0m in \u001b[0;36mpath_maybe_image\u001b[0;34m(klass, filename, sniff, sniff_max)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msniff\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msniff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta_sniff_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0msniff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0msniff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sniff_meta_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta_sniff_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msniff_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msniff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msniff\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msniff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta_sniff_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msniff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nibabel/filebasedimages.py\u001b[0m in \u001b[0;36m_sniff_meta_for\u001b[0;34m(klass, filename, sniff_nbytes, sniff)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mImageOpener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                 \u001b[0mbinaryblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msniff_nbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCOMPRESSION_ERRORS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nibabel/openers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWriteableBuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read() on write-only GzipFile object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    497\u001b[0m                 \u001b[0;31m# jump to the next member, if there is one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_gzip_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/gzip.py\u001b[0m in \u001b[0;36m_read_gzip_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_gzip_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0mlast_mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_gzip_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlast_mtime\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/gzip.py\u001b[0m in \u001b[0;36m_read_gzip_header\u001b[0;34m(fp)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0mlast\u001b[0m \u001b[0mmtime\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mheader\u001b[0m \u001b[0mwas\u001b[0m \u001b[0mpresent\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     '''\n\u001b[0;32m--> 423\u001b[0;31m     \u001b[0mmagic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here starts the U-net code with the new split imported form kaggle\n"
      ],
      "metadata": {
        "id": "6CkAa29qJvQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "import nibabel as nib\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "from skimage.transform import resize\n",
        "from tqdm import tqdm\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "from skimage.morphology import erosion, dilation, square\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.ndimage import rotate\n",
        "from skimage.transform import rescale"
      ],
      "metadata": {
        "id": "2b09I0qrJ4zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 256\n",
        "IMG_CHANNELS = 1\n",
        "BATCH_SIZE = 4  # U-Net 1 uses 8, U-Net 2 uses 4\n",
        "EPOCHS = 100\n",
        "INIT_LR = 1e-4\n",
        "N_FOLDS = 10  # 10-fold cross-validation as in paper\n",
        "SEED = 42\n",
        "\n",
        "# Path to CAMUS dataset\n",
        "DATA_PATH = \"/content/drive/MyDrive/database_nifti\"\n"
      ],
      "metadata": {
        "id": "CwYayKDpJ6dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define metrics\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1 - dice_coef(y_true, y_pred)\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n",
        "# Data loading and preprocessing\n",
        "def load_nifti_image(file_path):\n",
        "    img = nib.load(file_path)\n",
        "    data = img.get_fdata()\n",
        "    return np.squeeze(data)  # Remove singleton dimensions\n",
        "\n",
        "def preprocess_patient_with_augmentation(patient_folder):\n",
        "    images = []\n",
        "    masks_endo = []\n",
        "    masks_epi = []\n",
        "    masks_la = []\n",
        "\n",
        "    views = ['2CH', '4CH']\n",
        "    time_points = ['ED', 'ES']\n",
        "\n",
        "    for view in views:\n",
        "        for tp in time_points:\n",
        "            base_name = os.path.basename(patient_folder)\n",
        "            img_path = f\"{patient_folder}/{base_name}_{view}_{tp}.nii.gz\"\n",
        "            gt_path = f\"{patient_folder}/{base_name}_{view}_{tp}_gt.nii.gz\"\n",
        "\n",
        "            if not os.path.exists(img_path) or not os.path.exists(gt_path):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                img = nib.load(img_path).get_fdata()\n",
        "                gt = nib.load(gt_path).get_fdata()\n",
        "\n",
        "                img_resized = resize(img, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=True)\n",
        "                gt_resized = resize(gt, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=False)\n",
        "\n",
        "                img_resized = (img_resized - img_resized.min()) / (img_resized.max() - img_resized.min())\n",
        "\n",
        "                # Augmentation: Apply rotations and scalings\n",
        "                for angle in [0, 90, 180, 270]:\n",
        "                    img_rotated = rotate(img_resized, angle, reshape=False)\n",
        "                    gt_rotated = rotate(gt_resized, angle, reshape=False)\n",
        "\n",
        "                    for scale in [0.9, 1.0, 1.1]:\n",
        "                        img_scaled = rescale(img_rotated, scale, preserve_range=True, anti_aliasing=True)\n",
        "                        gt_scaled = rescale(gt_rotated, scale, preserve_range=True, anti_aliasing=False)\n",
        "\n",
        "                        # Ensure the scaled images are resized back to original dimensions\n",
        "                        img_scaled_resized = resize(img_scaled, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=True)\n",
        "                        gt_scaled_resized = resize(gt_scaled, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=False)\n",
        "\n",
        "                        mask_endo = (gt_scaled_resized == 1).astype(np.float32)\n",
        "                        mask_epi = (gt_scaled_resized == 2).astype(np.float32)\n",
        "                        mask_la = (gt_scaled_resized == 3).astype(np.float32)\n",
        "\n",
        "                        images.append(img_scaled_resized[..., np.newaxis])\n",
        "                        masks_endo.append(mask_endo[..., np.newaxis])\n",
        "                        masks_epi.append(mask_epi[..., np.newaxis])\n",
        "                        masks_la.append(mask_la[..., np.newaxis])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {view}_{tp}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    return np.array(images), np.array(masks_endo), np.array(masks_epi), np.array(masks_la)\n",
        "\n",
        "def preprocess_patient(patient_folder):\n",
        "    images = []\n",
        "    masks_endo = []\n",
        "    masks_epi = []\n",
        "    masks_la = []\n",
        "\n",
        "    views = ['2CH', '4CH']\n",
        "    time_points = ['ED', 'ES']\n",
        "\n",
        "    for view in views:\n",
        "        for tp in time_points:\n",
        "            base_name = os.path.basename(patient_folder)\n",
        "            img_path = f\"{patient_folder}/{base_name}_{view}_{tp}.nii.gz\"\n",
        "            gt_path = f\"{patient_folder}/{base_name}_{view}_{tp}_gt.nii.gz\"\n",
        "\n",
        "            if not os.path.exists(img_path) or not os.path.exists(gt_path):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                img = nib.load(img_path).get_fdata()\n",
        "                gt = nib.load(gt_path).get_fdata()\n",
        "\n",
        "                img_resized = resize(img, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=True)\n",
        "                gt_resized = resize(gt, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=False)\n",
        "\n",
        "                img_resized = (img_resized - img_resized.min()) / (img_resized.max() - img_resized.min())\n",
        "\n",
        "                mask_endo = (gt_resized == 1).astype(np.float32)\n",
        "                mask_epi = (gt_resized == 2).astype(np.float32)\n",
        "                mask_la = (gt_resized == 3).astype(np.float32)\n",
        "\n",
        "                images.append(img_resized[..., np.newaxis])\n",
        "                masks_endo.append(mask_endo[..., np.newaxis])\n",
        "                masks_epi.append(mask_epi[..., np.newaxis])\n",
        "                masks_la.append(mask_la[..., np.newaxis])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {view}_{tp}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    return np.array(images), np.array(masks_endo), np.array(masks_epi), np.array(masks_la)\n",
        "\n",
        "def load_dataset_with_split(base_path, test_ratio=0.15):\n",
        "    patient_folders = sorted([\n",
        "        os.path.join(base_path, f)\n",
        "        for f in os.listdir(base_path)\n",
        "        if f.startswith('patient') and os.path.isdir(os.path.join(base_path, f))\n",
        "    ])\n",
        "\n",
        "    # Verify we found patient folders\n",
        "    if not patient_folders:\n",
        "        raise ValueError(f\"No patient folders found in {base_path}. Check your DATA_PATH.\")\n",
        "\n",
        "    # Split into train/test\n",
        "    np.random.seed(SEED)\n",
        "    np.random.shuffle(patient_folders)\n",
        "    split_idx = int(len(patient_folders) * (1 - test_ratio))\n",
        "    train_folders = patient_folders[:split_idx]\n",
        "    test_folders = patient_folders[split_idx:]\n",
        "\n",
        "    print(f\"Found {len(patient_folders)} patients. Using {len(train_folders)} for training, {len(test_folders)} for testing.\")\n",
        "\n",
        "    # Preprocess train data with augmentation\n",
        "    all_images_train, all_masks_endo_train, all_masks_epi_train, all_masks_la_train = [], [], [], []\n",
        "\n",
        "    for patient_folder in tqdm(train_folders, desc=\"Loading training patients\"):\n",
        "        images, masks_endo, masks_epi, masks_la = preprocess_patient_with_augmentation(patient_folder)\n",
        "        if len(images) > 0:  # Only add if we got data\n",
        "            all_images_train.append(images)\n",
        "            all_masks_endo_train.append(masks_endo)\n",
        "            all_masks_epi_train.append(masks_epi)\n",
        "            all_masks_la_train.append(masks_la)\n",
        "\n",
        "    # Preprocess test data without augmentation\n",
        "    all_images_test, all_masks_endo_test, all_masks_epi_test, all_masks_la_test = [], [], [], []\n",
        "\n",
        "    for patient_folder in tqdm(test_folders, desc=\"Loading test patients\"):\n",
        "        images, masks_endo, masks_epi, masks_la = preprocess_patient(patient_folder)\n",
        "        if len(images) > 0:  # Only add if we got data\n",
        "            all_images_test.append(images)\n",
        "            all_masks_endo_test.append(masks_endo)\n",
        "            all_masks_epi_test.append(masks_epi)\n",
        "            all_masks_la_test.append(masks_la)\n",
        "\n",
        "    # Concatenate all data\n",
        "    X_train = np.concatenate(all_images_train, axis=0) if all_images_train else np.array([])\n",
        "    y_endo_train = np.concatenate(all_masks_endo_train, axis=0) if all_masks_endo_train else np.array([])\n",
        "    y_epi_train = np.concatenate(all_masks_epi_train, axis=0) if all_masks_epi_train else np.array([])\n",
        "    y_la_train = np.concatenate(all_masks_la_train, axis=0) if all_masks_la_train else np.array([])\n",
        "\n",
        "    X_test = np.concatenate(all_images_test, axis=0) if all_images_test else np.array([])\n",
        "    y_endo_test = np.concatenate(all_masks_endo_test, axis=0) if all_masks_endo_test else np.array([])\n",
        "    y_epi_test = np.concatenate(all_masks_epi_test, axis=0) if all_masks_epi_test else np.array([])\n",
        "    y_la_test = np.concatenate(all_masks_la_test, axis=0) if all_masks_la_test else np.array([])\n",
        "\n",
        "    # Verify we have data\n",
        "    if len(X_train) == 0:\n",
        "        raise ValueError(\"No training data was loaded. Check your data paths and preprocessing.\")\n",
        "    if len(X_test) == 0:\n",
        "        print(\"Warning: No test data was loaded.\")\n",
        "\n",
        "    print(f\"\\nTraining data shape: {X_train.shape}\")\n",
        "    print(f\"Test data shape: {X_test.shape}\")\n",
        "\n",
        "    return X_train, y_endo_train, y_epi_train, y_la_train, X_test, y_endo_test, y_epi_test, y_la_test\n"
      ],
      "metadata": {
        "id": "hRPnO_ehKQgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# U-Net architectures (same as before)\n",
        "def unet1(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Downsampling path\n",
        "    conv1 = Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(32, 3, activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(64, 3, activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, 3, activation='relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(128, 3, activation='relu', padding='same')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, 3, activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(256, 3, activation='relu', padding='same')(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    # Bottom\n",
        "    conv5 = Conv2D(512, 3, activation='relu', padding='same')(pool4)\n",
        "    conv5 = Conv2D(512, 3, activation='relu', padding='same')(conv5)\n",
        "\n",
        "    # Upsampling path\n",
        "    up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=-1)\n",
        "    conv6 = Conv2D(256, 3, activation='relu', padding='same')(up6)\n",
        "    conv6 = Conv2D(256, 3, activation='relu', padding='same')(conv6)\n",
        "\n",
        "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=-1)\n",
        "    conv7 = Conv2D(128, 3, activation='relu', padding='same')(up7)\n",
        "    conv7 = Conv2D(128, 3, activation='relu', padding='same')(conv7)\n",
        "\n",
        "    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=-1)\n",
        "    conv8 = Conv2D(64, 3, activation='relu', padding='same')(up8)\n",
        "    conv8 = Conv2D(64, 3, activation='relu', padding='same')(conv8)\n",
        "\n",
        "    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=-1)\n",
        "    conv9 = Conv2D(32, 3, activation='relu', padding='same')(up9)\n",
        "    conv9 = Conv2D(32, 3, activation='relu', padding='same')(conv9)\n",
        "\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def unet2(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Downsampling path\n",
        "    conv1 = Conv2D(64, 3, padding='same')(inputs)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Activation('relu')(conv1)\n",
        "    conv1 = Conv2D(64, 3, padding='same')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Activation('relu')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(128, 3, padding='same')(pool1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Activation('relu')(conv2)\n",
        "    conv2 = Conv2D(128, 3, padding='same')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Activation('relu')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(256, 3, padding='same')(pool2)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Activation('relu')(conv3)\n",
        "    conv3 = Conv2D(256, 3, padding='same')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Activation('relu')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(512, 3, padding='same')(pool3)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Activation('relu')(conv4)\n",
        "    conv4 = Conv2D(512, 3, padding='same')(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Activation('relu')(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    # Bottom\n",
        "    conv5 = Conv2D(1024, 3, padding='same')(pool4)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Activation('relu')(conv5)\n",
        "    conv5 = Conv2D(1024, 3, padding='same')(conv5)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Activation('relu')(conv5)\n",
        "\n",
        "    # Upsampling path\n",
        "    up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=-1)\n",
        "    conv6 = Conv2D(512, 3, padding='same')(up6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Activation('relu')(conv6)\n",
        "    conv6 = Conv2D(512, 3, padding='same')(conv6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Activation('relu')(conv6)\n",
        "\n",
        "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=-1)\n",
        "    conv7 = Conv2D(256, 3, padding='same')(up7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Activation('relu')(conv7)\n",
        "    conv7 = Conv2D(256, 3, padding='same')(conv7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Activation('relu')(conv7)\n",
        "\n",
        "    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=-1)\n",
        "    conv8 = Conv2D(128, 3, padding='same')(up8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "    conv8 = Activation('relu')(conv8)\n",
        "    conv8 = Conv2D(128, 3, padding='same')(conv8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "    conv8 = Activation('relu')(conv8)\n",
        "\n",
        "    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=-1)\n",
        "    conv9 = Conv2D(64, 3, padding='same')(up9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "    conv9 = Activation('relu')(conv9)\n",
        "    conv9 = Conv2D(64, 3, padding='same')(conv9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "    conv9 = Activation('relu')(conv9)\n",
        "\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "lZCW26KcKUs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics (same as before)\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    y_pred = (y_pred > 0.5).astype(np.float32)\n",
        "\n",
        "    # Dice coefficient\n",
        "    intersection = np.sum(y_true * y_pred)\n",
        "    dice = (2. * intersection + 1.) / (np.sum(y_true) + np.sum(y_pred) + 1.)\n",
        "\n",
        "    # Mean absolute distance\n",
        "    dt_true = distance_transform_edt(1 - y_true.squeeze())\n",
        "    dt_pred = distance_transform_edt(1 - y_pred.squeeze())\n",
        "    mean_dist = (np.mean(dt_pred[y_true.squeeze() > 0.5]) +\n",
        "                 np.mean(dt_true[y_pred.squeeze() > 0.5])) / 2\n",
        "\n",
        "    # Hausdorff distance (approximation)\n",
        "    contour_true = y_true.squeeze() - erosion(y_true.squeeze(), square(3))\n",
        "    contour_pred = y_pred.squeeze() - erosion(y_pred.squeeze(), square(3))\n",
        "\n",
        "    if np.sum(contour_true) == 0 or np.sum(contour_pred) == 0:\n",
        "        hausdorff = np.inf\n",
        "    else:\n",
        "        dt_true_contour = distance_transform_edt(1 - contour_true)\n",
        "        dt_pred_contour = distance_transform_edt(1 - contour_pred)\n",
        "        hd_true = np.max(contour_pred * dt_true_contour)\n",
        "        hd_pred = np.max(contour_true * dt_pred_contour)\n",
        "        hausdorff = max(hd_true, hd_pred)\n",
        "\n",
        "    return dice, mean_dist, hausdorff"
      ],
      "metadata": {
        "id": "1v6BnC1iKY3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"Loading and preprocessing dataset...\")\n",
        "    try:\n",
        "        (X_train, y_endo_train, y_epi_train, y_la_train,\n",
        "         X_test, y_endo_test, y_epi_test, y_la_test) = load_dataset_with_split(DATA_PATH)\n",
        "\n",
        "        # Verify we have data\n",
        "        if len(X_train) == 0:\n",
        "            raise ValueError(\"No training data available after loading.\")\n",
        "\n",
        "        print(f\"\\nTraining on {len(X_train)} samples\")\n",
        "        if len(X_test) > 0:\n",
        "            print(f\"Testing on {len(X_test)} samples\")\n",
        "\n",
        "        # Choose which U-Net to use\n",
        "        unet_version = 1\n",
        "        batch_size = 8 if unet_version == 1 else 4\n",
        "\n",
        "        # Adjust number of folds if we don't have enough samples\n",
        "        actual_folds = min(N_FOLDS, len(X_train))\n",
        "        if actual_folds < N_FOLDS:\n",
        "            print(f\"Reducing number of folds from {N_FOLDS} to {actual_folds} due to limited samples\")\n",
        "\n",
        "        kf = KFold(n_splits=actual_folds, shuffle=True, random_state=SEED)\n",
        "        fold_results = []\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "            print(f\"\\n=== Fold {fold + 1}/{actual_folds} ===\")\n",
        "            X_fold_train, X_val = X_train[train_idx], X_train[val_idx]\n",
        "            y_fold_train, y_val = y_endo_train[train_idx], y_endo_train[val_idx]\n",
        "\n",
        "            model = unet1() if unet_version == 1 else unet2()\n",
        "            model.compile(optimizer=Adam(learning_rate=INIT_LR),\n",
        "                          loss=bce_dice_loss,\n",
        "                          metrics=[dice_coef, 'accuracy'])\n",
        "\n",
        "            callbacks = [\n",
        "                ModelCheckpoint(f\"unet{unet_version}_fold{fold}_best.keras\",\n",
        "                                monitor='val_dice_coef',\n",
        "                                mode='max',\n",
        "                                save_best_only=True,\n",
        "                                verbose=1),\n",
        "                EarlyStopping(monitor='val_dice_coef', patience=15, mode='max', verbose=1),\n",
        "                ReduceLROnPlateau(monitor='val_dice_coef', factor=0.5, patience=5, min_lr=1e-6, mode='max', verbose=1)\n",
        "            ]\n",
        "\n",
        "            print(f\"Training on {len(X_fold_train)} samples, validating on {len(X_val)} samples\")\n",
        "            history = model.fit(\n",
        "                X_fold_train, y_fold_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=EPOCHS,\n",
        "                validation_data=(X_val, y_val),\n",
        "                callbacks=callbacks,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            model.load_weights(f\"unet{unet_version}_fold{fold}_best.keras\")\n",
        "\n",
        "            # Test on one random sample from test set if available\n",
        "            if len(X_test) > 0:\n",
        "                idx = np.random.randint(0, len(X_test))\n",
        "                X_sample, y_sample = X_test[idx:idx+1], y_endo_test[idx:idx+1]\n",
        "                y_pred_sample = model.predict(X_sample, batch_size=1)\n",
        "\n",
        "                plt.figure(figsize=(12, 4))\n",
        "                plt.subplot(1, 3, 1)\n",
        "                plt.imshow(X_sample[0].squeeze(), cmap='gray')\n",
        "                plt.title(\"Original Image\")\n",
        "\n",
        "                plt.subplot(1, 3, 2)\n",
        "                plt.imshow(y_sample[0].squeeze(), cmap='gray')\n",
        "                plt.title(\"Ground Truth\")\n",
        "\n",
        "                plt.subplot(1, 3, 3)\n",
        "                plt.imshow(y_pred_sample[0].squeeze() > 0.5, cmap='gray')\n",
        "                plt.title(\"Predicted Mask\")\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                # Collect fold results\n",
        "                dice, mean_dist, hausdorff = calculate_metrics(y_sample[0], y_pred_sample[0] > 0.5)\n",
        "                fold_results.append({\n",
        "                    'dice': dice,\n",
        "                    'mean_dist': mean_dist,\n",
        "                    'hausdorff': hausdorff,\n",
        "                })\n",
        "\n",
        "        # Print final cross-validation results if we have any\n",
        "        if fold_results:\n",
        "            print(\"\\n=== Final Cross-Validation Results ===\")\n",
        "            avg_dice = np.mean([r['dice'] for r in fold_results])\n",
        "            avg_mean_dist = np.mean([r['mean_dist'] for r in fold_results])\n",
        "            avg_hausdorff = np.mean([r['hausdorff'] for r in fold_results])\n",
        "\n",
        "            print(f\"Average Dice: {avg_dice:.3f}\")\n",
        "            print(f\"Average Mean Absolute Distance: {avg_mean_dist:.3f} mm\")\n",
        "            print(f\"Average Hausdorff Distance: {avg_hausdorff:.3f} mm\")\n",
        "        else:\n",
        "            print(\"\\nNo fold results to report (possibly no test data available)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "9mON7AiMKbm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kQlLFqCJrmh",
        "outputId": "5feff805-c39b-4930-feff-5ea72b29f073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing dataset...\n",
            "Found 500 patients. Using 425 for training, 75 for testing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading training patients: 100%|██████████| 425/425 [1:00:10<00:00,  8.50s/it]\n",
            "Loading test patients: 100%|██████████| 75/75 [09:22<00:00,  7.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training data shape: (20400, 256, 256, 1)\n",
            "Test data shape: (300, 256, 256, 1)\n",
            "\n",
            "Training on 20400 samples\n",
            "Testing on 300 samples\n",
            "\n",
            "=== Fold 1/10 ===\n",
            "Training on 18360 samples, validating on 2040 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Optimized version\n"
      ],
      "metadata": {
        "id": "Iq-AeNBHp4vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "import nibabel as nib\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "from skimage.transform import resize\n",
        "from tqdm import tqdm\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "from skimage.morphology import erosion, dilation, square\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.ndimage import rotate\n",
        "from skimage.transform import rescale\n",
        "import gc\n",
        "\n",
        "# Configuration\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 256\n",
        "IMG_CHANNELS = 1\n",
        "BATCH_SIZE = 4  # Reduced from original\n",
        "EPOCHS = 100\n",
        "INIT_LR = 1e-4\n",
        "N_FOLDS = 5  # Reduced from 10 to 5 for faster testing\n",
        "SEED = 42\n",
        "\n",
        "# Path to CAMUS dataset\n",
        "DATA_PATH = \"/content/drive/MyDrive/database_nifti\"\n",
        "\n",
        "# Define metrics\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1 - dice_coef(y_true, y_pred)\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n",
        "# Data Generator Class\n",
        "class CAMUSDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, patient_folders, target_shape=(256, 256), batch_size=8, augment=True, shuffle=True):\n",
        "        self.patient_folders = patient_folders\n",
        "        self.target_shape = target_shape\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augment\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.patient_folders) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_folders = self.patient_folders[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        X, y = self.__load_and_process_batch(batch_folders)\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.patient_folders)\n",
        "\n",
        "    def __load_and_process_batch(self, batch_folders):\n",
        "        batch_images = []\n",
        "        batch_masks = []\n",
        "\n",
        "        for patient_folder in batch_folders:\n",
        "            images, masks = self.__load_patient(patient_folder)\n",
        "            batch_images.extend(images)\n",
        "            batch_masks.extend(masks)\n",
        "\n",
        "        return np.array(batch_images), np.array(batch_masks)\n",
        "\n",
        "    def __load_patient(self, patient_folder):\n",
        "        images = []\n",
        "        masks = []\n",
        "\n",
        "        views = ['2CH', '4CH']\n",
        "        time_points = ['ED', 'ES']\n",
        "\n",
        "        for view in views:\n",
        "            for tp in time_points:\n",
        "                base_name = os.path.basename(patient_folder)\n",
        "                img_path = f\"{patient_folder}/{base_name}_{view}_{tp}.nii.gz\"\n",
        "                gt_path = f\"{patient_folder}/{base_name}_{view}_{tp}_gt.nii.gz\"\n",
        "\n",
        "                if not os.path.exists(img_path) or not os.path.exists(gt_path):\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    img = nib.load(img_path).get_fdata()\n",
        "                    gt = nib.load(gt_path).get_fdata()\n",
        "\n",
        "                    # Basic preprocessing\n",
        "                    img_resized = resize(img, self.target_shape, preserve_range=True, anti_aliasing=True)\n",
        "                    gt_resized = resize(gt, self.target_shape, preserve_range=True, anti_aliasing=False)\n",
        "\n",
        "                    img_resized = (img_resized - img_resized.min()) / (img_resized.max() - img_resized.min())\n",
        "\n",
        "                    # Only create endocardium mask for this example\n",
        "                    mask = (gt_resized == 1).astype(np.float32)\n",
        "\n",
        "                    if self.augment:\n",
        "                        # Reduced augmentation - only 2 angles and 2 scales\n",
        "                        for angle in [0, 90]:  # Only 0 and 90 degrees\n",
        "                            img_rotated = rotate(img_resized, angle, reshape=False)\n",
        "                            mask_rotated = rotate(mask, angle, reshape=False)\n",
        "\n",
        "                            for scale in [1.0, 1.05]:  # Only original and slight zoom\n",
        "                                img_scaled = rescale(img_rotated, scale, preserve_range=True, anti_aliasing=True)\n",
        "                                mask_scaled = rescale(mask_rotated, scale, preserve_range=True, anti_aliasing=False)\n",
        "\n",
        "                                # Ensure correct size\n",
        "                                img_scaled = resize(img_scaled, self.target_shape, preserve_range=True, anti_aliasing=True)\n",
        "                                mask_scaled = resize(mask_scaled, self.target_shape, preserve_range=True, anti_aliasing=False)\n",
        "\n",
        "                                images.append(img_scaled[..., np.newaxis])\n",
        "                                masks.append(mask_scaled[..., np.newaxis])\n",
        "                    else:\n",
        "                        # No augmentation for validation/test\n",
        "                        images.append(img_resized[..., np.newaxis])\n",
        "                        masks.append(mask[..., np.newaxis])\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {view}_{tp}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        return images, masks\n",
        "\n",
        "# U-Net architectures (unchanged from original)\n",
        "def unet1(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Downsampling path\n",
        "    conv1 = Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(32, 3, activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(64, 3, activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, 3, activation='relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(128, 3, activation='relu', padding='same')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, 3, activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(256, 3, activation='relu', padding='same')(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    # Bottom\n",
        "    conv5 = Conv2D(512, 3, activation='relu', padding='same')(pool4)\n",
        "    conv5 = Conv2D(512, 3, activation='relu', padding='same')(conv5)\n",
        "\n",
        "    # Upsampling path\n",
        "    up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=-1)\n",
        "    conv6 = Conv2D(256, 3, activation='relu', padding='same')(up6)\n",
        "    conv6 = Conv2D(256, 3, activation='relu', padding='same')(conv6)\n",
        "\n",
        "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=-1)\n",
        "    conv7 = Conv2D(128, 3, activation='relu', padding='same')(up7)\n",
        "    conv7 = Conv2D(128, 3, activation='relu', padding='same')(conv7)\n",
        "\n",
        "    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=-1)\n",
        "    conv8 = Conv2D(64, 3, activation='relu', padding='same')(up8)\n",
        "    conv8 = Conv2D(64, 3, activation='relu', padding='same')(conv8)\n",
        "\n",
        "    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=-1)\n",
        "    conv9 = Conv2D(32, 3, activation='relu', padding='same')(up9)\n",
        "    conv9 = Conv2D(32, 3, activation='relu', padding='same')(conv9)\n",
        "\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def unet2(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Downsampling path\n",
        "    conv1 = Conv2D(64, 3, padding='same')(inputs)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Activation('relu')(conv1)\n",
        "    conv1 = Conv2D(64, 3, padding='same')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Activation('relu')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(128, 3, padding='same')(pool1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Activation('relu')(conv2)\n",
        "    conv2 = Conv2D(128, 3, padding='same')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Activation('relu')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(256, 3, padding='same')(pool2)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Activation('relu')(conv3)\n",
        "    conv3 = Conv2D(256, 3, padding='same')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Activation('relu')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(512, 3, padding='same')(pool3)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Activation('relu')(conv4)\n",
        "    conv4 = Conv2D(512, 3, padding='same')(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Activation('relu')(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    # Bottom\n",
        "    conv5 = Conv2D(1024, 3, padding='same')(pool4)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Activation('relu')(conv5)\n",
        "    conv5 = Conv2D(1024, 3, padding='same')(conv5)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Activation('relu')(conv5)\n",
        "\n",
        "    # Upsampling path\n",
        "    up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=-1)\n",
        "    conv6 = Conv2D(512, 3, padding='same')(up6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Activation('relu')(conv6)\n",
        "    conv6 = Conv2D(512, 3, padding='same')(conv6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Activation('relu')(conv6)\n",
        "\n",
        "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=-1)\n",
        "    conv7 = Conv2D(256, 3, padding='same')(up7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Activation('relu')(conv7)\n",
        "    conv7 = Conv2D(256, 3, padding='same')(conv7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Activation('relu')(conv7)\n",
        "\n",
        "    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=-1)\n",
        "    conv8 = Conv2D(128, 3, padding='same')(up8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "    conv8 = Activation('relu')(conv8)\n",
        "    conv8 = Conv2D(128, 3, padding='same')(conv8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "    conv8 = Activation('relu')(conv8)\n",
        "\n",
        "    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=-1)\n",
        "    conv9 = Conv2D(64, 3, padding='same')(up9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "    conv9 = Activation('relu')(conv9)\n",
        "    conv9 = Conv2D(64, 3, padding='same')(conv9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "    conv9 = Activation('relu')(conv9)\n",
        "\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    y_pred = (y_pred > 0.5).astype(np.float32)\n",
        "\n",
        "    # Dice coefficient\n",
        "    intersection = np.sum(y_true * y_pred)\n",
        "    dice = (2. * intersection + 1.) / (np.sum(y_true) + np.sum(y_pred) + 1.)\n",
        "\n",
        "    # Mean absolute distance\n",
        "    dt_true = distance_transform_edt(1 - y_true.squeeze())\n",
        "    dt_pred = distance_transform_edt(1 - y_pred.squeeze())\n",
        "    mean_dist = (np.mean(dt_pred[y_true.squeeze() > 0.5]) +\n",
        "                 np.mean(dt_true[y_pred.squeeze() > 0.5])) / 2\n",
        "\n",
        "    # Hausdorff distance (approximation)\n",
        "    contour_true = y_true.squeeze() - erosion(y_true.squeeze(), square(3))\n",
        "    contour_pred = y_pred.squeeze() - erosion(y_pred.squeeze(), square(3))\n",
        "\n",
        "    if np.sum(contour_true) == 0 or np.sum(contour_pred) == 0:\n",
        "        hausdorff = np.inf\n",
        "    else:\n",
        "        dt_true_contour = distance_transform_edt(1 - contour_true)\n",
        "        dt_pred_contour = distance_transform_edt(1 - contour_pred)\n",
        "        hd_true = np.max(contour_pred * dt_true_contour)\n",
        "        hd_pred = np.max(contour_true * dt_pred_contour)\n",
        "        hausdorff = max(hd_true, hd_pred)\n",
        "\n",
        "    return dice, mean_dist, hausdorff\n",
        "\n",
        "def main():\n",
        "    print(\"Setting up data generators...\")\n",
        "\n",
        "    # Get list of patient folders\n",
        "    patient_folders = sorted([\n",
        "        os.path.join(DATA_PATH, f)\n",
        "        for f in os.listdir(DATA_PATH)\n",
        "        if f.startswith('patient') and os.path.isdir(os.path.join(DATA_PATH, f))\n",
        "    ])\n",
        "\n",
        "    if not patient_folders:\n",
        "        raise ValueError(f\"No patient folders found in {DATA_PATH}\")\n",
        "\n",
        "    # Split into train/test\n",
        "    np.random.seed(SEED)\n",
        "    np.random.shuffle(patient_folders)\n",
        "    split_idx = int(len(patient_folders) * 0.85)  # 85% train, 15% test\n",
        "    train_folders = patient_folders[:split_idx]\n",
        "    test_folders = patient_folders[split_idx:]\n",
        "\n",
        "    print(f\"Found {len(patient_folders)} patients. Using {len(train_folders)} for training, {len(test_folders)} for testing.\")\n",
        "\n",
        "    # Create generators\n",
        "    train_gen = CAMUSDataGenerator(train_folders, batch_size=BATCH_SIZE, augment=True)\n",
        "    test_gen = CAMUSDataGenerator(test_folders, batch_size=BATCH_SIZE, augment=False)\n",
        "\n",
        "    # Choose which U-Net to use\n",
        "    unet_version = 1\n",
        "\n",
        "    # Adjust number of folds if we don't have enough samples\n",
        "    actual_folds = min(N_FOLDS, len(train_folders))\n",
        "    if actual_folds < N_FOLDS:\n",
        "        print(f\"Reducing number of folds from {N_FOLDS} to {actual_folds} due to limited samples\")\n",
        "\n",
        "    kf = KFold(n_splits=actual_folds, shuffle=True, random_state=SEED)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_folders)):\n",
        "        print(f\"\\n=== Fold {fold + 1}/{actual_folds} ===\")\n",
        "\n",
        "        # Create train/val generators for this fold\n",
        "        fold_train_folders = np.array(train_folders)[train_idx]\n",
        "        fold_val_folders = np.array(train_folders)[val_idx]\n",
        "\n",
        "        train_gen = CAMUSDataGenerator(fold_train_folders, batch_size=BATCH_SIZE, augment=True)\n",
        "        val_gen = CAMUSDataGenerator(fold_val_folders, batch_size=BATCH_SIZE, augment=False)\n",
        "\n",
        "        # Create and compile model\n",
        "        model = unet1() if unet_version == 1 else unet2()\n",
        "        model.compile(optimizer=Adam(learning_rate=INIT_LR),\n",
        "                      loss=bce_dice_loss,\n",
        "                      metrics=[dice_coef, 'accuracy'])\n",
        "\n",
        "        callbacks = [\n",
        "            ModelCheckpoint(f\"unet{unet_version}_fold{fold}_best.keras\",\n",
        "                            monitor='val_dice_coef',\n",
        "                            mode='max',\n",
        "                            save_best_only=True,\n",
        "                            verbose=1),\n",
        "            EarlyStopping(monitor='val_dice_coef', patience=15, mode='max', verbose=1),\n",
        "            ReduceLROnPlateau(monitor='val_dice_coef', factor=0.5, patience=5, min_lr=1e-6, mode='max', verbose=1)\n",
        "        ]\n",
        "\n",
        "        # Train model\n",
        "        history = model.fit(\n",
        "            train_gen,\n",
        "            epochs=EPOCHS,\n",
        "            validation_data=val_gen,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Clean up to save memory\n",
        "        K.clear_session()\n",
        "        gc.collect()\n",
        "\n",
        "        # Evaluate on test set\n",
        "        model.load_weights(f\"unet{unet_version}_fold{fold}_best.keras\")\n",
        "\n",
        "        # Get a batch from test set for evaluation\n",
        "        X_test_batch, y_test_batch = next(iter(test_gen))\n",
        "        y_pred = model.predict(X_test_batch)\n",
        "\n",
        "        # Calculate metrics for each sample in batch\n",
        "        for i in range(len(X_test_batch)):\n",
        "            dice, mean_dist, hausdorff = calculate_metrics(y_test_batch[i], y_pred[i])\n",
        "            fold_results.append({\n",
        "                'dice': dice,\n",
        "                'mean_dist': mean_dist,\n",
        "                'hausdorff': hausdorff,\n",
        "            })\n",
        "\n",
        "            # Visualize first sample\n",
        "            if i == 0:\n",
        "                plt.figure(figsize=(12, 4))\n",
        "                plt.subplot(1, 3, 1)\n",
        "                plt.imshow(X_test_batch[i].squeeze(), cmap='gray')\n",
        "                plt.title(\"Original Image\")\n",
        "\n",
        "                plt.subplot(1, 3, 2)\n",
        "                plt.imshow(y_test_batch[i].squeeze(), cmap='gray')\n",
        "                plt.title(\"Ground Truth\")\n",
        "\n",
        "                plt.subplot(1, 3, 3)\n",
        "                plt.imshow(y_pred[i].squeeze() > 0.5, cmap='gray')\n",
        "                plt.title(\"Predicted Mask\")\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "    # Print final results\n",
        "    if fold_results:\n",
        "        print(\"\\n=== Final Results ===\")\n",
        "        avg_dice = np.mean([r['dice'] for r in fold_results])\n",
        "        avg_mean_dist = np.mean([r['mean_dist'] for r in fold_results])\n",
        "        avg_hausdorff = np.mean([r['hausdorff'] for r in fold_results])\n",
        "\n",
        "        print(f\"Average Dice: {avg_dice:.3f}\")\n",
        "        print(f\"Average Mean Absolute Distance: {avg_mean_dist:.3f} mm\")\n",
        "        print(f\"Average Hausdorff Distance: {avg_hausdorff:.3f} mm\")\n",
        "    else:\n",
        "        print(\"\\nNo results to report\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "fWgsu-eAp7jL",
        "outputId": "556ee04d-4814-454e-d15d-602eb3e50b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up data generators...\n",
            "Found 500 patients. Using 425 for training, 75 for testing.\n",
            "\n",
            "=== Fold 1/5 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m 7/85\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10:10\u001b[0m 8s/step - accuracy: 0.5807 - dice_coef: 0.1494 - loss: 1.5384"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f3014ddc0186>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-f3014ddc0186>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         history = model.fit(\n\u001b[0m\u001b[1;32m    358\u001b[0m             \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}