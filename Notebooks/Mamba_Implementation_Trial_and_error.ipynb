{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11345981,"sourceType":"datasetVersion","datasetId":7099200}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nimport nibabel as nib\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, UpSampling2D, \n                                    concatenate, BatchNormalization, Activation, \n                                    Multiply, Add, Lambda, DepthwiseConv2D)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nfrom skimage.transform import resize\nfrom tqdm import tqdm\nfrom scipy.ndimage import distance_transform_edt\nfrom skimage.morphology import erosion, dilation, square\nfrom sklearn.model_selection import train_test_split\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuration\nIMG_HEIGHT = 256\nIMG_WIDTH = 256\nIMG_CHANNELS = 1\nBATCH_SIZE = 8  # Reduced from original due to memory requirements of attention\nEPOCHS = 100\nINIT_LR = 1e-4\nN_FOLDS = 10\nSEED = 42\nDATA_PATH = \"/kaggle/input/camus-dataset/database_nifti\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Enhanced metrics with class-specific calculations\ndef dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_loss(y_true, y_pred):\n    return 1 - dice_coef(y_true, y_pred)\n\ndef bce_dice_loss(y_true, y_pred):\n    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Novel Attention Gate Module\ndef attention_gate(x, g, inter_channel):\n    \"\"\"Hybrid Attention Gate with proper dimension handling\"\"\"\n    # Get the number of channels in the input x\n    x_channels = K.int_shape(x)[-1]\n    \n    # Process the gating signal (upsampled feature)\n    g_conv = Conv2D(x_channels, (1, 1), strides=1, padding='same')(g)\n    g_conv = BatchNormalization()(g_conv)\n    g_conv = Activation('relu')(g_conv)\n    \n    # Process the input features\n    x_conv = Conv2D(x_channels, (1, 1), strides=1, padding='same')(x)\n    x_conv = BatchNormalization()(x_conv)\n    x_conv = Activation('relu')(x_conv)\n    \n    # Add the processed features\n    combined = Add()([x_conv, g_conv])\n    combined = Activation('relu')(combined)\n    \n    # Attention coefficients\n    attention = Conv2D(1, (1, 1), strides=1, padding='same', activation='sigmoid')(combined)\n    \n    # Apply attention\n    return Multiply()([x, attention])\n\n\n# Depthwise Separable Block\ndef depthwise_sep_block(x, filters, kernel_size=3, strides=1):\n    \"\"\"\n    Depthwise separable convolution block\n    Args:\n        x: input tensor\n        filters: number of output filters\n        kernel_size: size of convolution kernel\n        strides: stride length\n    Returns:\n        Output tensor after applying depthwise separable convolution\n    \"\"\"\n    # Depthwise convolution\n    x = DepthwiseConv2D(kernel_size=kernel_size,\n                       strides=strides,\n                       padding='same',\n                       depth_multiplier=1)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # Pointwise convolution\n    x = Conv2D(filters=filters,\n               kernel_size=1,\n               strides=1,\n               padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# HAG-UNet Architecture\ndef hag_unet(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n    inputs = Input(input_size)\n    \n    # Downsample path with depthwise blocks\n    # Block 1\n    conv1 = depthwise_sep_block(inputs, 32)\n    conv1 = depthwise_sep_block(conv1, 32)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    \n    # Block 2\n    conv2 = depthwise_sep_block(pool1, 64)\n    conv2 = depthwise_sep_block(conv2, 64)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    \n    # Block 3\n    conv3 = depthwise_sep_block(pool2, 128)\n    conv3 = depthwise_sep_block(conv3, 128)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    \n    # Block 4\n    conv4 = depthwise_sep_block(pool3, 256)\n    conv4 = depthwise_sep_block(conv4, 256)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n    \n    # Bottleneck\n    conv5 = depthwise_sep_block(pool4, 512)\n    conv5 = depthwise_sep_block(conv5, 512)\n    \n    # Upsample path with attention gates\n    # Up 1\n    up6 = UpSampling2D(size=(2, 2))(conv5)\n    att6 = attention_gate(conv4, up6, 256)\n    merge6 = concatenate([up6, att6], axis=-1)\n    conv6 = depthwise_sep_block(merge6, 256)\n    conv6 = depthwise_sep_block(conv6, 256)\n    \n    # Up 2\n    up7 = UpSampling2D(size=(2, 2))(conv6)\n    att7 = attention_gate(conv3, up7, 128)\n    merge7 = concatenate([up7, att7], axis=-1)\n    conv7 = depthwise_sep_block(merge7, 128)\n    conv7 = depthwise_sep_block(conv7, 128)\n    \n    # Up 3\n    up8 = UpSampling2D(size=(2, 2))(conv7)\n    att8 = attention_gate(conv2, up8, 64)\n    merge8 = concatenate([up8, att8], axis=-1)\n    conv8 = depthwise_sep_block(merge8, 64)\n    conv8 = depthwise_sep_block(conv8, 64)\n    \n    # Up 4\n    up9 = UpSampling2D(size=(2, 2))(conv8)\n    att9 = attention_gate(conv1, up9, 32)\n    merge9 = concatenate([up9, att9], axis=-1)\n    conv9 = depthwise_sep_block(merge9, 32)\n    conv9 = depthwise_sep_block(conv9, 32)\n    \n    # Output\n    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n    \n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\n# Enhanced evaluation metrics with contour refinement\ndef calculate_metrics(y_true, y_pred):\n    # Apply morphological refinement to predictions\n    refined_pred = np.zeros_like(y_pred)\n    for i in range(y_pred.shape[0]):\n        pred = (y_pred[i].squeeze() > 0.5).astype(np.uint8)\n        # Small dilation then erosion to close small holes\n        refined = dilation(pred, square(2))\n        refined = erosion(refined, square(2))\n        refined_pred[i] = refined[..., np.newaxis]\n    \n    y_pred = refined_pred\n    \n    # Dice coefficient\n    intersection = np.sum(y_true * y_pred)\n    dice = (2. * intersection + 1.) / (np.sum(y_true) + np.sum(y_pred) + 1.)\n    \n    # Mean absolute distance with contour weighting\n    dt_true = distance_transform_edt(1 - y_true.squeeze())\n    dt_pred = distance_transform_edt(1 - y_pred.squeeze())\n    \n    # Weight distances by how close they are to contours\n    contour_true = y_true.squeeze() - erosion(y_true.squeeze(), square(3))\n    contour_pred = y_pred.squeeze() - erosion(y_pred.squeeze(), square(3))\n    \n    contour_weights_true = 1 + 2 * contour_true  # Give 3x weight to contour pixels\n    contour_weights_pred = 1 + 2 * contour_pred\n    \n    mean_dist = (np.mean(dt_pred[y_true.squeeze() > 0.5] * contour_weights_true[y_true.squeeze() > 0.5]) + \n                np.mean(dt_true[y_pred.squeeze() > 0.5] * contour_weights_pred[y_pred.squeeze() > 0.5])) / 2\n    \n    # Hausdorff distance with 95th percentile (more robust)\n    if np.sum(contour_true) == 0 or np.sum(contour_pred) == 0:\n        hausdorff = np.inf\n    else:\n        dt_true_contour = distance_transform_edt(1 - contour_true)\n        dt_pred_contour = distance_transform_edt(1 - contour_pred)\n        hd_true = np.percentile(contour_pred * dt_true_contour, 95)\n        hd_pred = np.percentile(contour_true * dt_pred_contour, 95)\n        hausdorff = max(hd_true, hd_pred)\n    \n    return dice, mean_dist, hausdorff\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Main training and evaluation (similar to original but with HAG-UNet)\ndef main():\n    # Load dataset (same as original)\n    print(\"Loading and preprocessing dataset...\")\n    X, y_endo, y_epi, y_la = load_dataset(DATA_PATH)\n    \n    # Create 10-fold cross-validation\n    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n    fold_results = []\n    \n    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n        print(f\"\\n=== Fold {fold + 1}/{N_FOLDS} ===\")\n        \n        # Split data\n        X_train, X_test = X[train_idx], X[test_idx]\n        y_train, y_test = y_endo[train_idx], y_endo[test_idx]\n        \n        # Train/val split\n        X_train, X_val, y_train, y_val = train_test_split(\n            X_train, y_train, test_size=0.1, random_state=SEED)\n        \n        # Create HAG-UNet model\n        model = hag_unet()\n        \n        # Enhanced optimizer with warmup\n        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n            INIT_LR,\n            decay_steps=1000,\n            decay_rate=0.96,\n            staircase=True)\n        \n        model.compile(optimizer=Adam(learning_rate=lr_schedule), \n                     loss=bce_dice_loss, \n                     metrics=[dice_coef, 'accuracy'])\n        \n        callbacks = [\n            ModelCheckpoint(f\"hag_unet_fold{fold}_best.keras\", \n                           monitor='val_dice_coef', \n                           mode='max', \n                           save_best_only=True, \n                           verbose=1),\n            EarlyStopping(monitor='val_dice_coef', \n                         patience=20,  # Increased patience for attention learning\n                         mode='max', \n                         verbose=1),\n            ReduceLROnPlateau(monitor='val_dice_coef', \n                            factor=0.5, \n                            patience=8,  # More patience for learning rate\n                            min_lr=1e-6, \n                            mode='max', \n                            verbose=1)\n        ]\n        \n        # Train with class weights (focus more on boundary pixels)\n        sample_weights = np.ones_like(y_train)\n        for i in range(len(y_train)):\n            contours = y_train[i].squeeze() - erosion(y_train[i].squeeze(), square(3))\n            sample_weights[i] = 1 + 2*contours  # 3x weight for boundary pixels\n        \n        print(f\"Training on {len(X_train)} samples\")\n        history = model.fit(X_train, y_train,\n                          batch_size=BATCH_SIZE,\n                          epochs=EPOCHS,\n                          validation_data=(X_val, y_val),\n                          callbacks=callbacks,\n                          sample_weight=sample_weights,\n                          verbose=1)\n        \n        # Load best model\n        model.load_weights(f\"hag_unet_fold{fold}_best.keras\")\n        \n        # Evaluate\n        print(f\"Evaluating on {len(X_test)} test samples\")\n        y_pred = model.predict(X_test, batch_size=BATCH_SIZE)\n        \n        # Calculate enhanced metrics\n        dice_scores = []\n        mean_distances = []\n        hausdorff_distances = []\n        \n        for i in range(len(X_test)):\n            dice, mean_dist, hausdorff = calculate_metrics(y_test[i], y_pred[i])\n            dice_scores.append(dice)\n            mean_distances.append(mean_dist)\n            hausdorff_distances.append(hausdorff)\n        \n        fold_results.append({\n            'dice': np.mean(dice_scores),\n            'mean_dist': np.mean(mean_distances),\n            'hausdorff': np.mean(hausdorff_distances)\n        })\n        \n        print(f\"Fold {fold + 1} Results:\")\n        print(f\"Dice: {np.mean(dice_scores):.4f} ± {np.std(dice_scores):.4f}\")\n        print(f\"Mean Distance: {np.mean(mean_distances):.4f} ± {np.std(mean_distances):.4f} mm\")\n        print(f\"Hausdorff Distance: {np.mean(hausdorff_distances):.4f} ± {np.std(hausdorff_distances):.4f} mm\")\n        \n        # Visualize attention effects\n        plot_attention_maps(model, X_test[:3], y_test[:3], fold+1)\n    \n    # Final results\n    print(\"\\n=== HAG-UNet Final Results ===\")\n    avg_dice = np.mean([r['dice'] for r in fold_results])\n    avg_mean_dist = np.mean([r['mean_dist'] for r in fold_results])\n    avg_hausdorff = np.mean([r['hausdorff'] for r in fold_results])\n    \n    print(f\"Average Dice: {avg_dice:.4f}\")\n    print(f\"Average Mean Distance: {avg_mean_dist:.4f} mm\")\n    print(f\"Average Hausdorff Distance: {avg_hausdorff:.4f} mm\")\n    \n    # Compare with baseline\n    print(\"\\n=== Improvement Over Baseline ===\")\n    print(f\"Dice Improvement: {(avg_dice - 0.932):.4f} ({(avg_dice - 0.932)/0.932*100:.2f}%)\")\n    print(f\"Mean Distance Reduction: {(0.266 - avg_mean_dist):.4f} mm ({(0.266 - avg_mean_dist)/0.266*100:.2f}%)\")\n    print(f\"Hausdorff Distance Reduction: {(11.009 - avg_hausdorff):.4f} mm ({(11.009 - avg_hausdorff)/11.009*100:.2f}%)\")\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_attention_maps(model, images, masks, fold):\n    \"\"\"Visualize attention gate activations\"\"\"\n    # Create partial model to get attention outputs\n    layer_outputs = [layer.output for layer in model.layers if 'multiply' in layer.name]\n    attention_model = Model(inputs=model.input, outputs=layer_outputs)\n    \n    # Get attention maps\n    attention_maps = attention_model.predict(images)\n    \n    # Plot results\n    plt.figure(figsize=(18, 6*len(images)))\n    for i in range(len(images)):\n        # Original\n        plt.subplot(len(images), 4, 1+i*4)\n        plt.imshow(images[i].squeeze(), cmap='gray')\n        plt.title(f\"Original Image (Fold {fold})\")\n        plt.axis('off')\n        \n        # Ground truth\n        plt.subplot(len(images), 4, 2+i*4)\n        plt.imshow(masks[i].squeeze(), cmap='gray')\n        plt.title(\"Ground Truth\")\n        plt.axis('off')\n        \n        # Attention map 1 (deepest)\n        plt.subplot(len(images), 4, 3+i*4)\n        plt.imshow(attention_maps[0][i].squeeze(), cmap='hot')\n        plt.title(\"Deep Attention Map\")\n        plt.axis('off')\n        \n        # Attention map 2 (mid-level)\n        plt.subplot(len(images), 4, 4+i*4)\n        plt.imshow(attention_maps[2][i].squeeze(), cmap='hot')\n        plt.title(\"Mid-Level Attention Map\")\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == '__main__':\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nimport nibabel as nib\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, MaxPooling2D, UpSampling2D, \n    concatenate, BatchNormalization, Activation, \n    Multiply, Add, Lambda, DepthwiseConv2D\n)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nfrom skimage.transform import resize\nfrom tqdm import tqdm\nfrom scipy.ndimage import distance_transform_edt\nfrom skimage.morphology import erosion, dilation, square\nfrom sklearn.model_selection import train_test_split\n\n# Configuration\nIMG_HEIGHT = 256\nIMG_WIDTH = 256\nIMG_CHANNELS = 1\nBATCH_SIZE = 8\nEPOCHS = 100\nINIT_LR = 1e-4\nN_FOLDS = 10\nSEED = 42\nDATA_PATH = \"/kaggle/input/camus-dataset/database_nifti\"  # Make sure this path is correct\n\n# ==================== DATA LOADING FUNCTIONS ====================\ndef load_nifti_image(file_path):\n    img = nib.load(file_path)\n    data = img.get_fdata()\n    return np.squeeze(data)  # Remove singleton dimensions\n\ndef preprocess_patient(patient_folder):\n    images = []\n    masks_endo = []\n    masks_epi = []\n    masks_la = []\n    \n    views = ['2CH', '4CH']\n    time_points = ['ED', 'ES']\n    \n    for view in views:\n        for tp in time_points:\n            # Construct filenames\n            base_name = os.path.basename(patient_folder)\n            img_path = f\"{patient_folder}/{base_name}_{view}_{tp}.nii\"\n            gt_path = f\"{patient_folder}/{base_name}_{view}_{tp}_gt.nii\"\n            \n            if not os.path.exists(img_path) or not os.path.exists(gt_path):\n                print(f\"Files not found for {view}_{tp}\")\n                continue\n                \n            try:\n                # Load image\n                img = nib.load(img_path).get_fdata()\n                \n                # Load ground truth\n                gt = nib.load(gt_path).get_fdata()\n                \n                # Resize and normalize\n                img_resized = resize(img, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=True)\n                gt_resized = resize(gt, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=False)\n                \n                img_resized = (img_resized - img_resized.min()) / (img_resized.max() - img_resized.min())\n                \n                # Create masks\n                mask_endo = (gt_resized == 1).astype(np.float32)\n                mask_epi = (gt_resized == 2).astype(np.float32)\n                mask_la = (gt_resized == 3).astype(np.float32)\n                \n                images.append(img_resized[..., np.newaxis])\n                masks_endo.append(mask_endo[..., np.newaxis])\n                masks_epi.append(mask_epi[..., np.newaxis])\n                masks_la.append(mask_la[..., np.newaxis])\n                \n            except Exception as e:\n                print(f\"Error processing {view}_{tp}: {str(e)}\")\n                continue\n    \n    if not images:\n        print(\"No valid images found for this patient\")\n        return np.array([]), np.array([]), np.array([]), np.array([])\n    \n    return np.array(images), np.array(masks_endo), np.array(masks_epi), np.array(masks_la)\n\ndef load_dataset(base_path):\n    patient_folders = sorted([\n        os.path.join(base_path, f) \n        for f in os.listdir(base_path) \n        if f.startswith('patient') and os.path.isdir(os.path.join(base_path, f))\n    ])\n    \n    all_images = []\n    all_masks_endo = []\n    all_masks_epi = []\n    all_masks_la = []\n    \n    for patient_folder in tqdm(patient_folders, desc=\"Loading patients\"):\n        images, masks_endo, masks_epi, masks_la = preprocess_patient(patient_folder)\n        \n        if images.size > 0:  # Only append if we got valid data\n            all_images.append(images)\n            all_masks_endo.append(masks_endo)\n            all_masks_epi.append(masks_epi)\n            all_masks_la.append(masks_la)\n    \n    # Check if we got any data at all\n    if not all_images:\n        raise ValueError(\"No valid image data found in any patient folder!\")\n    \n    return (np.concatenate(all_images, axis=0),\n            np.concatenate(all_masks_endo, axis=0),\n            np.concatenate(all_masks_epi, axis=0),\n            np.concatenate(all_masks_la, axis=0))\n\n# ==================== MODEL ARCHITECTURE ====================\ndef attention_gate(x, g, inter_channel):\n    \"\"\"Hybrid Attention Gate with proper dimension handling\"\"\"\n    # Get the number of channels in the input x\n    x_channels = K.int_shape(x)[-1]\n    \n    # Process the gating signal (upsampled feature)\n    g_conv = Conv2D(x_channels, (1, 1), strides=1, padding='same')(g)\n    g_conv = BatchNormalization()(g_conv)\n    g_conv = Activation('relu')(g_conv)\n    \n    # Process the input features\n    x_conv = Conv2D(x_channels, (1, 1), strides=1, padding='same')(x)\n    x_conv = BatchNormalization()(x_conv)\n    x_conv = Activation('relu')(x_conv)\n    \n    # Add the processed features\n    combined = Add()([x_conv, g_conv])\n    combined = Activation('relu')(combined)\n    \n    # Attention coefficients\n    attention = Conv2D(1, (1, 1), strides=1, padding='same', activation='sigmoid')(combined)\n    \n    # Apply attention\n    return Multiply()([x, attention])\n\ndef depthwise_sep_block(x, filters, kernel_size=3, strides=1):\n    \"\"\"\n    Depthwise separable convolution block\n    Args:\n        x: input tensor\n        filters: number of output filters\n        kernel_size: size of convolution kernel\n        strides: stride length\n    Returns:\n        Output tensor after applying depthwise separable convolution\n    \"\"\"\n    # Depthwise convolution\n    x = DepthwiseConv2D(kernel_size=kernel_size,\n                       strides=strides,\n                       padding='same',\n                       depth_multiplier=1)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # Pointwise convolution\n    x = Conv2D(filters=filters,\n               kernel_size=1,\n               strides=1,\n               padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    return x\n\ndef hag_unet(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n    inputs = Input(input_size)\n    \n    # Downsample path\n    # Block 1\n    conv1 = depthwise_sep_block(inputs, 32)\n    conv1 = depthwise_sep_block(conv1, 32)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    \n    # Block 2\n    conv2 = depthwise_sep_block(pool1, 64)\n    conv2 = depthwise_sep_block(conv2, 64)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    \n    # Block 3\n    conv3 = depthwise_sep_block(pool2, 128)\n    conv3 = depthwise_sep_block(conv3, 128)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    \n    # Block 4\n    conv4 = depthwise_sep_block(pool3, 256)\n    conv4 = depthwise_sep_block(conv4, 256)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n    \n    # Bottleneck\n    conv5 = depthwise_sep_block(pool4, 512)\n    conv5 = depthwise_sep_block(conv5, 512)\n    \n    # Upsample path with attention gates\n    # Up 1\n    up6 = UpSampling2D(size=(2, 2))(conv5)\n    up6 = Conv2D(256, (2, 2), activation='relu', padding='same')(up6)  # Channel adjustment\n    att6 = attention_gate(conv4, up6, 256)\n    merge6 = concatenate([up6, att6], axis=-1)\n    conv6 = depthwise_sep_block(merge6, 256)\n    conv6 = depthwise_sep_block(conv6, 256)\n    \n    # Up 2\n    up7 = UpSampling2D(size=(2, 2))(conv6)\n    up7 = Conv2D(128, (2, 2), activation='relu', padding='same')(up7)  # Channel adjustment\n    att7 = attention_gate(conv3, up7, 128)\n    merge7 = concatenate([up7, att7], axis=-1)\n    conv7 = depthwise_sep_block(merge7, 128)\n    conv7 = depthwise_sep_block(conv7, 128)\n    \n    # Up 3\n    up8 = UpSampling2D(size=(2, 2))(conv7)\n    up8 = Conv2D(64, (2, 2), activation='relu', padding='same')(up8)  # Channel adjustment\n    att8 = attention_gate(conv2, up8, 64)\n    merge8 = concatenate([up8, att8], axis=-1)\n    conv8 = depthwise_sep_block(merge8, 64)\n    conv8 = depthwise_sep_block(conv8, 64)\n    \n    # Up 4\n    up9 = UpSampling2D(size=(2, 2))(conv8)\n    up9 = Conv2D(32, (2, 2), activation='relu', padding='same')(up9)  # Channel adjustment\n    att9 = attention_gate(conv1, up9, 32)\n    merge9 = concatenate([up9, att9], axis=-1)\n    conv9 = depthwise_sep_block(merge9, 32)\n    conv9 = depthwise_sep_block(conv9, 32)\n    \n    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n    \n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\n# ==================== METRICS AND LOSS ====================\ndef dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_loss(y_true, y_pred):\n    return 1 - dice_coef(y_true, y_pred)\n\ndef bce_dice_loss(y_true, y_pred):\n    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n\n# ==================== MAIN TRAINING LOOP ====================\ndef main():\n    # Load dataset\n    print(\"Loading and preprocessing dataset...\")\n    X, y_endo, y_epi, y_la = load_dataset(DATA_PATH)\n    \n    # Create 10-fold cross-validation\n    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n    fold_results = []\n    \n    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n        print(f\"\\n=== Fold {fold + 1}/{N_FOLDS} ===\")\n        \n        # Split data\n        X_train, X_test = X[train_idx], X[test_idx]\n        y_train, y_test = y_endo[train_idx], y_endo[test_idx]\n        \n        # Further split training into train/val (90/10)\n        X_train, X_val, y_train, y_val = train_test_split(\n            X_train, y_train, test_size=0.1, random_state=SEED)\n        \n        # Create model\n        model = hag_unet()\n        model.compile(optimizer=Adam(learning_rate=INIT_LR), \n                     loss=bce_dice_loss, \n                     metrics=[dice_coef, 'accuracy'])\n        \n        callbacks = [\n            ModelCheckpoint(f\"hag_unet_fold{fold}_best.keras\", \n                          monitor='val_dice_coef', \n                          mode='max', \n                          save_best_only=True),\n            EarlyStopping(monitor='val_dice_coef', \n                        patience=15, \n                        mode='max'),\n            ReduceLROnPlateau(monitor='val_dice_coef',\n                            factor=0.5,\n                            patience=5,\n                            min_lr=1e-6,\n                            mode='max')\n        ]\n        \n        # Train model\n        print(f\"Training on {len(X_train)} samples\")\n        history = model.fit(X_train, y_train,\n                          batch_size=BATCH_SIZE,\n                          epochs=EPOCHS,\n                          validation_data=(X_val, y_val),\n                          callbacks=callbacks,\n                          verbose=1)\n        \n        # Evaluate\n        model.load_weights(f\"hag_unet_fold{fold}_best.keras\")\n        y_pred = model.predict(X_test, batch_size=BATCH_SIZE)\n        \n        # Calculate dice scores\n        dice_scores = [dice_coef(y_test[i], y_pred[i]) for i in range(len(y_test))]\n        avg_dice = np.mean(dice_scores)\n        \n        fold_results.append(avg_dice)\n        print(f\"Fold {fold+1} Dice: {avg_dice:.4f}\")\n    \n    # Final results\n    print(\"\\n=== Final Cross-Validation Results ===\")\n    print(f\"Average Dice: {np.mean(fold_results):.4f} ± {np.std(fold_results):.4f}\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### ---------- The working code starts here --------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:38:55.101579Z","iopub.execute_input":"2025-04-18T16:38:55.102325Z","iopub.status.idle":"2025-04-18T16:38:55.105765Z","shell.execute_reply.started":"2025-04-18T16:38:55.102295Z","shell.execute_reply":"2025-04-18T16:38:55.105021Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import os\nimport numpy as np\nimport nibabel as nib\nfrom skimage.transform import resize\nfrom scipy.ndimage import rotate\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, ReLU, MaxPooling2D, \n                                     Conv2DTranspose, GlobalAveragePooling2D, Dense, Multiply, \n                                     Add, concatenate, LayerNormalization, Reshape, Lambda)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras import backend as K\nfrom sklearn.model_selection import KFold, train_test_split\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pickle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:38:57.429058Z","iopub.execute_input":"2025-04-18T16:38:57.429355Z","iopub.status.idle":"2025-04-18T16:38:57.434597Z","shell.execute_reply.started":"2025-04-18T16:38:57.429333Z","shell.execute_reply":"2025-04-18T16:38:57.433901Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Configuration\nIMG_HEIGHT = 256\nIMG_WIDTH = 256\nIMG_CHANNELS = 1\nBATCH_SIZE = 8\nEPOCHS = 100\nINIT_LR = 1e-4\nN_FOLDS = 3\nSEED = 42\nDATA_PATH = \"/kaggle/input/camus-dataset/database_nifti\"  # Update this with the actual dataset path\n\n# Metrics\ndef dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_loss(y_true, y_pred):\n    return 1 - dice_coef(y_true, y_pred)\n\ndef bce_dice_loss(y_true, y_pred):\n    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n\ndef iou_coef(y_true, y_pred, smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    union = K.sum(y_true_f) + K.sum(y_pred_f) - intersection\n    return (intersection + smooth) / (union + smooth)\n\ndef average_precision(y_true, y_pred, thresholds=tf.constant(np.arange(0.0, 1.1, 0.1))):\n    precisions = []\n    for threshold in thresholds:\n        y_pred_thresholded = tf.cast(y_pred > threshold, tf.float32)\n        tp = tf.reduce_sum(y_true * y_pred_thresholded)\n        fp = tf.reduce_sum((1 - y_true) * y_pred_thresholded)\n        precision = tp / (tp + fp + K.epsilon())\n        precisions.append(precision)\n    return tf.reduce_mean(tf.stack(precisions))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:38:59.818119Z","iopub.execute_input":"2025-04-18T16:38:59.818889Z","iopub.status.idle":"2025-04-18T16:38:59.826780Z","shell.execute_reply.started":"2025-04-18T16:38:59.818860Z","shell.execute_reply":"2025-04-18T16:38:59.826061Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Preprocessing\ndef load_nifti_image(file_path):\n    img = nib.load(file_path)\n    return np.squeeze(img.get_fdata())\n\ndef preprocess_patient_with_rotation(patient_folder, rotation_angles=[0, 90, 180, 270]):\n    images = []\n    masks_endo = []\n    masks_epi = []\n    masks_la = []\n    \n    views = ['2CH', '4CH']\n    time_points = ['ED', 'ES']\n    \n    for view in views:\n        for tp in time_points:\n            base_name = os.path.basename(patient_folder)\n            img_path = f\"{patient_folder}/{base_name}_{view}_{tp}.nii\"\n            gt_path = f\"{patient_folder}/{base_name}_{view}_{tp}_gt.nii\"\n            \n            if not os.path.exists(img_path) or not os.path.exists(gt_path):\n                continue\n            \n            try:\n                img = load_nifti_image(img_path)\n                gt = load_nifti_image(gt_path)\n                \n                img_resized = resize(img, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=True)\n                gt_resized = resize(gt, (IMG_HEIGHT, IMG_WIDTH), preserve_range=True, anti_aliasing=False)\n                \n                img_resized = (img_resized - img_resized.min()) / (img_resized.max() - img_resized.min())\n                \n                mask_endo = (gt_resized == 1).astype(np.float32)\n                mask_epi = (gt_resized == 2).astype(np.float32)\n                mask_la = (gt_resized == 3).astype(np.float32)\n                \n                for angle in rotation_angles:\n                    rotated_img = rotate(img_resized, angle, reshape=False, mode='reflect')\n                    rotated_mask_endo = rotate(mask_endo, angle, reshape=False, mode='reflect')\n                    rotated_mask_epi = rotate(mask_epi, angle, reshape=False, mode='reflect')\n                    rotated_mask_la = rotate(mask_la, angle, reshape=False, mode='reflect')\n                    \n                    images.append(rotated_img[..., np.newaxis])\n                    masks_endo.append(rotated_mask_endo[..., np.newaxis])\n                    masks_epi.append(rotated_mask_epi[..., np.newaxis])\n                    masks_la.append(rotated_mask_la[..., np.newaxis])\n            \n            except Exception as e:\n                print(f\"Error processing {base_name}: {e}\")\n                continue\n    \n    if images:\n        return np.array(images), np.array(masks_endo), np.array(masks_epi), np.array(masks_la)\n    else:\n        return np.array([]), np.array([]), np.array([]), np.array([])\n\ndef load_dataset(base_path):\n    patient_folders = sorted([\n        os.path.join(base_path, f) \n        for f in os.listdir(base_path) \n        if f.startswith('patient') and os.path.isdir(os.path.join(base_path, f))\n    ])\n    \n    all_images = []\n    all_masks_endo = []\n    all_masks_epi = []\n    all_masks_la = []\n    \n    for patient_folder in tqdm(patient_folders, desc=\"Loading patients\"):\n        images, masks_endo, masks_epi, masks_la = preprocess_patient_with_rotation(patient_folder)\n        \n        if images.size > 0:\n            all_images.append(images)\n            all_masks_endo.append(masks_endo)\n            all_masks_epi.append(masks_epi)\n            all_masks_la.append(masks_la)\n    \n    if not all_images:\n        raise ValueError(\"No valid image data found in any patient folder!\")\n    \n    return (np.concatenate(all_images, axis=0),\n            np.concatenate(all_masks_endo, axis=0),\n            np.concatenate(all_masks_epi, axis=0),\n            np.concatenate(all_masks_la, axis=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:39:01.693014Z","iopub.execute_input":"2025-04-18T16:39:01.693800Z","iopub.status.idle":"2025-04-18T16:39:01.705596Z","shell.execute_reply.started":"2025-04-18T16:39:01.693772Z","shell.execute_reply":"2025-04-18T16:39:01.704609Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"\n# Mamba-based TransUNet Model\ndef mamba_block(x, hidden_dim, ssm_dim, dropout_rate=0.1):\n    \"\"\"Implements a simplified Mamba block for 2D feature maps.\"\"\"\n    batch, height, width, channels = K.int_shape(x)\n    \n    # 1. Depthwise convolution for local feature mixing\n    x_res = x\n    x = LayerNormalization()(x)\n    x = Conv2D(channels, kernel_size=3, padding='same', groups=channels)(x)\n    \n    # 2. Project to hidden dimension\n    x = Conv2D(hidden_dim, kernel_size=1)(x)\n    x = ReLU()(x)\n    \n    # 3. Simplified SSM (State Space Model) path\n    # Reshape to sequence for SSM processing\n    x_reshaped = Reshape((height * width, hidden_dim))(x)\n    \n    # Simplified SSM implementation (using dense layers as approximation)\n    ssm = Dense(ssm_dim, activation='swish')(x_reshaped)\n    ssm = Dense(hidden_dim)(ssm)\n    \n    # Residual connection\n    ssm = Dense(hidden_dim)(x_reshaped) + ssm\n    \n    # Reshape back to spatial dimensions\n    x_ssm = Reshape((height, width, hidden_dim))(ssm)\n    \n    # 4. Project back to channel dimension\n    x_out = Conv2D(channels, kernel_size=1)(x_ssm)\n    \n    # Add dropout and residual\n    x_out = tf.keras.layers.Dropout(dropout_rate)(x_out)\n    x_out = x_res + x_out\n    \n    return x_out\n\ndef transunet_mamba(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n    \"\"\"TransUNet architecture with Mamba blocks in the encoder.\"\"\"\n    inputs = Input(input_size)\n    \n    # Initial convolution to project to higher dimension\n    x = Conv2D(64, kernel_size=7, strides=2, padding='same')(inputs)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    \n    # Encoder path with Mamba blocks\n    # Level 1\n    x1 = mamba_block(x, hidden_dim=128, ssm_dim=64)\n    x1 = mamba_block(x1, hidden_dim=128, ssm_dim=64)\n    p1 = MaxPooling2D(pool_size=(2, 2))(x1)\n    \n    # Level 2\n    x2 = mamba_block(p1, hidden_dim=256, ssm_dim=128)\n    x2 = mamba_block(x2, hidden_dim=256, ssm_dim=128)\n    p2 = MaxPooling2D(pool_size=(2, 2))(x2)\n    \n    # Level 3\n    x3 = mamba_block(p2, hidden_dim=512, ssm_dim=256)\n    x3 = mamba_block(x3, hidden_dim=512, ssm_dim=256)\n    p3 = MaxPooling2D(pool_size=(2, 2))(x3)\n    \n    # Level 4 (bottleneck)\n    x4 = mamba_block(p3, hidden_dim=1024, ssm_dim=512)\n    x4 = mamba_block(x4, hidden_dim=1024, ssm_dim=512)\n    \n    # Decoder path with skip connections\n    # Up level 4 to level 3\n    u3 = Conv2DTranspose(512, kernel_size=3, strides=2, padding='same')(x4)\n    u3 = concatenate([u3, x3], axis=-1)\n    u3 = Conv2D(512, kernel_size=3, padding='same')(u3)\n    u3 = BatchNormalization()(u3)\n    u3 = ReLU()(u3)\n    \n    # Up level 3 to level 2\n    u2 = Conv2DTranspose(256, kernel_size=3, strides=2, padding='same')(u3)\n    u2 = concatenate([u2, x2], axis=-1)\n    u2 = Conv2D(256, kernel_size=3, padding='same')(u2)\n    u2 = BatchNormalization()(u2)\n    u2 = ReLU()(u2)\n    \n    # Up level 2 to level 1\n    u1 = Conv2DTranspose(128, kernel_size=3, strides=2, padding='same')(u2)\n    u1 = concatenate([u1, x1], axis=-1)\n    u1 = Conv2D(128, kernel_size=3, padding='same')(u1)\n    u1 = BatchNormalization()(u1)\n    u1 = ReLU()(u1)\n    \n    # Final upsampling to original resolution\n    u0 = Conv2DTranspose(64, kernel_size=3, strides=2, padding='same')(u1)\n    u0 = Conv2D(64, kernel_size=3, padding='same')(u0)\n    u0 = BatchNormalization()(u0)\n    u0 = ReLU()(u0)\n    \n    # Output layer\n    outputs = Conv2D(1, kernel_size=1, activation='sigmoid')(u0)\n    \n    return Model(inputs=inputs, outputs=outputs)\n\n# KFold Splits\ndef save_kfold_splits(X, y, n_splits, seed, save_path):\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    splits = [(train_idx.tolist(), test_idx.tolist()) for train_idx, test_idx in kf.split(X)]\n    with open(save_path, 'wb') as f:\n        pickle.dump(splits, f)\n\ndef load_kfold_splits(file_path):\n    with open(file_path, 'rb') as f:\n        return pickle.load(f)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:39:03.993101Z","iopub.execute_input":"2025-04-18T16:39:03.993410Z","iopub.status.idle":"2025-04-18T16:39:04.006385Z","shell.execute_reply.started":"2025-04-18T16:39:03.993386Z","shell.execute_reply":"2025-04-18T16:39:04.005637Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Main Function\ndef main():\n    print(\"Loading and preprocessing dataset...\")\n    X, y_endo, _, _ = load_dataset(DATA_PATH)\n    save_kfold_splits(X, y_endo, N_FOLDS, SEED, \"kfold_splits.pkl\")\n    \n    splits = load_kfold_splits(\"kfold_splits.pkl\")\n    fold_results = []\n    \n    for fold, (train_idx, test_idx) in enumerate(splits):\n        print(f\"\\n=== Fold {fold + 1}/{N_FOLDS} ===\")\n        \n        X_train, X_test = X[train_idx], X[test_idx]\n        y_train, y_test = y_endo[train_idx], y_endo[test_idx]\n        \n        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=SEED)\n        \n        model = transunet_mamba()\n        model.compile(optimizer=Adam(learning_rate=INIT_LR), \n                      loss=bce_dice_loss, \n                      metrics=[dice_coef, iou_coef, 'accuracy'])\n        \n        callbacks = [\n            ModelCheckpoint(f\"mamba_unet_fold{fold}_best.keras\", \n                          monitor='val_dice_coef', \n                          mode='max', \n                          save_best_only=True, \n                          verbose=1),\n            EarlyStopping(monitor='val_dice_coef', \n                         patience=15, \n                         mode='max', \n                         verbose=1),\n            ReduceLROnPlateau(monitor='val_dice_coef', \n                            factor=0.5, \n                            patience=5, \n                            min_lr=1e-6, \n                            mode='max', \n                            verbose=1)\n        ]\n        \n        print(f\"Training on {len(X_train)} samples, validating on {len(X_val)} samples\")\n        model.fit(X_train, y_train, \n                batch_size=BATCH_SIZE, \n                epochs=EPOCHS, \n                validation_data=(X_val, y_val), \n                callbacks=callbacks, \n                verbose=1)\n        \n        model.load_weights(f\"mamba_unet_fold{fold}_best.keras\")\n        print(f\"Evaluating on {len(X_test)} test samples\")\n        y_pred = (model.predict(X_test, batch_size=BATCH_SIZE) > 0.5).astype(np.float32)\n        \n        dice_scores = [dice_coef(y_test[i], y_pred[i]).numpy() for i in range(len(y_test))]\n        fold_results.append({'dice': np.mean(dice_scores)})\n    \n    print(\"\\n=== Final Cross-Validation Results ===\")\n    avg_dice = np.mean([r['dice'] for r in fold_results])\n    print(f\"Average Dice: {avg_dice:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:39:08.741861Z","iopub.execute_input":"2025-04-18T16:39:08.742647Z","iopub.status.idle":"2025-04-18T16:39:08.750446Z","shell.execute_reply.started":"2025-04-18T16:39:08.742616Z","shell.execute_reply":"2025-04-18T16:39:08.749889Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"if __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:55:49.908780Z","iopub.execute_input":"2025-04-17T14:55:49.909678Z","execution_failed":"2025-04-17T18:38:32.318Z"}},"outputs":[{"name":"stdout","text":"Loading and preprocessing dataset...\n","output_type":"stream"},{"name":"stderr","text":"Loading patients: 100%|██████████| 500/500 [07:21<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n=== Fold 1/3 ===\nTraining on 4799 samples, validating on 534 samples\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1744902243.078811     278 service.cc:148] XLA service 0x7858d0002490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1744902243.078859     278 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1744902243.078863     278 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1744902246.491155     278 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1744902285.386093     278 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386ms/step - accuracy: 0.5907 - dice_coef: 0.3951 - iou_coef: 0.2546 - loss: 0.9742\nEpoch 1: val_dice_coef improved from -inf to 0.61768, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 422ms/step - accuracy: 0.5908 - dice_coef: 0.3953 - iou_coef: 0.2547 - loss: 0.9738 - val_accuracy: 0.6573 - val_dice_coef: 0.6177 - val_iou_coef: 0.4483 - val_loss: 0.5345 - learning_rate: 1.0000e-04\nEpoch 2/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.6587 - dice_coef: 0.6876 - iou_coef: 0.5261 - loss: 0.4245\nEpoch 2: val_dice_coef improved from 0.61768 to 0.78362, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 334ms/step - accuracy: 0.6587 - dice_coef: 0.6876 - iou_coef: 0.5262 - loss: 0.4244 - val_accuracy: 0.6649 - val_dice_coef: 0.7836 - val_iou_coef: 0.6454 - val_loss: 0.3029 - learning_rate: 1.0000e-04\nEpoch 3/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.6627 - dice_coef: 0.8029 - iou_coef: 0.6717 - loss: 0.2719\nEpoch 3: val_dice_coef improved from 0.78362 to 0.82475, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 333ms/step - accuracy: 0.6627 - dice_coef: 0.8029 - iou_coef: 0.6717 - loss: 0.2719 - val_accuracy: 0.6636 - val_dice_coef: 0.8247 - val_iou_coef: 0.7027 - val_loss: 0.2442 - learning_rate: 1.0000e-04\nEpoch 4/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.6642 - dice_coef: 0.8454 - iou_coef: 0.7331 - loss: 0.2180\nEpoch 4: val_dice_coef improved from 0.82475 to 0.84510, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 333ms/step - accuracy: 0.6642 - dice_coef: 0.8454 - iou_coef: 0.7331 - loss: 0.2180 - val_accuracy: 0.6624 - val_dice_coef: 0.8451 - val_iou_coef: 0.7332 - val_loss: 0.2273 - learning_rate: 1.0000e-04\nEpoch 5/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.6652 - dice_coef: 0.8697 - iou_coef: 0.7701 - loss: 0.1867\nEpoch 5: val_dice_coef improved from 0.84510 to 0.86953, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 335ms/step - accuracy: 0.6652 - dice_coef: 0.8697 - iou_coef: 0.7702 - loss: 0.1867 - val_accuracy: 0.6666 - val_dice_coef: 0.8695 - val_iou_coef: 0.7700 - val_loss: 0.1936 - learning_rate: 1.0000e-04\nEpoch 6/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.6666 - dice_coef: 0.8847 - iou_coef: 0.7938 - loss: 0.1665\nEpoch 6: val_dice_coef improved from 0.86953 to 0.88701, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 334ms/step - accuracy: 0.6666 - dice_coef: 0.8847 - iou_coef: 0.7938 - loss: 0.1665 - val_accuracy: 0.6687 - val_dice_coef: 0.8870 - val_iou_coef: 0.7976 - val_loss: 0.1661 - learning_rate: 1.0000e-04\nEpoch 7/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.6639 - dice_coef: 0.8953 - iou_coef: 0.8109 - loss: 0.1529\nEpoch 7: val_dice_coef improved from 0.88701 to 0.89748, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 335ms/step - accuracy: 0.6639 - dice_coef: 0.8953 - iou_coef: 0.8109 - loss: 0.1529 - val_accuracy: 0.6675 - val_dice_coef: 0.8975 - val_iou_coef: 0.8146 - val_loss: 0.1536 - learning_rate: 1.0000e-04\nEpoch 8/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.6685 - dice_coef: 0.9018 - iou_coef: 0.8216 - loss: 0.1437\nEpoch 8: val_dice_coef did not improve from 0.89748\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 332ms/step - accuracy: 0.6685 - dice_coef: 0.9018 - iou_coef: 0.8216 - loss: 0.1437 - val_accuracy: 0.6686 - val_dice_coef: 0.8867 - val_iou_coef: 0.7974 - val_loss: 0.1663 - learning_rate: 1.0000e-04\nEpoch 9/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.6673 - dice_coef: 0.9087 - iou_coef: 0.8330 - loss: 0.1345\nEpoch 9: val_dice_coef did not improve from 0.89748\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 333ms/step - accuracy: 0.6673 - dice_coef: 0.9087 - iou_coef: 0.8330 - loss: 0.1345 - val_accuracy: 0.6689 - val_dice_coef: 0.8961 - val_iou_coef: 0.8123 - val_loss: 0.1574 - learning_rate: 1.0000e-04\nEpoch 10/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.6666 - dice_coef: 0.9115 - iou_coef: 0.8377 - loss: 0.1302\nEpoch 10: val_dice_coef did not improve from 0.89748\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 332ms/step - accuracy: 0.6666 - dice_coef: 0.9115 - iou_coef: 0.8377 - loss: 0.1302 - val_accuracy: 0.6677 - val_dice_coef: 0.8940 - val_iou_coef: 0.8091 - val_loss: 0.1589 - learning_rate: 1.0000e-04\nEpoch 11/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.6659 - dice_coef: 0.9154 - iou_coef: 0.8443 - loss: 0.1248\nEpoch 11: val_dice_coef improved from 0.89748 to 0.89991, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 334ms/step - accuracy: 0.6659 - dice_coef: 0.9154 - iou_coef: 0.8443 - loss: 0.1248 - val_accuracy: 0.6682 - val_dice_coef: 0.8999 - val_iou_coef: 0.8188 - val_loss: 0.1504 - learning_rate: 1.0000e-04\nEpoch 12/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.6679 - dice_coef: 0.9191 - iou_coef: 0.8506 - loss: 0.1194\nEpoch 12: val_dice_coef did not improve from 0.89991\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 332ms/step - accuracy: 0.6679 - dice_coef: 0.9191 - iou_coef: 0.8506 - loss: 0.1194 - val_accuracy: 0.6697 - val_dice_coef: 0.8750 - val_iou_coef: 0.7789 - val_loss: 0.1951 - learning_rate: 1.0000e-04\nEpoch 13/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.6698 - dice_coef: 0.9194 - iou_coef: 0.8512 - loss: 0.1187\nEpoch 13: val_dice_coef did not improve from 0.89991\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 332ms/step - accuracy: 0.6698 - dice_coef: 0.9194 - iou_coef: 0.8512 - loss: 0.1187 - val_accuracy: 0.6648 - val_dice_coef: 0.8885 - val_iou_coef: 0.8002 - val_loss: 0.1727 - learning_rate: 1.0000e-04\nEpoch 14/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.6683 - dice_coef: 0.9231 - iou_coef: 0.8574 - loss: 0.1135\nEpoch 14: val_dice_coef improved from 0.89991 to 0.90626, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 334ms/step - accuracy: 0.6683 - dice_coef: 0.9231 - iou_coef: 0.8574 - loss: 0.1135 - val_accuracy: 0.6708 - val_dice_coef: 0.9063 - val_iou_coef: 0.8291 - val_loss: 0.1397 - learning_rate: 1.0000e-04\nEpoch 15/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.6677 - dice_coef: 0.9252 - iou_coef: 0.8609 - loss: 0.1108\nEpoch 15: val_dice_coef did not improve from 0.90626\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 333ms/step - accuracy: 0.6677 - dice_coef: 0.9252 - iou_coef: 0.8609 - loss: 0.1108 - val_accuracy: 0.6666 - val_dice_coef: 0.9031 - val_iou_coef: 0.8241 - val_loss: 0.1513 - learning_rate: 1.0000e-04\nEpoch 16/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - accuracy: 0.6680 - dice_coef: 0.9271 - iou_coef: 0.8644 - loss: 0.1080\nEpoch 16: val_dice_coef did not improve from 0.90626\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 331ms/step - accuracy: 0.6680 - dice_coef: 0.9271 - iou_coef: 0.8644 - loss: 0.1080 - val_accuracy: 0.6687 - val_dice_coef: 0.9015 - val_iou_coef: 0.8215 - val_loss: 0.1487 - learning_rate: 1.0000e-04\nEpoch 17/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.6696 - dice_coef: 0.9286 - iou_coef: 0.8669 - loss: 0.1054\nEpoch 17: val_dice_coef improved from 0.90626 to 0.90882, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 333ms/step - accuracy: 0.6696 - dice_coef: 0.9286 - iou_coef: 0.8669 - loss: 0.1054 - val_accuracy: 0.6707 - val_dice_coef: 0.9088 - val_iou_coef: 0.8335 - val_loss: 0.1419 - learning_rate: 1.0000e-04\nEpoch 18/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.6678 - dice_coef: 0.9311 - iou_coef: 0.8713 - loss: 0.1018\nEpoch 18: val_dice_coef improved from 0.90882 to 0.91192, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 334ms/step - accuracy: 0.6678 - dice_coef: 0.9311 - iou_coef: 0.8713 - loss: 0.1018 - val_accuracy: 0.6677 - val_dice_coef: 0.9119 - val_iou_coef: 0.8386 - val_loss: 0.1399 - learning_rate: 1.0000e-04\nEpoch 19/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.6701 - dice_coef: 0.9335 - iou_coef: 0.8755 - loss: 0.0983\nEpoch 19: val_dice_coef did not improve from 0.91192\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 332ms/step - accuracy: 0.6701 - dice_coef: 0.9335 - iou_coef: 0.8755 - loss: 0.0983 - val_accuracy: 0.6698 - val_dice_coef: 0.9105 - val_iou_coef: 0.8363 - val_loss: 0.1417 - learning_rate: 1.0000e-04\nEpoch 20/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.6696 - dice_coef: 0.9344 - iou_coef: 0.8770 - loss: 0.0970\nEpoch 20: val_dice_coef did not improve from 0.91192\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 332ms/step - accuracy: 0.6696 - dice_coef: 0.9344 - iou_coef: 0.8770 - loss: 0.0970 - val_accuracy: 0.6678 - val_dice_coef: 0.9100 - val_iou_coef: 0.8357 - val_loss: 0.1428 - learning_rate: 1.0000e-04\nEpoch 21/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.6694 - dice_coef: 0.9372 - iou_coef: 0.8821 - loss: 0.0931\nEpoch 21: val_dice_coef improved from 0.91192 to 0.91645, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 333ms/step - accuracy: 0.6694 - dice_coef: 0.9372 - iou_coef: 0.8821 - loss: 0.0931 - val_accuracy: 0.6695 - val_dice_coef: 0.9165 - val_iou_coef: 0.8462 - val_loss: 0.1323 - learning_rate: 1.0000e-04\nEpoch 22/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.6695 - dice_coef: 0.9394 - iou_coef: 0.8859 - loss: 0.0897\nEpoch 22: val_dice_coef did not improve from 0.91645\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 332ms/step - accuracy: 0.6695 - dice_coef: 0.9394 - iou_coef: 0.8859 - loss: 0.0898 - val_accuracy: 0.6675 - val_dice_coef: 0.9102 - val_iou_coef: 0.8357 - val_loss: 0.1376 - learning_rate: 1.0000e-04\nEpoch 23/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.6690 - dice_coef: 0.9396 - iou_coef: 0.8862 - loss: 0.0896\nEpoch 23: val_dice_coef did not improve from 0.91645\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 332ms/step - accuracy: 0.6690 - dice_coef: 0.9396 - iou_coef: 0.8862 - loss: 0.0896 - val_accuracy: 0.6705 - val_dice_coef: 0.9136 - val_iou_coef: 0.8415 - val_loss: 0.1379 - learning_rate: 1.0000e-04\nEpoch 24/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.6690 - dice_coef: 0.9423 - iou_coef: 0.8909 - loss: 0.0857\nEpoch 24: val_dice_coef did not improve from 0.91645\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 331ms/step - accuracy: 0.6690 - dice_coef: 0.9423 - iou_coef: 0.8909 - loss: 0.0857 - val_accuracy: 0.6679 - val_dice_coef: 0.9076 - val_iou_coef: 0.8316 - val_loss: 0.1485 - learning_rate: 1.0000e-04\nEpoch 25/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.6705 - dice_coef: 0.9427 - iou_coef: 0.8918 - loss: 0.0851\nEpoch 25: val_dice_coef improved from 0.91645 to 0.91696, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 333ms/step - accuracy: 0.6705 - dice_coef: 0.9427 - iou_coef: 0.8918 - loss: 0.0851 - val_accuracy: 0.6689 - val_dice_coef: 0.9170 - val_iou_coef: 0.8471 - val_loss: 0.1308 - learning_rate: 1.0000e-04\nEpoch 26/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.6713 - dice_coef: 0.9457 - iou_coef: 0.8970 - loss: 0.0805\nEpoch 26: val_dice_coef did not improve from 0.91696\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 332ms/step - accuracy: 0.6713 - dice_coef: 0.9457 - iou_coef: 0.8970 - loss: 0.0805 - val_accuracy: 0.6687 - val_dice_coef: 0.9166 - val_iou_coef: 0.8467 - val_loss: 0.1369 - learning_rate: 1.0000e-04\nEpoch 27/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - accuracy: 0.6694 - dice_coef: 0.9474 - iou_coef: 0.9001 - loss: 0.0782\nEpoch 27: val_dice_coef improved from 0.91696 to 0.91912, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 333ms/step - accuracy: 0.6694 - dice_coef: 0.9474 - iou_coef: 0.9001 - loss: 0.0782 - val_accuracy: 0.6701 - val_dice_coef: 0.9191 - val_iou_coef: 0.8508 - val_loss: 0.1322 - learning_rate: 1.0000e-04\nEpoch 28/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.6714 - dice_coef: 0.9481 - iou_coef: 0.9014 - loss: 0.0768\nEpoch 28: val_dice_coef did not improve from 0.91912\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 331ms/step - accuracy: 0.6714 - dice_coef: 0.9481 - iou_coef: 0.9014 - loss: 0.0768 - val_accuracy: 0.6708 - val_dice_coef: 0.9162 - val_iou_coef: 0.8460 - val_loss: 0.1366 - learning_rate: 1.0000e-04\nEpoch 29/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.6687 - dice_coef: 0.9485 - iou_coef: 0.9021 - loss: 0.0768\nEpoch 29: val_dice_coef did not improve from 0.91912\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 332ms/step - accuracy: 0.6688 - dice_coef: 0.9485 - iou_coef: 0.9021 - loss: 0.0768 - val_accuracy: 0.6713 - val_dice_coef: 0.9164 - val_iou_coef: 0.8460 - val_loss: 0.1436 - learning_rate: 1.0000e-04\nEpoch 30/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.6686 - dice_coef: 0.9494 - iou_coef: 0.9037 - loss: 0.0752\nEpoch 30: val_dice_coef did not improve from 0.91912\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 332ms/step - accuracy: 0.6686 - dice_coef: 0.9494 - iou_coef: 0.9037 - loss: 0.0752 - val_accuracy: 0.6678 - val_dice_coef: 0.9183 - val_iou_coef: 0.8493 - val_loss: 0.1327 - learning_rate: 1.0000e-04\nEpoch 31/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - accuracy: 0.6712 - dice_coef: 0.9515 - iou_coef: 0.9075 - loss: 0.0719\nEpoch 31: val_dice_coef did not improve from 0.91912\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 331ms/step - accuracy: 0.6712 - dice_coef: 0.9515 - iou_coef: 0.9075 - loss: 0.0719 - val_accuracy: 0.6691 - val_dice_coef: 0.9180 - val_iou_coef: 0.8488 - val_loss: 0.1425 - learning_rate: 1.0000e-04\nEpoch 32/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.6714 - dice_coef: 0.9537 - iou_coef: 0.9116 - loss: 0.0685\nEpoch 32: val_dice_coef did not improve from 0.91912\n\nEpoch 32: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 332ms/step - accuracy: 0.6714 - dice_coef: 0.9537 - iou_coef: 0.9116 - loss: 0.0685 - val_accuracy: 0.6687 - val_dice_coef: 0.9184 - val_iou_coef: 0.8497 - val_loss: 0.1357 - learning_rate: 1.0000e-04\nEpoch 33/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.6721 - dice_coef: 0.9590 - iou_coef: 0.9214 - loss: 0.0608\nEpoch 33: val_dice_coef improved from 0.91912 to 0.92212, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 333ms/step - accuracy: 0.6721 - dice_coef: 0.9590 - iou_coef: 0.9214 - loss: 0.0608 - val_accuracy: 0.6687 - val_dice_coef: 0.9221 - val_iou_coef: 0.8559 - val_loss: 0.1328 - learning_rate: 5.0000e-05\nEpoch 34/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - accuracy: 0.6714 - dice_coef: 0.9613 - iou_coef: 0.9256 - loss: 0.0575\nEpoch 34: val_dice_coef did not improve from 0.92212\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 331ms/step - accuracy: 0.6714 - dice_coef: 0.9613 - iou_coef: 0.9256 - loss: 0.0575 - val_accuracy: 0.6696 - val_dice_coef: 0.9217 - val_iou_coef: 0.8552 - val_loss: 0.1346 - learning_rate: 5.0000e-05\nEpoch 35/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.6702 - dice_coef: 0.9623 - iou_coef: 0.9273 - loss: 0.0561\nEpoch 35: val_dice_coef did not improve from 0.92212\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 332ms/step - accuracy: 0.6702 - dice_coef: 0.9623 - iou_coef: 0.9273 - loss: 0.0561 - val_accuracy: 0.6689 - val_dice_coef: 0.9217 - val_iou_coef: 0.8553 - val_loss: 0.1338 - learning_rate: 5.0000e-05\nEpoch 36/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.6724 - dice_coef: 0.9631 - iou_coef: 0.9288 - loss: 0.0549\nEpoch 36: val_dice_coef did not improve from 0.92212\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 332ms/step - accuracy: 0.6724 - dice_coef: 0.9631 - iou_coef: 0.9288 - loss: 0.0549 - val_accuracy: 0.6704 - val_dice_coef: 0.9219 - val_iou_coef: 0.8556 - val_loss: 0.1346 - learning_rate: 5.0000e-05\nEpoch 37/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.6713 - dice_coef: 0.9644 - iou_coef: 0.9313 - loss: 0.0529\nEpoch 37: val_dice_coef improved from 0.92212 to 0.92227, saving model to mamba_unet_fold0_best.keras\n\nEpoch 37: val_dice_coef improved from 0.92212 to 0.92227, saving model to mamba_unet_fold0_best.keras\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"##Mamba Try Number 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:38:21.694516Z","iopub.execute_input":"2025-04-18T16:38:21.694843Z","iopub.status.idle":"2025-04-18T16:38:21.698755Z","shell.execute_reply.started":"2025-04-18T16:38:21.694820Z","shell.execute_reply":"2025-04-18T16:38:21.697975Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"if __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:39:16.517140Z","iopub.execute_input":"2025-04-18T16:39:16.517441Z","execution_failed":"2025-04-18T19:31:36.577Z"}},"outputs":[{"name":"stdout","text":"Loading and preprocessing dataset...\n","output_type":"stream"},{"name":"stderr","text":"Loading patients: 100%|██████████| 500/500 [06:01<00:00,  1.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n=== Fold 1/3 ===\nTraining on 4799 samples, validating on 534 samples\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1744994760.122192     175 service.cc:148] XLA service 0x7fc440002780 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1744994760.127102     175 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1744994760.127125     175 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1744994763.766654     175 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1744994808.541280     175 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475ms/step - accuracy: 0.6069 - dice_coef: 0.4381 - iou_coef: 0.2916 - loss: 0.8788\nEpoch 1: val_dice_coef improved from -inf to 0.23565, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 514ms/step - accuracy: 0.6069 - dice_coef: 0.4383 - iou_coef: 0.2918 - loss: 0.8784 - val_accuracy: 0.5981 - val_dice_coef: 0.2356 - val_iou_coef: 0.1351 - val_loss: 1.0325 - learning_rate: 1.0000e-04\nEpoch 2/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - accuracy: 0.6587 - dice_coef: 0.7245 - iou_coef: 0.5705 - loss: 0.3798\nEpoch 2: val_dice_coef improved from 0.23565 to 0.79953, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 418ms/step - accuracy: 0.6587 - dice_coef: 0.7245 - iou_coef: 0.5705 - loss: 0.3798 - val_accuracy: 0.6650 - val_dice_coef: 0.7995 - val_iou_coef: 0.6674 - val_loss: 0.2968 - learning_rate: 1.0000e-04\nEpoch 3/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - accuracy: 0.6626 - dice_coef: 0.8117 - iou_coef: 0.6844 - loss: 0.2643\nEpoch 3: val_dice_coef improved from 0.79953 to 0.84259, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 420ms/step - accuracy: 0.6626 - dice_coef: 0.8118 - iou_coef: 0.6844 - loss: 0.2643 - val_accuracy: 0.6666 - val_dice_coef: 0.8426 - val_iou_coef: 0.7292 - val_loss: 0.2349 - learning_rate: 1.0000e-04\nEpoch 4/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - accuracy: 0.6641 - dice_coef: 0.8524 - iou_coef: 0.7436 - loss: 0.2109\nEpoch 4: val_dice_coef did not improve from 0.84259\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 417ms/step - accuracy: 0.6641 - dice_coef: 0.8524 - iou_coef: 0.7436 - loss: 0.2109 - val_accuracy: 0.6483 - val_dice_coef: 0.7777 - val_iou_coef: 0.6392 - val_loss: 0.3347 - learning_rate: 1.0000e-04\nEpoch 5/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - accuracy: 0.6661 - dice_coef: 0.8710 - iou_coef: 0.7723 - loss: 0.1868\nEpoch 5: val_dice_coef improved from 0.84259 to 0.86839, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 417ms/step - accuracy: 0.6661 - dice_coef: 0.8710 - iou_coef: 0.7723 - loss: 0.1868 - val_accuracy: 0.6647 - val_dice_coef: 0.8684 - val_iou_coef: 0.7684 - val_loss: 0.1932 - learning_rate: 1.0000e-04\nEpoch 6/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - accuracy: 0.6658 - dice_coef: 0.8850 - iou_coef: 0.7943 - loss: 0.1670\nEpoch 6: val_dice_coef did not improve from 0.86839\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 417ms/step - accuracy: 0.6658 - dice_coef: 0.8850 - iou_coef: 0.7943 - loss: 0.1670 - val_accuracy: 0.6587 - val_dice_coef: 0.8446 - val_iou_coef: 0.7328 - val_loss: 0.2323 - learning_rate: 1.0000e-04\nEpoch 7/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - accuracy: 0.6672 - dice_coef: 0.8957 - iou_coef: 0.8114 - loss: 0.1521\nEpoch 7: val_dice_coef improved from 0.86839 to 0.88965, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 418ms/step - accuracy: 0.6672 - dice_coef: 0.8957 - iou_coef: 0.8114 - loss: 0.1521 - val_accuracy: 0.6678 - val_dice_coef: 0.8896 - val_iou_coef: 0.8019 - val_loss: 0.1668 - learning_rate: 1.0000e-04\nEpoch 8/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - accuracy: 0.6662 - dice_coef: 0.8999 - iou_coef: 0.8185 - loss: 0.1470\nEpoch 8: val_dice_coef did not improve from 0.88965\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 417ms/step - accuracy: 0.6662 - dice_coef: 0.8999 - iou_coef: 0.8185 - loss: 0.1470 - val_accuracy: 0.6674 - val_dice_coef: 0.8691 - val_iou_coef: 0.7700 - val_loss: 0.2095 - learning_rate: 1.0000e-04\nEpoch 9/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - accuracy: 0.6677 - dice_coef: 0.9059 - iou_coef: 0.8283 - loss: 0.1384\nEpoch 9: val_dice_coef improved from 0.88965 to 0.89912, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 417ms/step - accuracy: 0.6677 - dice_coef: 0.9059 - iou_coef: 0.8283 - loss: 0.1384 - val_accuracy: 0.6692 - val_dice_coef: 0.8991 - val_iou_coef: 0.8174 - val_loss: 0.1535 - learning_rate: 1.0000e-04\nEpoch 10/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - accuracy: 0.6702 - dice_coef: 0.9091 - iou_coef: 0.8338 - loss: 0.1331\nEpoch 10: val_dice_coef did not improve from 0.89912\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 417ms/step - accuracy: 0.6702 - dice_coef: 0.9091 - iou_coef: 0.8338 - loss: 0.1331 - val_accuracy: 0.6689 - val_dice_coef: 0.8848 - val_iou_coef: 0.7946 - val_loss: 0.1801 - learning_rate: 1.0000e-04\nEpoch 11/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - accuracy: 0.6677 - dice_coef: 0.9150 - iou_coef: 0.8435 - loss: 0.1253\nEpoch 11: val_dice_coef improved from 0.89912 to 0.90058, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 419ms/step - accuracy: 0.6677 - dice_coef: 0.9150 - iou_coef: 0.8435 - loss: 0.1253 - val_accuracy: 0.6670 - val_dice_coef: 0.9006 - val_iou_coef: 0.8199 - val_loss: 0.1481 - learning_rate: 1.0000e-04\nEpoch 12/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - accuracy: 0.6675 - dice_coef: 0.9177 - iou_coef: 0.8483 - loss: 0.1217\nEpoch 12: val_dice_coef improved from 0.90058 to 0.90332, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 420ms/step - accuracy: 0.6675 - dice_coef: 0.9177 - iou_coef: 0.8483 - loss: 0.1217 - val_accuracy: 0.6691 - val_dice_coef: 0.9033 - val_iou_coef: 0.8243 - val_loss: 0.1521 - learning_rate: 1.0000e-04\nEpoch 13/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - accuracy: 0.6676 - dice_coef: 0.9208 - iou_coef: 0.8535 - loss: 0.1169\nEpoch 13: val_dice_coef improved from 0.90332 to 0.90680, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 419ms/step - accuracy: 0.6676 - dice_coef: 0.9208 - iou_coef: 0.8535 - loss: 0.1169 - val_accuracy: 0.6692 - val_dice_coef: 0.9068 - val_iou_coef: 0.8301 - val_loss: 0.1411 - learning_rate: 1.0000e-04\nEpoch 14/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - accuracy: 0.6693 - dice_coef: 0.9218 - iou_coef: 0.8551 - loss: 0.1158\nEpoch 14: val_dice_coef improved from 0.90680 to 0.90829, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 417ms/step - accuracy: 0.6693 - dice_coef: 0.9218 - iou_coef: 0.8551 - loss: 0.1158 - val_accuracy: 0.6698 - val_dice_coef: 0.9083 - val_iou_coef: 0.8325 - val_loss: 0.1418 - learning_rate: 1.0000e-04\nEpoch 15/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - accuracy: 0.6682 - dice_coef: 0.9239 - iou_coef: 0.8589 - loss: 0.1123\nEpoch 15: val_dice_coef improved from 0.90829 to 0.91038, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 418ms/step - accuracy: 0.6682 - dice_coef: 0.9239 - iou_coef: 0.8589 - loss: 0.1123 - val_accuracy: 0.6682 - val_dice_coef: 0.9104 - val_iou_coef: 0.8361 - val_loss: 0.1395 - learning_rate: 1.0000e-04\nEpoch 16/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - accuracy: 0.6690 - dice_coef: 0.9273 - iou_coef: 0.8647 - loss: 0.1073\nEpoch 16: val_dice_coef did not improve from 0.91038\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 416ms/step - accuracy: 0.6690 - dice_coef: 0.9273 - iou_coef: 0.8647 - loss: 0.1073 - val_accuracy: 0.6691 - val_dice_coef: 0.9036 - val_iou_coef: 0.8248 - val_loss: 0.1503 - learning_rate: 1.0000e-04\nEpoch 17/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - accuracy: 0.6689 - dice_coef: 0.9290 - iou_coef: 0.8676 - loss: 0.1054\nEpoch 17: val_dice_coef did not improve from 0.91038\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 418ms/step - accuracy: 0.6689 - dice_coef: 0.9290 - iou_coef: 0.8676 - loss: 0.1054 - val_accuracy: 0.6619 - val_dice_coef: 0.8855 - val_iou_coef: 0.7955 - val_loss: 0.1801 - learning_rate: 1.0000e-04\nEpoch 18/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - accuracy: 0.6694 - dice_coef: 0.9307 - iou_coef: 0.8705 - loss: 0.1028\nEpoch 18: val_dice_coef improved from 0.91038 to 0.91069, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 419ms/step - accuracy: 0.6694 - dice_coef: 0.9307 - iou_coef: 0.8705 - loss: 0.1028 - val_accuracy: 0.6666 - val_dice_coef: 0.9107 - val_iou_coef: 0.8367 - val_loss: 0.1389 - learning_rate: 1.0000e-04\nEpoch 19/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - accuracy: 0.6710 - dice_coef: 0.9329 - iou_coef: 0.8744 - loss: 0.0992\nEpoch 19: val_dice_coef improved from 0.91069 to 0.91248, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 420ms/step - accuracy: 0.6710 - dice_coef: 0.9329 - iou_coef: 0.8744 - loss: 0.0992 - val_accuracy: 0.6706 - val_dice_coef: 0.9125 - val_iou_coef: 0.8395 - val_loss: 0.1344 - learning_rate: 1.0000e-04\nEpoch 20/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - accuracy: 0.6694 - dice_coef: 0.9358 - iou_coef: 0.8795 - loss: 0.0951\nEpoch 20: val_dice_coef improved from 0.91248 to 0.91572, saving model to mamba_unet_fold0_best.keras\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 421ms/step - accuracy: 0.6694 - dice_coef: 0.9358 - iou_coef: 0.8795 - loss: 0.0951 - val_accuracy: 0.6697 - val_dice_coef: 0.9157 - val_iou_coef: 0.8450 - val_loss: 0.1328 - learning_rate: 1.0000e-04\nEpoch 21/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - accuracy: 0.6696 - dice_coef: 0.9366 - iou_coef: 0.8809 - loss: 0.0940\nEpoch 21: val_dice_coef did not improve from 0.91572\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 416ms/step - accuracy: 0.6696 - dice_coef: 0.9366 - iou_coef: 0.8808 - loss: 0.0940 - val_accuracy: 0.6683 - val_dice_coef: 0.9074 - val_iou_coef: 0.8312 - val_loss: 0.1498 - learning_rate: 1.0000e-04\nEpoch 22/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - accuracy: 0.6685 - dice_coef: 0.9375 - iou_coef: 0.8826 - loss: 0.0927\nEpoch 22: val_dice_coef did not improve from 0.91572\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 418ms/step - accuracy: 0.6685 - dice_coef: 0.9375 - iou_coef: 0.8826 - loss: 0.0927 - val_accuracy: 0.6678 - val_dice_coef: 0.9102 - val_iou_coef: 0.8358 - val_loss: 0.1449 - learning_rate: 1.0000e-04\nEpoch 23/100\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - accuracy: 0.6707 - dice_coef: 0.9380 - iou_coef: 0.8833 - loss: 0.0919\nEpoch 23: val_dice_coef did not improve from 0.91572\n\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 417ms/step - accuracy: 0.6707 - dice_coef: 0.9380 - iou_coef: 0.8833 - loss: 0.0919 - val_accuracy: 0.6678 - val_dice_coef: 0.9140 - val_iou_coef: 0.8424 - val_loss: 0.1382 - learning_rate: 1.0000e-04\nEpoch 24/100\n\u001b[1m542/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m23s\u001b[0m 403ms/step - accuracy: 0.6721 - dice_coef: 0.9419 - iou_coef: 0.8902 - loss: 0.0863","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### Here start the implementaion of the true mamba structure with SSM that failed.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport nibabel as nib\nfrom skimage.transform import resize\nfrom scipy.ndimage import rotate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, BatchNormalization, ReLU, MaxPooling2D,\n    Conv2DTranspose, concatenate, LayerNormalization, \n    Reshape, Dense, Multiply, Add, Lambda, Dropout\n)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nfrom sklearn.model_selection import KFold, train_test_split\nfrom tqdm import tqdm\nimport pickle\n\n# Configuration\nIMG_HEIGHT = 256\nIMG_WIDTH = 256\nIMG_CHANNELS = 1\nBATCH_SIZE = 4  # Reduced to help with memory\nEPOCHS = 100\nINIT_LR = 1e-4\nN_FOLDS = 3\nSEED = 42\nDATA_PATH = \"/kaggle/input/camus-dataset/database_nifti\"\n\n# Metrics (unchanged from your original code)\ndef dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_loss(y_true, y_pred):\n    return 1 - dice_coef(y_true, y_pred)\n\ndef bce_dice_loss(y_true, y_pred):\n    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n\n# Preprocessing (unchanged from your original code)\ndef load_nifti_image(file_path):\n    img = nib.load(file_path)\n    return np.squeeze(img.get_fdata())\n\ndef preprocess_patient_with_rotation(patient_folder, rotation_angles=[0, 90, 180, 270]):\n    # ... (keep your existing implementation)\n    pass\n\ndef load_dataset(base_path):\n    # ... (keep your existing implementation)\n    pass\n\n# ----------------------------\n# Mamba-Specific Components\n# ----------------------------\n\nclass SelectiveSSM(tf.keras.layers.Layer):\n    def __init__(self, d_model, ssm_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.d_model = d_model\n        self.ssm_dim = ssm_dim\n        \n        # Projections\n        self.in_proj = Dense(d_model * 3)  # Produces delta, B, and skip\n        self.out_proj = Dense(d_model)\n        \n        # SSM parameters\n        self.A = self.add_weight(shape=(ssm_dim,), initializer='random_normal')  # State matrix\n        self.C = self.add_weight(shape=(d_model, ssm_dim), initializer='random_normal')  # Output projection\n        \n    def call(self, x):\n        batch, seq_len, _ = x.shape\n        \n        # Project input to get delta, B, and skip connection\n        x_proj = self.in_proj(x)  # (batch, seq_len, 3*d_model)\n        delta, B, x_skip = tf.split(x_proj, 3, axis=-1)  # Each (batch, seq_len, d_model)\n        \n        # Discretization\n        A_bar = tf.exp(tf.einsum('bnd,d->bnd', delta, self.A))  # (batch, seq_len, d_model)\n        B_bar = tf.einsum('bnd,bnd->bn', delta, B)  # (batch, seq_len)\n        \n        # Selective scan\n        h = tf.zeros((batch, self.ssm_dim))\n        outputs = []\n        for t in range(seq_len):\n            h = A_bar[:,t,:] * h + B_bar[:,t,None]\n            yt = tf.einsum('bd,dk->bk', h, self.C)\n            outputs.append(yt)\n        \n        y = tf.stack(outputs, axis=1)  # (batch, seq_len, d_model)\n        return self.out_proj(y + x_skip)\n\ndef mamba_block(x, d_model, ssm_dim, dropout_rate=0.1):\n    \"\"\"Full Mamba block implementation\"\"\"\n    # 1. Layer normalization\n    x_norm = LayerNormalization()(x)\n    \n    # 2. Depthwise convolution for local feature mixing\n    x_conv = Conv2D(d_model, kernel_size=3, padding='same', groups=d_model)(x_norm)\n    \n    # 3. Tokenization: Flatten spatial dims to sequence\n    batch, h, w, c = tf.shape(x_conv)\n    tokens = Reshape((h * w, c))(x_conv)  # (batch, seq_len, d_model)\n    \n    # 4. Add positional embeddings\n    positions = tf.range(h * w)\n    pos_emb = tf.one_hot(positions, depth=h*w)\n    pos_emb = Dense(d_model)(pos_emb)  # (seq_len, d_model)\n    tokens = tokens + pos_emb[None,:,:]\n    \n    # 5. Apply selective SSM\n    ssm = SelectiveSSM(d_model, ssm_dim)(tokens)\n    \n    # 6. Reshape back to spatial\n    x_ssm = Reshape((h, w, d_model))(ssm)\n    \n    # 7. Project back to channel dimension\n    x_out = Conv2D(K.int_shape(x)[-1], kernel_size=1)(x_ssm)\n    \n    # 8. Add dropout and residual\n    x_out = Dropout(dropout_rate)(x_out)\n    return x + x_out\n\n# ----------------------------\n# CNN Front-End\n# ----------------------------\n\ndef cnn_feature_extractor(inputs):\n    \"\"\"CNN backbone to extract multi-scale features\"\"\"\n    # Level 1 (1/2 resolution)\n    x1 = Conv2D(64, kernel_size=3, strides=1, padding='same')(inputs)\n    x1 = BatchNormalization()(x1)\n    x1 = ReLU()(x1)\n    p1 = MaxPooling2D(pool_size=2)(x1)\n    \n    # Level 2 (1/4 resolution)\n    x2 = Conv2D(128, kernel_size=3, strides=1, padding='same')(p1)\n    x2 = BatchNormalization()(x2)\n    x2 = ReLU()(x2)\n    p2 = MaxPooling2D(pool_size=2)(x2)\n    \n    # Level 3 (1/8 resolution)\n    x3 = Conv2D(256, kernel_size=3, strides=1, padding='same')(p2)\n    x3 = BatchNormalization()(x3)\n    x3 = ReLU()(x3)\n    p3 = MaxPooling2D(pool_size=2)(x3)\n    \n    return x1, x2, x3, p3  # Return all skip connections\n\n# ----------------------------\n# Full Mamba TransUNet Model\n# ----------------------------\n\ndef transunet_mamba(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)):\n    inputs = Input(input_size)\n    \n    # CNN Front-End\n    x1, x2, x3, cnn_feats = cnn_feature_extractor(inputs)\n    \n    # Mamba Encoder\n    x_mamba = mamba_block(cnn_feats, d_model=256, ssm_dim=128)\n    x_mamba = mamba_block(x_mamba, d_model=256, ssm_dim=128)\n    \n    # Decoder with skip connections\n    # Up 1/8 -> 1/4\n    u3 = Conv2DTranspose(256, kernel_size=3, strides=2, padding='same')(x_mamba)\n    u3 = concatenate([u3, x3], axis=-1)\n    u3 = Conv2D(256, kernel_size=3, padding='same')(u3)\n    u3 = BatchNormalization()(u3)\n    u3 = ReLU()(u3)\n    \n    # Up 1/4 -> 1/2\n    u2 = Conv2DTranspose(128, kernel_size=3, strides=2, padding='same')(u3)\n    u2 = concatenate([u2, x2], axis=-1)\n    u2 = Conv2D(128, kernel_size=3, padding='same')(u2)\n    u2 = BatchNormalization()(u2)\n    u2 = ReLU()(u2)\n    \n    # Up 1/2 -> original\n    u1 = Conv2DTranspose(64, kernel_size=3, strides=2, padding='same')(u2)\n    u1 = concatenate([u1, x1], axis=-1)\n    u1 = Conv2D(64, kernel_size=3, padding='same')(u1)\n    u1 = BatchNormalization()(u1)\n    u1 = ReLU()(u1)\n    \n    # Final output\n    outputs = Conv2D(1, kernel_size=1, activation='sigmoid')(u1)\n    \n    return Model(inputs=inputs, outputs=outputs)\n\n# Rest of your code remains the same (KFold, main function, etc.)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T15:40:17.746821Z","iopub.execute_input":"2025-04-18T15:40:17.747726Z","iopub.status.idle":"2025-04-18T15:40:32.992186Z","shell.execute_reply.started":"2025-04-18T15:40:17.747689Z","shell.execute_reply":"2025-04-18T15:40:32.991606Z"}},"outputs":[{"name":"stderr","text":"2025-04-18 15:40:20.006870: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744990820.272115      82 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744990820.345105      82 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ----------------------------\n# K-Fold Cross Validation Setup\n# ----------------------------\n\ndef save_kfold_splits(X, y, n_splits, seed, save_path):\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    splits = [(train_idx.tolist(), test_idx.tolist()) for train_idx, test_idx in kf.split(X)]\n    with open(save_path, 'wb') as f:\n        pickle.dump(splits, f)\n\ndef load_kfold_splits(file_path):\n    with open(file_path, 'rb') as f:\n        return pickle.load(f)\n\n# ----------------------------\n# Training and Evaluation\n# ----------------------------\n\ndef train_model(X_train, y_train, X_val, y_val, fold, input_shape):\n    # Build model\n    model = transunet_mamba(input_shape)\n    \n    # Compile with mixed precision\n    optimizer = Adam(learning_rate=INIT_LR)\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=bce_dice_loss,\n        metrics=[dice_coef, iou_coef, 'accuracy']\n    )\n    \n    # Callbacks\n    callbacks = [\n        ModelCheckpoint(\n            f\"mamba_unet_fold{fold}_best.keras\",\n            monitor='val_dice_coef',\n            mode='max',\n            save_best_only=True,\n            save_weights_only=False\n        ),\n        EarlyStopping(\n            monitor='val_dice_coef',\n            patience=15,\n            mode='max',\n            restore_best_weights=True\n        ),\n        ReduceLROnPlateau(\n            monitor='val_dice_coef',\n            factor=0.5,\n            patience=5,\n            min_lr=1e-6,\n            mode='max'\n        )\n    ]\n    \n    # Train with reduced memory footprint\n    history = model.fit(\n        X_train, y_train,\n        batch_size=BATCH_SIZE,\n        epochs=EPOCHS,\n        validation_data=(X_val, y_val),\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    return model, history\n\ndef evaluate_model(model, X_test, y_test):\n    y_pred = model.predict(X_test, batch_size=BATCH_SIZE)\n    y_pred_thresh = (y_pred > 0.5).astype(np.float32)\n    \n    dice_scores = [dice_coef(y_test[i], y_pred_thresh[i]).numpy() \n                  for i in range(len(y_test))]\n    return np.mean(dice_scores)\n\n\ndef preprocess_patient_with_rotation(patient_folder, rotation_angles=[0, 90, 180, 270]):\n    images = []\n    masks_endo = []\n    masks_epi = []\n    masks_la = []\n    \n    views = ['2CH', '4CH']\n    time_points = ['ED', 'ES']\n    \n    for view in views:\n        for tp in time_points:\n            base_name = os.path.basename(patient_folder)\n            img_path = f\"{patient_folder}/{base_name}_{view}_{tp}.nii\"\n            gt_path = f\"{patient_folder}/{base_name}_{view}_{tp}_gt.nii\"\n            \n            if not os.path.exists(img_path) or not os.path.exists(gt_path):\n                continue\n            \n            try:\n                img = load_nifti_image(img_path)\n                gt = load_nifti_image(gt_path)\n                \n                img_resized = resize(img, (IMG_HEIGHT, IMG_WIDTH), \n                                  preserve_range=True, anti_aliasing=True)\n                gt_resized = resize(gt, (IMG_HEIGHT, IMG_WIDTH), \n                                  preserve_range=True, anti_aliasing=False)\n                \n                img_resized = (img_resized - img_resized.min()) / (img_resized.max() - img_resized.min())\n                \n                mask_endo = (gt_resized == 1).astype(np.float32)\n                mask_epi = (gt_resized == 2).astype(np.float32)\n                mask_la = (gt_resized == 3).astype(np.float32)\n                \n                for angle in rotation_angles:\n                    rotated_img = rotate(img_resized, angle, reshape=False, mode='reflect')\n                    rotated_mask_endo = rotate(mask_endo, angle, reshape=False, mode='reflect')\n                    rotated_mask_epi = rotate(mask_epi, angle, reshape=False, mode='reflect')\n                    rotated_mask_la = rotate(mask_la, angle, reshape=False, mode='reflect')\n                    \n                    images.append(rotated_img[..., np.newaxis])\n                    masks_endo.append(rotated_mask_endo[..., np.newaxis])\n                    masks_epi.append(rotated_mask_epi[..., np.newaxis])\n                    masks_la.append(rotated_mask_la[..., np.newaxis])\n            \n            except Exception as e:\n                print(f\"Error processing {base_name}: {e}\")\n                continue\n    \n    return np.array(images), np.array(masks_endo), np.array(masks_epi), np.array(masks_la)\n\ndef load_dataset(base_path):\n    patient_folders = sorted([\n        os.path.join(base_path, f) \n        for f in os.listdir(base_path) \n        if f.startswith('patient') and os.path.isdir(os.path.join(base_path, f))\n    ])\n    \n    all_images = []\n    all_masks_endo = []\n    all_masks_epi = []\n    all_masks_la = []\n    \n    for patient_folder in tqdm(patient_folders, desc=\"Loading patients\"):\n        images, masks_endo, masks_epi, masks_la = preprocess_patient_with_rotation(patient_folder)\n        \n        if images.size > 0:\n            all_images.append(images)\n            all_masks_endo.append(masks_endo)\n            all_masks_epi.append(masks_epi)\n            all_masks_la.append(masks_la)\n    \n    if not all_images:\n        raise ValueError(\"No valid image data found in any patient folder!\")\n    \n    # Stack all patient data\n    X = np.concatenate(all_images, axis=0)\n    y_endo = np.concatenate(all_masks_endo, axis=0)\n    y_epi = np.concatenate(all_masks_epi, axis=0)\n    y_la = np.concatenate(all_masks_la, axis=0)\n    \n    return X, y_endo, y_epi, y_la\n\n\n\n\n\n# ----------------------------\n# Main Execution\n# ----------------------------\n\ndef main():\n    # Load and preprocess dataset\n    print(\"Loading and preprocessing dataset...\")\n    X, y_endo, _, _ = load_dataset(DATA_PATH)\n    \n    # Save k-fold splits\n    save_kfold_splits(X, y_endo, N_FOLDS, SEED, \"kfold_splits.pkl\")\n    splits = load_kfold_splits(\"kfold_splits.pkl\")\n    \n    fold_results = []\n    \n    for fold, (train_idx, test_idx) in enumerate(splits):\n        print(f\"\\n=== Fold {fold + 1}/{N_FOLDS} ===\")\n        \n        # Split data\n        X_train, X_test = X[train_idx], X[test_idx]\n        y_train, y_test = y_endo[train_idx], y_endo[test_idx]\n        \n        # Further split into train/val\n        X_train, X_val, y_train, y_val = train_test_split(\n            X_train, y_train, \n            test_size=0.1, \n            random_state=SEED\n        )\n        \n        # Train model\n        print(f\"Training on {len(X_train)} samples, validating on {len(X_val)} samples\")\n        model, history = train_model(\n            X_train, y_train,\n            X_val, y_val,\n            fold,\n            input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n        )\n        \n        # Evaluate on test set\n        print(f\"Evaluating on {len(X_test)} test samples\")\n        model.load_weights(f\"mamba_unet_fold{fold}_best.keras\")  # Load best weights\n        fold_dice = evaluate_model(model, X_test, y_test)\n        fold_results.append(fold_dice)\n        print(f\"Fold {fold + 1} Dice: {fold_dice:.4f}\")\n        \n        # Clean up to save memory\n        del model\n        tf.keras.backend.clear_session()\n    \n    # Final results\n    print(\"\\n=== Cross-Validation Results ===\")\n    print(f\"Average Dice: {np.mean(fold_results):.4f} ± {np.std(fold_results):.4f}\")\n    print(\"Per-fold results:\", [f\"{x:.4f}\" for x in fold_results])\n\nif __name__ == '__main__':\n    # Configure TensorFlow for better memory management\n    physical_devices = tf.config.list_physical_devices('GPU')\n    if physical_devices:\n        try:\n            tf.config.experimental.set_memory_growth(physical_devices[0], True)\n        except:\n            pass\n    \n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:06:33.862069Z","iopub.execute_input":"2025-04-18T16:06:33.862808Z","execution_failed":"2025-04-18T16:12:18.579Z"}},"outputs":[{"name":"stdout","text":"Loading and preprocessing dataset...\n","output_type":"stream"},{"name":"stderr","text":"Loading patients: 100%|██████████| 500/500 [05:20<00:00,  1.56it/s]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Optimized version of the mamba implementation","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport nibabel as nib\nfrom skimage.transform import resize\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, BatchNormalization, ReLU, MaxPooling2D,\n    Conv2DTranspose, concatenate, LayerNormalization, \n    Reshape, Dense, Dropout, Lambda\n)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\n# Reduced configuration for memory\nIMG_HEIGHT, IMG_WIDTH = 224, 224  # Reduced from 256\nIMG_CHANNELS = 1\nBATCH_SIZE = 2  # Reduced from 8\nEPOCHS = 50\nINIT_LR = 1e-4\nDATA_PATH = \"/kaggle/input/camus-dataset/database_nifti\"\n\n# Simplified metrics\ndef dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = tf.reshape(y_true, [-1])\n    y_pred_f = tf.reshape(y_pred, [-1])\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n\ndef dice_loss(y_true, y_pred):\n    return 1 - dice_coef(y_true, y_pred)\n\n# Memory-efficient data loading\ndef load_and_preprocess_single(path, target_size):\n    img = nib.load(path).get_fdata()\n    img = resize(img, target_size, preserve_range=True, anti_aliasing=True)\n    return (img - img.min()) / (img.max() - img.min() + 1e-7)\n\ndef load_patient(patient_folder):\n    images, masks = [], []\n    views = ['2CH', '4CH']\n    time_points = ['ED', 'ES']\n    \n    for view in views:\n        for tp in time_points:\n            base = os.path.basename(patient_folder)\n            img_path = f\"{patient_folder}/{base}_{view}_{tp}.nii\"\n            mask_path = f\"{patient_folder}/{base}_{view}_{tp}_gt.nii\"\n            \n            if os.path.exists(img_path) and os.path.exists(mask_path):\n                try:\n                    img = load_and_preprocess_single(img_path, (IMG_HEIGHT, IMG_WIDTH))\n                    mask = load_and_preprocess_single(mask_path, (IMG_HEIGHT, IMG_WIDTH))\n                    \n                    images.append(img[..., np.newaxis])\n                    masks.append(mask[..., np.newaxis])\n                except Exception as e:\n                    print(f\"Error loading {base}: {str(e)}\")\n    \n    return np.array(images), np.array(masks)\n\nclass SelectiveSSM(tf.keras.layers.Layer):\n    def __init__(self, d_model, ssm_dim):\n        super().__init__()\n        self.d_model = d_model\n        self.ssm_dim = ssm_dim\n        \n    def build(self, input_shape):\n        # Input shape: (batch, seq_len, d_model)\n        self.dense1 = Dense(self.d_model * 2)\n        self.dense2 = Dense(self.d_model)\n        self.built = True\n        \n    def call(self, x):\n        x_proj = self.dense1(x)\n        delta, B = tf.split(x_proj, 2, axis=-1)\n        A_bar = tf.exp(delta)\n        B_bar = tf.einsum('bnd,bnd->bn', delta, B)\n        \n        seq_len = tf.shape(x)[1]  # Dynamic sequence length\n\n        # Initialize the hidden state with the same shape as A_bar[:, 0, :]\n        h_init = tf.zeros((tf.shape(x)[0], self.d_model))  # Match d_model dimension\n        \n        # Define the recurrent step for tf.scan\n        def step(h, t):\n            h = A_bar[:, t, :] * h + B_bar[:, t, None]\n            return h\n\n        # Use tf.scan to replace the for loop\n        h_states = tf.scan(step, tf.range(seq_len), initializer=h_init)\n        \n        # Transpose to match the required shape: (batch, seq_len, d_model)\n        h_states = tf.transpose(h_states, perm=[1, 0, 2])\n        \n        return self.dense2(h_states)\n    \n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[1], self.d_model)\n\ndef mamba_block(x, d_model, ssm_dim):\n    x_norm = LayerNormalization()(x)\n    x_conv = Conv2D(d_model, 3, padding='same')(x_norm)\n    \n    # Wrap dynamic spatial dimension computation in a Lambda layer\n    def reshape_fn(x_conv):\n        batch, h, w, c = tf.shape(x_conv)[0], tf.shape(x_conv)[1], tf.shape(x_conv)[2], tf.shape(x_conv)[3]\n        return tf.reshape(x_conv, [batch, h * w, d_model])\n    \n    # Compute the output shape for reshape_fn\n    def reshape_output_shape(input_shape):\n        batch_size, height, width, _ = input_shape\n        return (batch_size, height * width, d_model)\n    \n    x_flat = Lambda(reshape_fn, output_shape=reshape_output_shape)(x_conv)\n    x_ssm = SelectiveSSM(d_model, ssm_dim)(x_flat)\n    \n    # Wrap output reshaping in a Lambda layer\n    def reshape_back_fn(x_ssm):\n        # Dynamically calculate dimensions using the shape of x_ssm\n        batch = tf.shape(x_ssm)[0]\n        seq_len = tf.shape(x_ssm)[1]\n        height = width = tf.cast(tf.sqrt(tf.cast(seq_len, tf.float32)), tf.int32)  # Fix: Cast seq_len to float32 before applying sqrt\n        return tf.reshape(x_ssm, [batch, height, width, d_model])\n    \n    # Compute the output shape for reshape_back_fn\n    def reshape_back_output_shape(input_shape):\n        batch_size, seq_len, _ = input_shape\n        height = width = int(tf.sqrt(seq_len))  # This is static and will not be used at runtime\n        return (batch_size, height, width, d_model)\n    \n    x_out = Lambda(reshape_back_fn, output_shape=reshape_back_output_shape)(x_ssm)\n    return x + x_out\n#Simplified model\ndef build_model(input_shape):\n    inputs = Input(input_shape)\n    \n    # Encoder\n    x1 = Conv2D(32, 3, strides=2, padding='same')(inputs)\n    x1 = mamba_block(x1, 32, 16)\n    \n    x2 = Conv2D(64, 3, strides=2, padding='same')(x1)\n    x2 = mamba_block(x2, 64, 32)\n    \n    # Decoder\n    u1 = Conv2DTranspose(32, 3, strides=2, padding='same')(x2)\n    u1 = concatenate([u1, x1])\n    u1 = Conv2D(32, 3, padding='same')(u1)\n    \n    outputs = Conv2DTranspose(1, 3, strides=2, padding='same', activation='sigmoid')(u1)\n    \n    return Model(inputs, outputs)\n\n# Training with memory monitoring\ndef train():\n    # Load data incrementally\n    patient_folders = [f for f in os.listdir(DATA_PATH) if f.startswith('patient')]\n    X, y = [], []\n    \n    for folder in tqdm(patient_folders[:20]):  # Limit to 20 patients for memory\n        imgs, masks = load_patient(os.path.join(DATA_PATH, folder))\n        X.extend(imgs)\n        y.extend(masks)\n    \n    X, y = np.array(X), np.array(y)\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Build and train\n    model = build_model((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n    model.compile(optimizer=Adam(INIT_LR), loss=dice_loss, metrics=[dice_coef])\n    \n    model.fit(X_train, y_train,\n             batch_size=BATCH_SIZE,\n             epochs=EPOCHS,\n             validation_data=(X_val, y_val),\n             callbacks=[\n                 ModelCheckpoint(\"best_model.keras\", save_best_only=True),\n                 EarlyStopping(patience=5)\n             ])\n\nif __name__ == '__main__':\n    # Configure GPU memory growth\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n        except RuntimeError as e:\n            print(e)\n    \n    train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:37:21.900582Z","iopub.execute_input":"2025-04-18T16:37:21.900924Z","iopub.status.idle":"2025-04-18T16:37:23.336433Z","shell.execute_reply.started":"2025-04-18T16:37:21.900901Z","shell.execute_reply":"2025-04-18T16:37:23.335343Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 20/20 [00:01<00:00, 15.13it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_133/682727213.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_133/682727213.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;31m# Build and train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_HEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_CHANNELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINIT_LR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdice_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdice_coef\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_133/682727213.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmamba_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_133/682727213.py\u001b[0m in \u001b[0;36mmamba_block\u001b[0;34m(x, d_model, ssm_dim)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mx_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshape_back_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreshape_back_output_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ssm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;31m#Simplified model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_133/682727213.py\u001b[0m in \u001b[0;36mreshape_back_output_shape\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreshape_back_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# This is static and will not be used at runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling Lambda.call().\n\n\u001b[1mValue for attr 'T' of int32 is not in the list of allowed values: bfloat16, half, float, double, complex64, complex128\n\t; NodeDef: {{node Sqrt}}; Op<name=Sqrt; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]> [Op:Sqrt] name: \u001b[0m\n\nArguments received by Lambda.call():\n  • args=('<KerasTensor shape=(None, 12544, 32), dtype=float32, sparse=False, name=keras_tensor_111>',)\n  • kwargs={'mask': 'None'}"],"ename":"InvalidArgumentError","evalue":"Exception encountered when calling Lambda.call().\n\n\u001b[1mValue for attr 'T' of int32 is not in the list of allowed values: bfloat16, half, float, double, complex64, complex128\n\t; NodeDef: {{node Sqrt}}; Op<name=Sqrt; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]> [Op:Sqrt] name: \u001b[0m\n\nArguments received by Lambda.call():\n  • args=('<KerasTensor shape=(None, 12544, 32), dtype=float32, sparse=False, name=keras_tensor_111>',)\n  • kwargs={'mask': 'None'}","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}