{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10994898,"sourceType":"datasetVersion","datasetId":6843932},{"sourceId":288332,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":247033,"modelId":268575}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport nibabel as nib\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\nimport albumentations as A\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-18T15:14:23.586642Z","iopub.execute_input":"2025-03-18T15:14:23.586847Z","iopub.status.idle":"2025-03-18T15:14:27.279546Z","shell.execute_reply.started":"2025-03-18T15:14:23.586826Z","shell.execute_reply":"2025-03-18T15:14:27.278653Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\ndataset_path = \"/kaggle/input/camus-datasert\"\nfiles = os.listdir(dataset_path)\nprint(files)  # To verify the dataset is available\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T15:14:35.035147Z","iopub.execute_input":"2025-03-18T15:14:35.035472Z","iopub.status.idle":"2025-03-18T15:14:35.043385Z","shell.execute_reply.started":"2025-03-18T15:14:35.035445Z","shell.execute_reply":"2025-03-18T15:14:35.042703Z"}},"outputs":[{"name":"stdout","text":"['database_nifti']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Set fixed image size\nIMG_SIZE = 256\n\n# Define dataset root path\nDATASET_ROOT = \"/kaggle/input/camus-datasert/database_nifti\"  # Change this to your actual dataset folder\n\n# Define the number of augmented samples per image\nAUGMENTATIONS_PER_IMAGE = 2  # Adjust this to reach your desired dataset size\n\n# Define a more aggressive augmentation pipeline\naugmentation = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.ElasticTransform(alpha=1, sigma=50, p=0.5),\n    A.RandomBrightnessContrast(contrast_limit=0.2, brightness_limit=0.2, p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=45, p=0.5),\n    A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n    A.OpticalDistortion(distort_limit=0.3, shift_limit=0.1, p=0.5),\n    A.RandomGamma(gamma_limit=(80, 120), p=0.5),\n    A.GaussianBlur(blur_limit=(3, 7), p=0.5),\n])\n\ndef load_nii(filepath):\n    \"\"\"Load a .nii.gz file and return the image as a NumPy array.\"\"\"\n    img = nib.load(filepath).get_fdata()\n    img = np.squeeze(img)  # Remove single dimensions\n    return img\n\ndef preprocess_image(image):\n    \"\"\"Resize and normalize image.\"\"\"\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))  # Resize\n    image = (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-7)  # Normalize\n    return image\n\n# Lists to store images & masks\nimages, masks = [], []\n\n# Check if augmented dataset already exists\nif os.path.exists(\"augmented_images.npy\") and os.path.exists(\"augmented_masks.npy\"):\n    print(\"Loading augmented dataset from disk...\")\n    images = np.load(\"augmented_images.npy\")\n    masks = np.load(\"augmented_masks.npy\")\nelse:\n    print(\"Augmenting dataset...\")\n    # Loop through patient folders\n    for patient_folder in os.listdir(DATASET_ROOT):\n        patient_path = os.path.join(DATASET_ROOT, patient_folder)\n\n        # Ensure it's a folder\n        if os.path.isdir(patient_path):\n            # Get all .nii.gz files in the patient folder\n            nii_files = sorted([f for f in os.listdir(patient_path) if f.endswith(\".nii\")])\n\n            for nii_file in nii_files:\n                if \"_gt.nii\" in nii_file:  # This is a mask file, skip it in this loop\n                    continue\n                \n                # Define corresponding mask filename\n                mask_file = nii_file.replace(\".nii\", \"_gt.nii\")\n\n                img_path = os.path.join(patient_path, nii_file)\n                mask_path = os.path.join(patient_path, mask_file)\n\n                # Check if both image & mask exist\n                if os.path.exists(mask_path):\n                    try:\n                        # Load image and mask\n                        image = load_nii(img_path)\n                        mask = load_nii(mask_path)\n\n                        # Ensure image and mask are 2D\n                        if image.ndim > 2:\n                            image = image[..., 0]  # Take the first slice if 3D\n                        if mask.ndim > 2:\n                            mask = mask[..., 0]  # Take the first slice if 3D\n\n                        # Resize and normalize\n                        image = preprocess_image(image)\n                        mask = preprocess_image(mask)\n\n                        # Append original image and mask\n                        images.append(image)\n                        masks.append(mask)\n\n                        # Generate augmented samples\n                        for _ in range(AUGMENTATIONS_PER_IMAGE):\n                            augmented = augmentation(image=image, mask=mask)\n                            image_aug, mask_aug = augmented[\"image\"], augmented[\"mask\"]\n                            images.append(image_aug)\n                            masks.append(mask_aug)\n\n                    except Exception as e:\n                        print(f\"❌ Error loading {nii_file}: {e}\")\n\n    # Convert to NumPy arrays\n    images = np.stack(images).astype(np.float32)  # Ensure all are the same shape\n    masks = np.stack(masks).astype(np.float32)    # Ensure all are the same shape\n\n    # Add channel dimension (for compatibility with U-Net)\n    images = np.expand_dims(images, axis=-1)  # Shape: (N, IMG_SIZE, IMG_SIZE, 1)\n    masks = np.expand_dims(masks, axis=-1)    # Shape: (N, IMG_SIZE, IMG_SIZE, 1)\n\n    # Save the augmented dataset\n    np.save(\"augmented_images.npy\", images)\n    np.save(\"augmented_masks.npy\", masks)\n\n# Split dataset\nX_train, X_temp, Y_train, Y_temp = train_test_split(images, masks, test_size=0.2, random_state=42)\nX_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n\n# Print dataset sizes\nprint(f\"✅ Training: {X_train.shape[0]} images\")\nprint(f\"✅ Validation: {X_val.shape[0]} images\")\nprint(f\"✅ Testing: {X_test.shape[0]} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:42:48.064253Z","iopub.execute_input":"2025-03-16T17:42:48.064559Z","iopub.status.idle":"2025-03-16T17:51:28.656420Z","shell.execute_reply.started":"2025-03-16T17:42:48.064536Z","shell.execute_reply":"2025-03-16T17:51:28.655286Z"}},"outputs":[{"name":"stdout","text":"Augmenting dataset...\n✅ Training: 7200 images\n✅ Validation: 900 images\n✅ Testing: 900 images\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\n\n# Load the augmented dataset\nimages = np.load(\"augmented_images.npy\")\nmasks = np.load(\"augmented_masks.npy\")\n\n# Print dataset shape\nprint(f\"✅ Loaded augmented dataset:\")\nprint(f\"Images shape: {images.shape}\")\nprint(f\"Masks shape: {masks.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:51:43.763911Z","iopub.execute_input":"2025-03-16T17:51:43.764239Z","iopub.status.idle":"2025-03-16T17:52:03.571629Z","shell.execute_reply.started":"2025-03-16T17:51:43.764217Z","shell.execute_reply":"2025-03-16T17:52:03.570878Z"}},"outputs":[{"name":"stdout","text":"✅ Loaded augmented dataset:\nImages shape: (9000, 256, 256, 1)\nMasks shape: (9000, 256, 256, 1)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Split dataset\nX_train, X_temp, Y_train, Y_temp = train_test_split(images, masks, test_size=0.2, random_state=42)\nX_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:52:10.599397Z","iopub.execute_input":"2025-03-16T17:52:10.599721Z","iopub.status.idle":"2025-03-16T17:52:15.238939Z","shell.execute_reply.started":"2025-03-16T17:52:10.599692Z","shell.execute_reply":"2025-03-16T17:52:15.237899Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, concatenate, UpSampling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T15:15:57.334696Z","iopub.execute_input":"2025-03-18T15:15:57.335015Z","iopub.status.idle":"2025-03-18T15:16:09.422837Z","shell.execute_reply.started":"2025-03-18T15:15:57.334993Z","shell.execute_reply":"2025-03-18T15:16:09.422125Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Define Dice Loss\ndef dice_loss(y_true, y_pred, smooth=1e-7):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    intersection = tf.reduce_sum(y_true * y_pred)\n    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n    dice = (2.0 * intersection + smooth) / (union + smooth)\n    return 1.0 - dice\n\n# Define Combined Loss (Dice + Binary Cross-Entropy)\ndef combined_loss(y_true, y_pred):\n    bce = tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)\n    dice = dice_loss(y_true, y_pred)\n    return bce + dice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T15:16:13.164797Z","iopub.execute_input":"2025-03-18T15:16:13.165389Z","iopub.status.idle":"2025-03-18T15:16:13.170912Z","shell.execute_reply.started":"2025-03-18T15:16:13.165362Z","shell.execute_reply":"2025-03-18T15:16:13.169821Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\n# Define Dice Coefficient Metric\ndef dice_coefficient(y_true, y_pred, smooth=1e-7):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    intersection = tf.reduce_sum(y_true * y_pred)\n    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n    dice = (2.0 * intersection + smooth) / (union + smooth)\n    return dice\n\n# Define IoU Metric\ndef iou(y_true, y_pred, smooth=1e-7):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    intersection = tf.reduce_sum(y_true * y_pred)\n    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n    return (intersection + smooth) / (union + smooth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T15:16:22.929988Z","iopub.execute_input":"2025-03-18T15:16:22.930306Z","iopub.status.idle":"2025-03-18T15:16:22.935609Z","shell.execute_reply.started":"2025-03-18T15:16:22.930277Z","shell.execute_reply":"2025-03-18T15:16:22.934614Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Define Classic U-Net\ndef classic_unet(input_shape=(256, 256, 1)):\n    inputs = Input(input_shape)\n\n    # Encoder\n    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n    drop4 = Dropout(0.5)(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n\n    # Bottleneck\n    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n    drop5 = Dropout(0.5)(conv5)\n\n    # Decoder\n    up6 = Conv2D(512, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(drop5))\n    merge6 = concatenate([drop4, up6], axis=-1)\n    conv6 = Conv2D(512, 3, activation='relu', padding='same')(merge6)\n    conv6 = Conv2D(512, 3, activation='relu', padding='same')(conv6)\n\n    up7 = Conv2D(256, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv6))\n    merge7 = concatenate([conv3, up7], axis=-1)\n    conv7 = Conv2D(256, 3, activation='relu', padding='same')(merge7)\n    conv7 = Conv2D(256, 3, activation='relu', padding='same')(conv7)\n\n    up8 = Conv2D(128, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv7))\n    merge8 = concatenate([conv2, up8], axis=-1)\n    conv8 = Conv2D(128, 3, activation='relu', padding='same')(merge8)\n    conv8 = Conv2D(128, 3, activation='relu', padding='same')(conv8)\n\n    up9 = Conv2D(64, 2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv8))\n    merge9 = concatenate([conv1, up9], axis=-1)\n    conv9 = Conv2D(64, 3, activation='relu', padding='same')(merge9)\n    conv9 = Conv2D(64, 3, activation='relu', padding='same')(conv9)\n\n    # Output\n    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n\n    model = Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T15:16:26.887870Z","iopub.execute_input":"2025-03-18T15:16:26.888180Z","iopub.status.idle":"2025-03-18T15:16:26.899419Z","shell.execute_reply.started":"2025-03-18T15:16:26.888154Z","shell.execute_reply":"2025-03-18T15:16:26.898590Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Load the best model\nmodel = load_model(\"/kaggle/input/model_v1/keras/default/1/best_model.keras\", custom_objects={\n    \"combined_loss\": combined_loss,\n    \"dice_coefficient\": dice_coefficient,\n    \"iou\": iou\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T15:16:31.052012Z","iopub.execute_input":"2025-03-18T15:16:31.052334Z","iopub.status.idle":"2025-03-18T15:16:38.231648Z","shell.execute_reply.started":"2025-03-18T15:16:31.052306Z","shell.execute_reply":"2025-03-18T15:16:38.230939Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Compile the Model\nmodel = classic_unet(input_shape=(256, 256, 1))\nmodel.compile(optimizer=Adam(learning_rate=1e-4),\n              loss=combined_loss,\n              metrics=[dice_coefficient, iou])\n\n# Learning Rate Scheduler\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7,verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T23:54:46.683065Z","iopub.execute_input":"2025-03-15T23:54:46.683355Z","iopub.status.idle":"2025-03-15T23:54:49.147878Z","shell.execute_reply.started":"2025-03-15T23:54:46.683331Z","shell.execute_reply":"2025-03-15T23:54:49.146980Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\n\n# Define the checkpoint callback\ncheckpoint_callback = ModelCheckpoint(\n    filepath=\"/kaggle/working//best_modelV2.keras\",  # Path to save the best model\n    monitor=\"val_dice_coefficient\",  # Metric to monitor\n    mode=\"max\",  # Save the model with the maximum validation Dice Coefficient\n    save_best_only=True,  # Only save the best model\n    verbose=1  # Print a message when the model is saved\n)\n\n# Learning Rate Scheduler\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7,verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:52:55.407450Z","iopub.execute_input":"2025-03-16T17:52:55.407753Z","iopub.status.idle":"2025-03-16T17:52:55.412601Z","shell.execute_reply.started":"2025-03-16T17:52:55.407731Z","shell.execute_reply":"2025-03-16T17:52:55.411617Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Train the Model\nhistory = model.fit(X_train, Y_train,\n                    validation_data=(X_val, Y_val),\n                    batch_size=16,\n                    epochs=10,\n                    callbacks=[reduce_lr,checkpoint_callback])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:53:07.914309Z","iopub.execute_input":"2025-03-16T17:53:07.914651Z","iopub.status.idle":"2025-03-16T19:11:55.698508Z","shell.execute_reply.started":"2025-03-16T17:53:07.914628Z","shell.execute_reply":"2025-03-16T19:11:55.697495Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 984ms/step - dice_coefficient: 0.6525 - iou: 0.4845 - loss: 0.5506\nEpoch 1: val_dice_coefficient improved from -inf to 0.64467, saving model to /kaggle/working//best_modelV2.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m580s\u001b[0m 1s/step - dice_coefficient: 0.6525 - iou: 0.4845 - loss: 0.5506 - val_dice_coefficient: 0.6447 - val_iou: 0.4758 - val_loss: 0.5514 - learning_rate: 1.0000e-04\nEpoch 2/10\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987ms/step - dice_coefficient: 0.6644 - iou: 0.4976 - loss: 0.5300\nEpoch 2: val_dice_coefficient improved from 0.64467 to 0.65651, saving model to /kaggle/working//best_modelV2.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 1s/step - dice_coefficient: 0.6644 - iou: 0.4976 - loss: 0.5300 - val_dice_coefficient: 0.6565 - val_iou: 0.4889 - val_loss: 0.5464 - learning_rate: 1.0000e-04\nEpoch 3/10\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988ms/step - dice_coefficient: 0.6710 - iou: 0.5051 - loss: 0.5190\nEpoch 3: val_dice_coefficient improved from 0.65651 to 0.65917, saving model to /kaggle/working//best_modelV2.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 1s/step - dice_coefficient: 0.6710 - iou: 0.5051 - loss: 0.5190 - val_dice_coefficient: 0.6592 - val_iou: 0.4918 - val_loss: 0.5388 - learning_rate: 1.0000e-04\nEpoch 4/10\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989ms/step - dice_coefficient: 0.6751 - iou: 0.5097 - loss: 0.5121\nEpoch 4: val_dice_coefficient did not improve from 0.65917\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 1s/step - dice_coefficient: 0.6751 - iou: 0.5097 - loss: 0.5121 - val_dice_coefficient: 0.6574 - val_iou: 0.4898 - val_loss: 0.5392 - learning_rate: 1.0000e-04\nEpoch 5/10\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988ms/step - dice_coefficient: 0.6802 - iou: 0.5156 - loss: 0.5034\nEpoch 5: val_dice_coefficient did not improve from 0.65917\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 1s/step - dice_coefficient: 0.6802 - iou: 0.5156 - loss: 0.5034 - val_dice_coefficient: 0.6581 - val_iou: 0.4906 - val_loss: 0.5383 - learning_rate: 1.0000e-04\nEpoch 6/10\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988ms/step - dice_coefficient: 0.6827 - iou: 0.5185 - loss: 0.4986\nEpoch 6: val_dice_coefficient improved from 0.65917 to 0.66578, saving model to /kaggle/working//best_modelV2.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 1s/step - dice_coefficient: 0.6827 - iou: 0.5185 - loss: 0.4986 - val_dice_coefficient: 0.6658 - val_iou: 0.4992 - val_loss: 0.5406 - learning_rate: 1.0000e-04\nEpoch 7/10\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989ms/step - dice_coefficient: 0.6876 - iou: 0.5241 - loss: 0.4900\nEpoch 7: val_dice_coefficient did not improve from 0.66578\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 1s/step - dice_coefficient: 0.6876 - iou: 0.5241 - loss: 0.4900 - val_dice_coefficient: 0.6649 - val_iou: 0.4982 - val_loss: 0.5326 - learning_rate: 1.0000e-04\nEpoch 8/10\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987ms/step - dice_coefficient: 0.6882 - iou: 0.5247 - loss: 0.4887\nEpoch 8: val_dice_coefficient improved from 0.66578 to 0.66995, saving model to /kaggle/working//best_modelV2.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 1s/step - dice_coefficient: 0.6882 - iou: 0.5247 - loss: 0.4887 - val_dice_coefficient: 0.6699 - val_iou: 0.5038 - val_loss: 0.5359 - learning_rate: 1.0000e-04\nEpoch 9/10\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988ms/step - dice_coefficient: 0.6902 - iou: 0.5271 - loss: 0.4864\nEpoch 9: val_dice_coefficient did not improve from 0.66995\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 1s/step - dice_coefficient: 0.6902 - iou: 0.5271 - loss: 0.4864 - val_dice_coefficient: 0.6686 - val_iou: 0.5024 - val_loss: 0.5360 - learning_rate: 1.0000e-04\nEpoch 10/10\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988ms/step - dice_coefficient: 0.6918 - iou: 0.5290 - loss: 0.4833\nEpoch 10: val_dice_coefficient did not improve from 0.66995\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 1s/step - dice_coefficient: 0.6918 - iou: 0.5290 - loss: 0.4833 - val_dice_coefficient: 0.6692 - val_iou: 0.5030 - val_loss: 0.5325 - learning_rate: 1.0000e-04\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Train the Model\nhistory = model.fit(X_train, Y_train,\n                    validation_data=(X_val, Y_val),\n                    batch_size=16,\n                    epochs=15,\n                    callbacks=[reduce_lr,checkpoint_callback])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T23:55:18.354617Z","iopub.execute_input":"2025-03-15T23:55:18.354959Z","iopub.status.idle":"2025-03-16T01:51:58.609218Z","shell.execute_reply.started":"2025-03-15T23:55:18.354936Z","shell.execute_reply":"2025-03-16T01:51:58.608445Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/15\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 986ms/step - dice_coefficient: 0.3684 - iou: 0.2302 - loss: 1.0195\nEpoch 1: val_dice_coefficient improved from -inf to 0.51086, saving model to /kaggle/working/best_model.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m583s\u001b[0m 1s/step - dice_coefficient: 0.3685 - iou: 0.2303 - loss: 1.0192 - val_dice_coefficient: 0.5109 - val_iou: 0.3433 - val_loss: 0.7801 - learning_rate: 1.0000e-04\nEpoch 2/15\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985ms/step - dice_coefficient: 0.5403 - iou: 0.3707 - loss: 0.7383\nEpoch 2: val_dice_coefficient improved from 0.51086 to 0.55061, saving model to /kaggle/working/best_model.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 1s/step - dice_coefficient: 0.5403 - iou: 0.3707 - loss: 0.7383 - val_dice_coefficient: 0.5506 - val_iou: 0.3801 - val_loss: 0.7073 - learning_rate: 1.0000e-04\nEpoch 3/15\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 984ms/step - dice_coefficient: 0.5799 - iou: 0.4088 - loss: 0.6720\nEpoch 3: val_dice_coefficient improved from 0.55061 to 0.59545, saving model to /kaggle/working/best_model.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 1s/step - dice_coefficient: 0.5799 - iou: 0.4088 - loss: 0.6720 - val_dice_coefficient: 0.5954 - val_iou: 0.4242 - val_loss: 0.6562 - learning_rate: 1.0000e-04\nEpoch 4/15\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987ms/step - dice_coefficient: 0.6061 - iou: 0.4351 - loss: 0.6289\nEpoch 4: val_dice_coefficient improved from 0.59545 to 0.60371, saving model to /kaggle/working/best_model.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 1s/step - dice_coefficient: 0.6061 - iou: 0.4352 - loss: 0.6289 - val_dice_coefficient: 0.6037 - val_iou: 0.4326 - val_loss: 0.6292 - learning_rate: 1.0000e-04\nEpoch 5/15\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981ms/step - dice_coefficient: 0.6228 - iou: 0.4525 - loss: 0.6002\nEpoch 5: val_dice_coefficient improved from 0.60371 to 0.60540, saving model to /kaggle/working/best_model.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 1s/step - dice_coefficient: 0.6228 - iou: 0.4525 - loss: 0.6002 - val_dice_coefficient: 0.6054 - val_iou: 0.4343 - val_loss: 0.6119 - learning_rate: 1.0000e-04\nEpoch 6/15\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983ms/step - dice_coefficient: 0.6336 - iou: 0.4639 - loss: 0.5807\nEpoch 6: val_dice_coefficient improved from 0.60540 to 0.61630, saving model to /kaggle/working/best_model.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 1s/step - dice_coefficient: 0.6336 - iou: 0.4639 - loss: 0.5807 - val_dice_coefficient: 0.6163 - val_iou: 0.4456 - val_loss: 0.6006 - learning_rate: 1.0000e-04\nEpoch 7/15\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 982ms/step - dice_coefficient: 0.6443 - iou: 0.4754 - loss: 0.5633\nEpoch 7: val_dice_coefficient improved from 0.61630 to 0.62690, saving model to /kaggle/working/best_model.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 1s/step - dice_coefficient: 0.6443 - iou: 0.4754 - loss: 0.5633 - val_dice_coefficient: 0.6269 - val_iou: 0.4568 - val_loss: 0.5909 - learning_rate: 1.0000e-04\nEpoch 8/15\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979ms/step - dice_coefficient: 0.6513 - iou: 0.4832 - loss: 0.5520\nEpoch 8: val_dice_coefficient improved from 0.62690 to 0.63326, saving model to /kaggle/working/best_model.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 1s/step - dice_coefficient: 0.6513 - iou: 0.4832 - loss: 0.5520 - val_dice_coefficient: 0.6333 - val_iou: 0.4636 - val_loss: 0.5820 - learning_rate: 1.0000e-04\nEpoch 9/15\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 982ms/step - dice_coefficient: 0.6564 - iou: 0.4887 - loss: 0.5428\nEpoch 9: val_dice_coefficient improved from 0.63326 to 0.63690, saving model to /kaggle/working/best_model.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 1s/step - dice_coefficient: 0.6564 - iou: 0.4887 - loss: 0.5428 - val_dice_coefficient: 0.6369 - val_iou: 0.4674 - val_loss: 0.5769 - learning_rate: 1.0000e-04\nEpoch 10/15\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981ms/step - dice_coefficient: 0.6587 - iou: 0.4912 - loss: 0.5390\nEpoch 10: val_dice_coefficient improved from 0.63690 to 0.64719, saving model to /kaggle/working/best_model.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 1s/step - dice_coefficient: 0.6587 - iou: 0.4912 - loss: 0.5390 - val_dice_coefficient: 0.6472 - val_iou: 0.4786 - val_loss: 0.5701 - learning_rate: 1.0000e-04\nEpoch 11/15\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 982ms/step - dice_coefficient: 0.6667 - iou: 0.5003 - loss: 0.5248\nEpoch 11: val_dice_coefficient improved from 0.64719 to 0.64998, saving model to /kaggle/working/best_model.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 1s/step - dice_coefficient: 0.6667 - iou: 0.5003 - loss: 0.5248 - val_dice_coefficient: 0.6500 - val_iou: 0.4816 - val_loss: 0.5697 - learning_rate: 1.0000e-04\nEpoch 12/15\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977ms/step - dice_coefficient: 0.6701 - iou: 0.5040 - loss: 0.5188\nEpoch 12: val_dice_coefficient improved from 0.64998 to 0.65018, saving model to /kaggle/working/best_model.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 1s/step - dice_coefficient: 0.6701 - iou: 0.5040 - loss: 0.5188 - val_dice_coefficient: 0.6502 - val_iou: 0.4819 - val_loss: 0.5642 - learning_rate: 1.0000e-04\nEpoch 13/15\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975ms/step - dice_coefficient: 0.6733 - iou: 0.5077 - loss: 0.5139\nEpoch 13: val_dice_coefficient improved from 0.65018 to 0.65537, saving model to /kaggle/working/best_model.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 1s/step - dice_coefficient: 0.6733 - iou: 0.5077 - loss: 0.5139 - val_dice_coefficient: 0.6554 - val_iou: 0.4876 - val_loss: 0.5712 - learning_rate: 1.0000e-04\nEpoch 14/15\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976ms/step - dice_coefficient: 0.6754 - iou: 0.5100 - loss: 0.5098\nEpoch 14: val_dice_coefficient improved from 0.65537 to 0.65694, saving model to /kaggle/working/best_model.keras\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 1s/step - dice_coefficient: 0.6754 - iou: 0.5100 - loss: 0.5098 - val_dice_coefficient: 0.6569 - val_iou: 0.4894 - val_loss: 0.5612 - learning_rate: 1.0000e-04\nEpoch 15/15\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975ms/step - dice_coefficient: 0.6783 - iou: 0.5133 - loss: 0.5073\nEpoch 15: val_dice_coefficient did not improve from 0.65694\n\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m454s\u001b[0m 1s/step - dice_coefficient: 0.6782 - iou: 0.5133 - loss: 0.5073 - val_dice_coefficient: 0.6518 - val_iou: 0.4836 - val_loss: 0.5532 - learning_rate: 1.0000e-04\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Load the saved model\nmodel = load_model(\"my_model.keras\", custom_objects={\n    \"combined_loss\": combined_loss,\n    \"dice_coefficient\": dice_coefficient,\n    \"iou\": iou\n})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the model\nmodel.save('/kaggle/working/my_model2.keras')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T21:07:02.385108Z","iopub.status.idle":"2025-03-15T21:07:02.385372Z","shell.execute_reply":"2025-03-15T21:07:02.385270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the entire model\nmodel.save(\"my_model.keras\")  # or use .h5 for HDF5 format","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the entire model\nmodel.save(\"my_model.h5\")  # or use .h5 for HDF5 format","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Load the saved model\nmodel = load_model(\"my_model.keras\", custom_objects={\n    \"combined_loss\": combined_loss,\n    \"dice_coefficient\": dice_coefficient,\n    \"iou\": iou\n})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Continue training for another 30 epochs\nhistory_continued = model.fit(\n    X_train, Y_train,\n    validation_data=(X_val, Y_val),\n    batch_size=16,\n    epochs=60,  # Total epochs (30 new + 30 previous)\n    initial_epoch=30,  # Start from epoch 30\n    callbacks=[checkpoint_callback, reduce_lr]  # Include the checkpoint callback\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}